[
  {
    "objectID": "posts/Web_tools/Web_tools.html",
    "href": "posts/Web_tools/Web_tools.html",
    "title": "Web Tools",
    "section": "",
    "text": "ここでは、参考にさせていただいてるWeb URLを備忘録として保存する。"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#計量生物学会",
    "href": "posts/Web_tools/Web_tools.html#計量生物学会",
    "title": "Web Tools",
    "section": "1 計量生物学会",
    "text": "1 計量生物学会\n\n計量生物学会\n統計家の行動基準"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#論文雑誌",
    "href": "posts/Web_tools/Web_tools.html#論文雑誌",
    "title": "Web Tools",
    "section": "2 論文雑誌",
    "text": "2 論文雑誌\n\nPharmaceutical Statistics\nInternational Journal of Epidemiology\nAmerican Journal of Epidemiology\nStatistics in Medicine\nBiometrical Journal\nPharmacoepidemiology ＆ Drug Safety\nEpidemiology\nEpidemiologic Methods\nJournal of Causal Inference\nEurpean Journal of Epidmiology\nJournal of Clinical Epidemiology\nJournal of Epideimology\nBiostatistics\nThe International Journal of Biostatistics\n計量生物学\n薬剤疫学\nすうがくぶんか株式会社\nQuarto\nGit公式書籍（日本語）"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#医薬品開発",
    "href": "posts/Web_tools/Web_tools.html#医薬品開発",
    "title": "Web Tools",
    "section": "3 医薬品開発",
    "text": "3 医薬品開発\n\nPMDA 審査関連業務の概要について\nICH Efficacyガイドライン\n製薬協データサイエンス部会 成果物\n製薬協 医薬品評価委員会シンポジウム\nFDA Clinical Trials Guidance Documents\nFDA Real-World Evidence Documents\nEMA Biostatistics guidelines\nEMA Real-world evidence guidelines\nCDISC ADaM\nStatistical Analysi Plan(Australian Clinical Trials)"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#統計",
    "href": "posts/Web_tools/Web_tools.html#統計",
    "title": "Web Tools",
    "section": "4 統計",
    "text": "4 統計\n\nPankaj Kumar Choudhary先生\nFrank Harrell先生\nBayesian Data Analysis Course\n早稲田大学 村田先生 講義資料等\nCamden Lopez（海外の生物統計家ブログ）\nBiostatistics for Biomedical Research\n久保川達也先生 書籍関連\n医学のための因果推論"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#sasプログラミング",
    "href": "posts/Web_tools/Web_tools.html#sasプログラミング",
    "title": "Web Tools",
    "section": "5 SASプログラミング",
    "text": "5 SASプログラミング\n\nSAS備忘録\nPharmaceutical Software Users Group\nConference Proceedings (1976 - present) … and more\nSASユーザー会\nSASユーザー総会 論文集アーカイブ\nSAS Forumユーザー会 2006\n大阪SAS勉強会\nデータステップ100万回　SAS新手一生\n晴れ時々SAS\nSAS One Dash\n我輩はブロガーではない。ネタもまだない\n僕の頁 \nデータサイエンス100本ノック（構造化データ加工編）のSAS版\nRTFファイルをPDF化するDDEプログラム\nSGPLOTでForest plot"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#rプログラミング書籍",
    "href": "posts/Web_tools/Web_tools.html#rプログラミング書籍",
    "title": "Web Tools",
    "section": "6 Rプログラミング（書籍）",
    "text": "6 Rプログラミング（書籍）\n\nR for Data Science (2nd Edtion)\nAdvanced R (2nd Edition)\nR Cookbook, 2nd Edition\nQuarto 公式guide"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#rプログラミング海外サイト",
    "href": "posts/Web_tools/Web_tools.html#rプログラミング海外サイト",
    "title": "Web Tools",
    "section": "7 Rプログラミング（海外サイト）",
    "text": "7 Rプログラミング（海外サイト）\n\nTidyverse style guide\nR Workflow for Reproducible Data Analysis and Reporting\nBuilding reproducible analytical pipelines with R\nR Rpharma\nR Workflow（Frank Harrell先生\nReproducible Medical Research with R\nR for Clinical Study Reports and Submission\nAn Introduction to Statistical Programming Methods with R\nTables in Clinical Trials with R\nIntroduction to tern\nWorkshops at rstudio::conf 2022\nTLG Catalog\nIntroduction to {rtables}\nReproducible Environments(Posit社)\nR Submission Pilot 1\nR Submission Pilot 3"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#rプログラミング日本語",
    "href": "posts/Web_tools/Web_tools.html#rプログラミング日本語",
    "title": "Web Tools",
    "section": "8 Rプログラミング（日本語）",
    "text": "8 Rプログラミング（日本語）\n\nデータサイエンス100本ノック（構造化データ加工編）をRで解く\nRによる再現可能なデータ分析（瓜生真也先生）\nRによるデータ解析のための前処理（瓜生真也先生）\n次の一歩を踏み出すためのtidyverse入門（瓜生真也先生）\n今日からできる再現可能な論文執筆（国里愛彦先生・竹林由武先生）\nはじめよう！R（小杉考司先生）\n気軽にRでWebサイト\nRではじめようモダンなデータ分析\n私たちのR\nRプログラムの個人ブログ\nデータハンドリング入門\n2025年4月2日 ワークショップ「オープンソースソフトウェア「R」の承認申請業務での利用実態と今後の展望について」\ndplyrを使いこなす！Window関数編\nR Markdownで数式を使ったPDFを作成するときのメモ"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#研究関連",
    "href": "posts/Web_tools/Web_tools.html#研究関連",
    "title": "Web Tools",
    "section": "9 研究関連",
    "text": "9 研究関連\n\nWriting-Tips Series(Journal of Clinical Epidemiology)"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#論文",
    "href": "posts/Web_tools/Web_tools.html#論文",
    "title": "Web Tools",
    "section": "10 論文",
    "text": "10 論文\n\nTen Simple Rules for Reproducible Computational Research\n山本先生_LatexによるBibTeXにおけるbibファイルの書き方"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#ブログ関係",
    "href": "posts/Web_tools/Web_tools.html#ブログ関係",
    "title": "Web Tools",
    "section": "11 ブログ関係",
    "text": "11 ブログ関係\n\n長島健悟先生のブログ\nKRSK先生のブログ\n司馬先生のブログ\nYanagimoto先生のブログ\nすきとほるさんのブログ"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#quartoブログ",
    "href": "posts/Web_tools/Web_tools.html#quartoブログ",
    "title": "Web Tools",
    "section": "12 Quartoブログ",
    "text": "12 Quartoブログ\n\ntutorial\nBuilding a blog with Quarto\nMarkdown記法チート\nQuartoでアカデミックなスライドを作るためのTips"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#tips",
    "href": "posts/Web_tools/Web_tools.html#tips",
    "title": "Web Tools",
    "section": "13 Tips",
    "text": "13 Tips\n\nExcelシートの一括変換 note\nExcelファイルをまとめてPDFファイルに変換したい\n複数のExcelファイルを一括でPDFファイルに変換する方法\nメモ帳だけで作成！複数のExcelファイルをPDFに一括変換\nWinMerge日本語版"
  },
  {
    "objectID": "posts/Web_tools/Web_tools.html#その他",
    "href": "posts/Web_tools/Web_tools.html#その他",
    "title": "Web Tools",
    "section": "14 その他",
    "text": "14 その他\n\n熊本大学大学教育統括管理運営機構附属数理科学総合教育センター"
  },
  {
    "objectID": "posts/statistics/index.html",
    "href": "posts/statistics/index.html",
    "title": "Notes",
    "section": "",
    "text": "抗がん剤試験におけるEstimandを考える\n\n\n\n臨床試験\n\n\n\n\n\n\n2025-09-21\n\n\n\n\n\n\n\nFDAガイダンス\n\n\n\n臨床試験\n\n\n\n\n\n\n2025-09-21\n\n\n\n\n\n\n\nSTROBE\n\n\n\n博士課程\n\n\n\n\n\n\n2025-09-02\n\n\n\n\n\n\n\nHow to check the simulation study\n\n\n\n疫学研究\n\n\n\n\n\n\n2025-08-26\n\n\n\n\n\n\n\nCausal Inference in Studying the Long-Term Health Effects of Disasters: Challenges and Potential Solutions\n\n\n\n博士課程\n\n\n\n\n\n\n2025-08-26\n\n\n\n\n\n\n\nUsing Propensity Score for Causal Inference -Pitfalls and Tips\n\n\n\n博士課程\n\n\n\n\n\n\n2025-08-26\n\n\n\n\n\n\n\nUsing simulation studies to evaluate statistical methods\n\n\n\n臨床統計\n\n\n\n\n\n\n2025-08-26\n\n\n\n\n\n\n\n医学論文を書くための心構え\n\n\n\n医学論文\n\n\n\n\n\n\n2025-08-24\n\n\n\n\n\n\n\n形容詞・副詞\n\n\n\nEnglish\n\n\n\n\n\n\n2025-08-23\n\n\n\n\n\n\n\n接続詞\n\n\n\nEnglish\n\n\n\n\n\n\n2025-08-23\n\n\n\n\n\n\n\n前置詞\n\n\n\nEnglish\n\n\n\n\n\n\n2025-08-23\n\n\n\n\n\n\n\n否定表現\n\n\n\nEnglish\n\n\n\n\n\n\n2025-08-23\n\n\n\n\n\n\n\nコロン_セミコロン_ハイフン_ダッシュ\n\n\n\nEnglish\n\n\n\n\n\n\n2025-08-23\n\n\n\n\n\n\n\n業務受託時のフォルダ構造\n\n\n\n博士課程\n\n\n\n\n\n\n2025-08-20\n\n\n\n\n\n\n\n統計のための線形代数・微分積分まとめ1\n\n\n\n数理統計\n\n\n\n\n\n\n2025-08-18\n\n\n\n\n\n\n\n統計のための線形代数・微分積分まとめ2\n\n\n\n数理統計\n\n\n\n\n\n\n2025-08-18\n\n\n\n\n\n\n\n関係詞\n\n\n\nEnglish\n\n\n\n\n\n\n2025-08-15\n\n\n\n\n\n\n\n主語と動詞\n\n\n\nEnglish\n\n\n\n\n\n\n2025-08-08\n\n\n\n\n\n\n\n能動態と受動態\n\n\n\nEnglish\n\n\n\n\n\n\n2025-08-08\n\n\n\n\n\n\n\n潜在結果変数と統計的推測\n\n\n\n臨床試験\n\n\n\n\n\n\n2025-07-28\n\n\n\n\n\n\n\n臨床試験の中間事象\n\n\n\n臨床試験\n\n\n\n\n\n\n2025-07-28\n\n\n\n\n\n\n\n冠詞について\n\n\n\nEnglish\n\n\n\n\n\n\n2025-07-26\n\n\n\n\n\n\n\n大人の英語学習勉強1\n\n\n\nEnglish\n\n\n\n\n\n\n2025-07-23\n\n\n\n\n\n\n\n利用しない：臨床研究のための実データ解析フォルダ構成\n\n\n\n博士課程\n\n\n\n\n\n\n2025-07-13\n\n\n\n\n\n\n\n方法論研究におけるフォルダ構成を考える\n\n\n\n博士課程\n\n\n\n\n\n\n2025-07-11\n\n\n\n\n\n\n\nSAS・Rでの相対パス ../../ の使い方完全ガイド\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-07-03\n\n\n\n\n\n\n\n研究メモ\n\n\n\nメモ\n\n\n\n\n\n\n2025-07-02\n\n\n\n\n\n\n\nRWD研究における解析用データセット仕様書\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-07-02\n\n\n\n\n\n\n\nSASによる便利関数1\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-07-02\n\n\n\n\n\n\n\n解析プロジェクトの最初に作成するSASプログラム\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-07-02\n\n\n\n\n\n\n\nJoint Interventionにおけるモデル構築\n\n\n\n方法論\n\n\n\n\n\n\n2025-07-01\n\n\n\n\n\n\n\nProc LIFEREG\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-07-01\n\n\n\n\n\n\n\nADaM作成においてDataステップにおける便利な関数\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-29\n\n\n\n\n\n\n\n臨床試験データ処理の実践：人口統計データ（ADSL）作成テクニック\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-29\n\n\n\n\n\n\n\nDDEによるExcelデータの読み込み\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-29\n\n\n\n\n\n\n\nProc Contentsを利用したRawデータの変数を_varにするマクロ\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-29\n\n\n\n\n\n\n\nデータセット作成のTips\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-29\n\n\n\n\n\n\n\nProc Transpose/ARRAYによる転置\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-29\n\n\n\n\n\n\n\nSASのProc SGPLOTに関するTips\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-29\n\n\n\n\n\n\n\nTTE総説論文\n\n\n\n薬剤疫学\n\n\n\n\n\n\n2025-06-29\n\n\n\n\n\n\n\nSAS：要約統計量作成マクロ\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-29\n\n\n\n\n\n\n\nSASプログラミングのPitfalls and Bad Habits\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-28\n\n\n\n\n\n\n\nProc Contents ProcedureとProc Dataset Procedure\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-25\n\n\n\n\n\n\n\n臨床試験における有害事象データの集計：PROC SQL\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-25\n\n\n\n\n\n\n\nPROGRAMMING FOR JOB SECURITY REVISITED\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-25\n\n\n\n\n\n\n\nプログラミング一般的な考え方（MUST DO , MUST NOT DO)\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-25\n\n\n\n\n\n\n\nADaM IG1.3\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-18\n\n\n\n\n\n\n\nSASによる解析業務開始時のフォルダ整理・作成\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-16\n\n\n\n\n\n\n\n解析用データセット作成の流れ2\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-16\n\n\n\n\n\n\n\nMarkdown記法について\n\n\n\nMarkdown\n\n\n\n\n\n\n2025-06-15\n\n\n\n\n\n\n\nSQL入門1\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-15\n\n\n\n\n\n\n\ngithubのブログ更新手順について\n\n\n\ngithub\n\n\n\n\n\n\n2025-06-14\n\n\n\n\n\n\n\nSASプログラミング業務のフレームワーク\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-14\n\n\n\n\n\n\n\nSASマクロ入門1\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-14\n\n\n\n\n\n\n\n解析用データセット仕様書\n\n\n\nSAS\n\nR\n\n解析用データセット\n\n\n\n\n\n\n2025-06-14\n\n\n\n\n\n\n\n解析用データセット作成の流れ1（フォルダ構造等）\n\n\n\nSAS\n\n解析プログラミング\n\n\n\n\n\n\n2025-06-14\n\n\n\n\n\n\n\n第2相抗がん剤開発及び検証的試験の中間解析\n\n\n\n臨床試験\n\nSAS\n\n\n\n\n\n\n2025-05-24\n\n\n\n\n\n\n\n臨床試験のサンプルサイズ設計\n\n\n\n臨床試験\n\nSAS\n\n\n\n\n\n\n2025-05-17\n\n\n\n\n\n一致なし"
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html",
    "href": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html",
    "title": "Using Propensity Score for Causal Inference -Pitfalls and Tips",
    "section": "",
    "text": "ノート\n\n\n\nMethods based on propensity score (PS) have become increasingly popular as a tool for causal inference. A better understanding of the relative advantages and disadvantages of the alternative analytic approaches can contribute to the optimal choice and use of a specific PS method over other methods. In this article, we provide an accessible overview of causal inference from observational data and two major PS-based methods (matching and inverse probability weighting), focusing on the underlying assumptions and decision-making processes. We then discuss common pitfalls and tips for applying the PS methods to empirical research and compare the conventional multivariable outcome regression and the two alternative PS-based methods (ie, matching and inverse probability weighting) and discuss their similarities and differences. Although we note subtle differences in causal identification assumptions, we highlight that the methods are distinct primarily in terms of the statistical modeling assumptions involved and the target population for which exposure effects are being estimated."
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#abstract",
    "href": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#abstract",
    "title": "Using Propensity Score for Causal Inference -Pitfalls and Tips",
    "section": "",
    "text": "ノート\n\n\n\nMethods based on propensity score (PS) have become increasingly popular as a tool for causal inference. A better understanding of the relative advantages and disadvantages of the alternative analytic approaches can contribute to the optimal choice and use of a specific PS method over other methods. In this article, we provide an accessible overview of causal inference from observational data and two major PS-based methods (matching and inverse probability weighting), focusing on the underlying assumptions and decision-making processes. We then discuss common pitfalls and tips for applying the PS methods to empirical research and compare the conventional multivariable outcome regression and the two alternative PS-based methods (ie, matching and inverse probability weighting) and discuss their similarities and differences. Although we note subtle differences in causal identification assumptions, we highlight that the methods are distinct primarily in terms of the statistical modeling assumptions involved and the target population for which exposure effects are being estimated."
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#steps",
    "href": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#steps",
    "title": "Using Propensity Score for Causal Inference -Pitfalls and Tips",
    "section": "0.2 3 Steps",
    "text": "0.2 3 Steps\n\nspecifying causal estimand\ncausal identifiation\nestimation"
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#basics-of-propensity-scores",
    "href": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#basics-of-propensity-scores",
    "title": "Using Propensity Score for Causal Inference -Pitfalls and Tips",
    "section": "0.3 Basics of propensity scores",
    "text": "0.3 Basics of propensity scores"
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#matching",
    "href": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#matching",
    "title": "Using Propensity Score for Causal Inference -Pitfalls and Tips",
    "section": "0.4 Matching",
    "text": "0.4 Matching"
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#inverse-probability-weighting",
    "href": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#inverse-probability-weighting",
    "title": "Using Propensity Score for Causal Inference -Pitfalls and Tips",
    "section": "0.5 Inverse probability weighting",
    "text": "0.5 Inverse probability weighting"
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#section",
    "href": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#section",
    "title": "Using Propensity Score for Causal Inference -Pitfalls and Tips",
    "section": "0.6 ",
    "text": "0.6"
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#on-the-application-of-the-ps-methods.-tips",
    "href": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#on-the-application-of-the-ps-methods.-tips",
    "title": "Using Propensity Score for Causal Inference -Pitfalls and Tips",
    "section": "0.7 On the application of the PS methods. Tips",
    "text": "0.7 On the application of the PS methods. Tips\n\nThe goal of propensity score models is not to predict an exposure perfectly.\nThe goal of the PS methods is to achieve balance in observed confounders; hence, an optimal propensity model is not the one that best predicts an exposure. To achieve this goal of the PS methods as a tool for confounding adjustment, applied users need to consider the following two aspects of a propensity model specification: 1) variable selection and 2) model evaluation. It is recommended that a propensity model include only variables that affect an outcome. If the variables also affect an exposure, they are confounders and should be adjusted for (L1 in Figure 1). Even when they do not affect exposure and, thus, are not confounders (L2 in Figure 1), their distributions are likely not identical between the exposure groups in a finite sample; adjusting for these imbalanced variables reduces bias in an effect estimate and its variance. Importantly, the “variables that affect an outcome” should be selected based on subject-matter knowledge about underlying causal structures rather than statistical associa- tions with the outcome.24 Variables that affect only an exposure (L3 in Figure 1) should not be included in a propensity model because such variables can inflate the variance of the effect estimates.25 Moreover, variables in a propensity model should ideally be measured prior to the exposure to avoid accidentally adjusting for potential mediators (M in Figure 1).\nStudies using the PS methods often report measures of “model fit” such as c-statistic (ie, area under the curve), aiming to evaluate the predictive performance of a propensity model.Because the prediction of exposure is not the goal of a propensity model, reporting the measures of model fit is of limited value.26 For instance, adding exposure predictors that are not confounders (eg, L3 in Figure 1) increases the c-statistic but does not necessarily enhance causal inference. To evaluate a propensity model in terms of confounding adjustment, covariate balance should instead be checked after PS estimation. Covariate balance can be assessed by calculating a standardized difference for each covariate using the matched sample for PSM and the weighted sample for IPW. The formulas to calculate standardized differences are available elsewhere.3,27 Some scholars have used &lt;0.1 standardized difference as support for covariate balance.28\nPropensity Score Matching suffer from residual confounding even when conditional exchangeability holds.\nIt is generally hard to find pairs with identical PS values because PS is a continuous variable by definition and can take any value between 0 and 1. Thus, a common practice is to select an unexposed individual with a PS value closest to that of an exposed individual (nearest neighbor matching), often from a pool of unexposed individuals within a pre-specified range of PS differences from their exposed counterpart (caliper distance). Wider caliper distance may result in pairs with large differences in PS values, leading to unbalanced confounders and resulting bias in the matched sample. Austin recommends using calipers of 0.2 standard deviations of PS in the logit scale as a rule of thumb29; however, the optimal caliper width should ideally be determined based on the covariate balance in the matched sample.\nPropensity Score Matching discards unmathced observations and and addresses potential positivity violations in exchange for a loss of statistical efficiency.\nPSM discards unmatched observations with extreme PS values. This property of PSM has an advantage and a disadvantage. The advantage is that it can explicitly address potential positivity violations.13 For example, individuals with extremely high PS values tend to have covariate patterns in which only exposed individuals exist. Because PSM discards information from these individuals and analyzes people within the overlapped range of the PS distribution (ie, common support), it does not rely on model extrapolation that other analytic approaches (eg, PS regression adjustment) might do. The disadvantage is that discarding information may result in imprecise estimates and loss of statistical power. Notably, discarding unmatched observations will change the target population of interest.\nPost-matching adjustment can sometimes induce bias.\nAlthough PSM can achieve balance in observed covariates in expectation, applying PSM to a finite sample sometimes results in imbalanced covariates even after matching, which can cause residual confounding. To address the residual confounding, PSM is sometimes accompanied by stratification or regression adjustment after matching.30 However, bias can arise if such post- matching adjustment includes variables that are not used in a propensity model.31\nIn IPW , using stabilized weights can sometimes gain statistical efficiency.\nThe inverse probability weights we described above had the numerator of one and are called unstabilized weights. Alter- natively, the stabilized weights can be defined using other constant numbers for their numerator, often a marginal prevalence of exposure. Stabilized weights can gain statistical efficiency when a weighted outcome model makes modeling assumptions and is unsaturated (eg, a weighted outcome model for non-binary exposures or conditional effects with baseline covariates).21,32 When IPW was used for a binary treatment to estimate a marginal effect, a weighted outcome model would be saturated; hence, unstabilized weights and stabilized weights would give identical results. There are some cases where unstabilized weights, not stabilized weights, should be used (eg, estimating an effect of a dynamic treatment regimen), but these cases are beyond the scope of this introductory paper.33\nIPW can be used to address selection bias too.\nIPW can also address selection bias.34 Inverse probability weights for censoring (IPCW) are calculated based on probabilities of selection=censoring conditional on exposure and common causes of the censoring and the outcome. Note that the censoring weights can incorporate variables that are not confounders but cause selection bias (eg, L2 in the causal diagram in Figure 2). While IPCW is a useful tool to advance causal inference, the interpretation of the estimated effects after IPCW may not be straightforward in the presence of competing risk.35–37 Moreover, the weight calculation requires the information on exposure and covariates among the censored individuals, which often is unavailable.38\nPSM and IPW both require methods for the analysis of correlated observations.\nIn PSM, the post-matching analysis needs to take account of the within-matched pair correlations.39 For example, post-matching analysis can use paired t-test or Wilcoxon’s rank sum test for continuous outcomes, and McNemar’s test and conditional logistic regression for binary outcomes, and cox proportional hazards regression for survival outcomes.39,40 Similarly, in IPW, standard errors from the IP-weighted outcome regression need to be corrected due to the dependent observations in the weighted data; using robust variance or non-parametric bootstrapping is recommended to estimate standard errors (R and SAS codes are available in eMaterials 2).32\nThe PS methods and multivariable outcome regression both assume no unmeasured confounding. However , there are properties of the PS methods that are sometimes advantageous.\nThe PS methods and the multivariable outcome regression approach both assume conditional exchangeability given measured covariates. Thus, they can only address confounding caused by measured covariates and are equally prone to bias due to unmeasured confounders. Nevertheless, the PS methods can sometimes be preferable for the following five reasons.\nFirst, in theory, the PS methods can result in data analysis with more integrity and work against p-hacking.41 Most of the PS methods’ modeling decisions come before looking at outcome data. Thus, investigators may be less tempted to change model specifications to make the results align with their expectations. In PSM, for instance, the investigator first specifies a propensity model and estimate PS, creates a matched sample, checks the balance of observed covariates between the exposed and the unexposed, and, if unbalanced, goes back and re-specifies a propensity model, all of which can be done without outcome data. Even for the methods that specify an outcome model (ie, regression adjustment and IPW), the outcome model generally makes fewer or even no modeling assumptions than a multivariable outcome regression conditioning on numerous covariates. However, this first advantage may not fully be leveraged in the applied research because careless application of the PS methods would not yield this theoretical property.\nSecond, potential positivity violations tend to become more visible in the PS methods because extreme PS values can signal covariate patterns in which only the exposed or the unexposed are present. As we describe below, the PS methods handle potential positivity violations differently.\nThird, when the outcome is rare, conditioning on numerous covariates via a multivariable regression can result in imprecise estimation. When the exposure is non-rare, the PS methods can work better for rare outcomes because they convert the high- dimensional covariates into a single variable, PS.\nFourth, the PS methods and the multivariable outcome regression make qualitatively different modeling assumptions. The PS methods’ primary modeling decisions are for a propensity model. Although the propensity models and outcome models conditional on measured covariates are both prone to misspecification, one may feel more confident of correctly specifying an exposure model in situations where more knowledge about the relationships with covariates is available for exposure than for an outcome. A doubly-robust method (eg, targeted maximum likelihood estimation) can accommodate both models and consistently estimate conditional expectancies of interest if either a propensity model or an outcome model is correctly specified.42,43 Notably, multivariable outcome regression technically estimates conditional effects within the strata of observed covariates. Although a marginal effect can rigorously be estimated via standardization(R and SAS codes are available in eMaterials 2),44 a more common approach for estimating marginal effects with multivariable outcome regression is to assume no effect measure modification by ANY of measured covariates (ie, no product term between exposure and covariates). On the other hand, the PS methods tend to make no or fewer assumptions for effect measure modification, although they instead make assumptions for a propensity model.\nLastly, IPW can be expanded to causal inference for a time- varying exposure in the presence of time-varying confounding.21,45 Conventional analytic approaches, including other PS methods, fail to estimate the effects of a time-varying exposure when prior exposure affects confounders of subsequent exposure.46\nThe alternative PS methods rely on the same assumptions for exchangeability and consistency but deal with the positivity assumption differently.\nBoth PSM and IPW rely on the same identifiability assumptions of conditional exchangeability and consistency. In contrast, these methods take different approaches to handle potential positivity violations. In IPW, individuals will receive substantially large or small weights when their covariate patterns potentially violate positivity. Trimming such observations with extreme weights is often recommended.47 On the other hand, PSM explicitly addresses potential positivity violations by excluding those who have extreme PS values and, thus, cannot be matched (so-called “off-support” individuals). While such explicit handling of positivity violations is the advantage of the PS methods, one caveat is that causal estimand of interest generally changes after excluding individuals who potentially violate positivity.20\nAlthough the PS methods both make the same exchangeability asuumption, PSM can suffer from residual confounding\nBoth PSM and IPW are based on the same conditional exchangeability (ie, no confounding conditional on measured covariates). However, as we noted in #2 above, PSM may result in an insufficient balance of the measured covariates when the pre-specified caliper is wide. On the other hand, IPW does not suffer from residual confounding, assuming the models involved are correctly specified.\nThe PS methods make different modeling assumptions after propensity score estimation\nBoth PSM and IPW specify a propensity model to estimate PS. PSM often does not require any further modeling once PS is estimated although an outcome model is sometimes used to make post-matching adjustment. IPW specifies a weighted outcome model to approximate a marginal structural model, but the outcome model tends to make fewer assumptions than it does in PSM (when post-matching adjustment via regression is conducted) or even be saturated (no modeling assumption) when estimating the marginal effect of a single-point binary exposure.\nThe PS methods target different causal estimands.\nPSM and IPW generally target different causal estimands.20,48,49 In other words, when an effect estimate from one PS method differs from an estimate from another PS method, they can both be correct but simply answer different questions. PSM estimates a marginal effect in a population represented by a matched sample. Because the matched sample excludes individuals with extreme PS values, PSM does not estimate an exposure effect among individuals who would always or never be exposed unless they were intervened and forced to have an alternative exposure level. PSM often uses all exposed individuals and matches them with their unexposed pairs. This approach will estimate an exposure effect among the people who were in fact exposed (ie, average treatment effect in the treated).3 IPW can estimate both marginal and conditional effects, depending on the definition of weights and specification of a marginal structural model."
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#section-1",
    "href": "posts/statistics/2025/論文紹介_Shiba_2021_PS_tips.html#section-1",
    "title": "Using Propensity Score for Causal Inference -Pitfalls and Tips",
    "section": "1.1 ",
    "text": "1.1"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html",
    "title": "解析用データセット作成の流れ2",
    "section": "",
    "text": "本記事では、以下の私のブログを踏まえて、より具体的な解析データセット作成の流れをまとめていく。\n\n\n\n\n\n\n\n\n\n\n\n\n\n坂本航太\n\n\n2025/06/14\n\n\n\n\n\n\n\n\n\n\n\n\n個人的な解析データセット作成方法をまとめます。\n\n\n\n坂本航太\n\n\n2025/06/14\n\n\n\n\n\n\n一致なし\n\n\n\n\nPharmaSUG2010 - Paper AD16 Automating the Link between Metadata and Analysis Dataset Misha Rittmann, Octagon Research Solutions, Inc., Wayne, PA\nPharmaSUG2010 - Paper TT06 SAS Programming Techniques for Manipulating Metadata on the Database Level Chris Speck, PAREXEL International, Durham, NC\nPharmaSUG2011 – Paper CD17 Making a List, Checking it Twice (Part 1): Techniques for Specifying and Validating Analysis Datasets\nPharmaSUG2011 - Paper CD12ADaM Standard Naming Conventions are Good to HaveChristine Teng, Merck Sharp & Dohme Corp, Rahway, NJ"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#参考文献",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#参考文献",
    "title": "解析用データセット作成の流れ2",
    "section": "",
    "text": "PharmaSUG2010 - Paper AD16 Automating the Link between Metadata and Analysis Dataset Misha Rittmann, Octagon Research Solutions, Inc., Wayne, PA\nPharmaSUG2010 - Paper TT06 SAS Programming Techniques for Manipulating Metadata on the Database Level Chris Speck, PAREXEL International, Durham, NC\nPharmaSUG2011 – Paper CD17 Making a List, Checking it Twice (Part 1): Techniques for Specifying and Validating Analysis Datasets\nPharmaSUG2011 - Paper CD12ADaM Standard Naming Conventions are Good to HaveChristine Teng, Merck Sharp & Dohme Corp, Rahway, NJ"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#はじめに",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#はじめに",
    "title": "解析用データセット作成の流れ2",
    "section": "2.1 はじめに",
    "text": "2.1 はじめに\n臨床研究でADaMデータセットを作成する際、仕様書通りの変数属性（長さ、ラベル）を確実に設定することは極めて重要です。しかし、実データを処理しながら属性を設定すると、元データの属性に引きずられてしまうリスクがあります。 今回は、Excelの仕様書から「空箱データセット」を自動生成し、確実に仕様書通りの構造を作る方法をご紹介します。\n\n2.1.1 よくある失敗パターン\n通常、ADSLデータセットを作成する際は以下のように手動でattrib文を記述します：\ndata adsl;\n    set dm;  /* 実データを先に読み込み */\n    \n    /* 後からattribを設定しても... */\n    attrib \n        STUDYID length=$20 label=\"Study Identifier\"\n        AGE     length=8   label=\"Age\";\n    \n    /* 元データの属性に引きずられる可能性 */\nrun;\n\n\n2.1.2 この方法の問題点\n\n元データの変数属性が優先される場合がある\n仕様書通りの長さに設定されない\nラベルが正しく設定されない場合がある\n\n\n\n2.1.3 解決方法：空箱データセットの活用\n\n\n2.1.4 コンセプト\n\n空箱データセット：仕様書通りの構造だけを持つ0観測のデータセット\n構造の継承：空箱の構造を継承して実データを処理\n確実性：仕様書の属性が必ず適用される"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#実践的な仕様書構造",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#実践的な仕様書構造",
    "title": "解析用データセット作成の流れ2",
    "section": "2.2 実践的な仕様書構造",
    "text": "2.2 実践的な仕様書構造\n\n2.2.1 Analysis Dataset Metadataシート\n\n\n\n\n\n\n\n\n\nDataset Name\nDataset Description\nDataset Structure\nKey Variables\n\n\n\n\nADSL\nSubject Population, demographic and baseline\none record per subject\nUSUBJID\n\n\nADAE\nAdverse Event Analysis Dataset\none record per subject per AE\nUSUBJID, AESEQ\n\n\nADEF\nAnalysis Dataset for Efficacy Disease Parameters\n1 record per subject parameter\nUSUBJID, PARAMCD\n\n\n\n\n\n2.2.2 個別データセットシート（ADSL例）\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nVariable Name\nVariable Label\nType\nLength\nDisplay Format\nOrigin\nSource/Derivation\n\n\n\n\nADSL\nSTUDYID\nStudy Identifier\ntext\n200\n$20\nPredecessor\nDM.STUDYID\n\n\nADSL\nUSUBJID\nUnique Subject Identifier\ntext\n200\n$20\nPredecessor\nDM.USUBJID\n\n\nADSL\nAGE\nAge\ninteger\n8\n8.0\nPredecessor\nDM.AGE\n\n\nADSL\nSEX\nSex\ntext\n1\n$1\nPredecessor\nDM.SEX\n\n\nADSL\nSAFFL\nSafety Population Flag\ntext\n1\n$1\nDerived\n条件により設定"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#sasコードによる自動生成",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#sasコードによる自動生成",
    "title": "解析用データセット作成の流れ2",
    "section": "2.3 SASコードによる自動生成",
    "text": "2.3 SASコードによる自動生成"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#excel仕様書の読み込み",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#excel仕様書の読み込み",
    "title": "解析用データセット作成の流れ2",
    "section": "2.4 Excel仕様書の読み込み",
    "text": "2.4 Excel仕様書の読み込み\n/* ADaM仕様書を読み込み */\nproc import \n    datafile=\"C:\\specs\\ADaM_Specifications.xlsx\"  \n    out=work.spec_adsl\n    dbms=excel\n    replace;\n    sheet=\"ADSL\";\n    getnames=yes;\nrun;\n\n/* 内容確認 */\nproc print data=work.spec_adsl;\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#空箱データセット生成コードの作成",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#空箱データセット生成コードの作成",
    "title": "解析用データセット作成の流れ2",
    "section": "2.5 空箱データセット生成コードの作成",
    "text": "2.5 空箱データセット生成コードの作成\n/* 空箱データセット生成コードを作成 */\ndata _null_;\n    file \"C:\\temp\\ADSL_shell.sas\";\n    set work.spec_adsl end=last_obs;\n    \n    if _n_ = 1 then do;\n        put \"/* ADSL空箱データセット */\";\n        put \"data adsl_shell;\";\n        put \"    attrib\";\n    end;\n    \n    /* 各変数の属性を出力 */\n    put \"        \" Variable_Name @20 \"length=\" @;\n    if upcase(Type) = \"TEXT\" then put \"$\" @;\n    \n    /* Display Formatを使用 */\n    if Display_Format ne '' then \n        put Display_Format @35 \"label='\" Variable_Label \"'\";\n    else \n        put Length @35 \"label='\" Variable_Label \"'\";\n    \n    if last_obs then do;\n        put \"    ;\";\n        put \"    call missing(of _all_);  /* 全変数を欠損値に設定 */\";\n        put \"    stop;                    /* 0観測データセット */\";\n        put \"run;\";\n    end;\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#生成されるコード例",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#生成されるコード例",
    "title": "解析用データセット作成の流れ2",
    "section": "2.6 生成されるコード例",
    "text": "2.6 生成されるコード例\n実行後、C:\\temp\\ADSL_shell.sasに以下が生成されます：\n/* ADSL空箱データセット */\ndata adsl_shell;\n    attrib\n        STUDYID             length=$20              label='Study Identifier'\n        USUBJID             length=$20              label='Unique Subject Identifier'\n        AGE                 length=8                label='Age'\n        SEX                 length=$1               label='Sex'\n        SAFFL               length=$1               label='Safety Population Flag'\n    ;\n    call missing(of _all_);  /* 全変数を欠損値に設定 */\n    stop;                    /* 0観測データセット */\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#空箱データセットを使った実データ処理",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#空箱データセットを使った実データ処理",
    "title": "解析用データセット作成の流れ2",
    "section": "2.7 空箱データセットを使った実データ処理",
    "text": "2.7 空箱データセットを使った実データ処理\n/* 空箱データセットを作成 */\n%include \"C:\\temp\\ADSL_shell.sas\";\n\n/* 実際のADSL作成 */\ndata derived.adsl;\n    if 0 then set adsl_shell;  /* 構造のみ継承、実際は読み込まない */\n    set rawdata.dm;            /* 実データを処理 */\n    \n    /* データ処理 */\n    studyid = _studyid;\n    usubjid = _usubjid;\n    age = _age;\n    sex = _sex;\n    \n    /* 派生変数の作成 */\n    if /* 条件 */ then saffl = 'Y';\n    else saffl = 'N';\n    \n    keep studyid usubjid age sex saffl;\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#重要なポイント",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#重要なポイント",
    "title": "解析用データセット作成の流れ2",
    "section": "2.8 重要なポイント",
    "text": "2.8 重要なポイント\n\n2.8.1 call missing(of all) の役割\n全ての変数を欠損値に設定し、データ型と長さのみを定義します。\ncall missing(of _all_);\n\n\n2.8.2 if 0 then set トリックの活用\nこのテクニックにより、空レコードを処理することなく、純粋に構造のみを継承できます。\nif 0 then set adsl_shell;  /* 決して実行されないが構造は継承 */\n\n\n2.8.3 stop ステートメントの必要性\nデータの読み込みを即座に停止し、構造のみのデータセットを作成します。\nstop;  /* 0観測データセットにする */"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#例",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#例",
    "title": "解析用データセット作成の流れ2",
    "section": "2.9 例",
    "text": "2.9 例\n%macro create_shell(dsname);\n    /* 仕様書読み込み */\n    proc import \n        datafile=\"C:\\specs\\ADaM_Specifications.xlsx\"  \n        out=work.spec_&dsname\n        dbms=excel\n        replace;\n        sheet=\"&dsname\";\n        getnames=yes;\n    run;\n    \ndata _null_;\n    file \"C:\\Users\\temp\\ADSL_shell.sas\";\n    set work.spec_clean end=last_obs;\n    \n    if _n_ = 1 then do;\n        put \"/* ADSL空箱データセット */\";\n        put \"data adsl_shell;\";\n        put \"    attrib\";\n    end;\n\n    put \"        \" Variable_Name \" length=\" @;\n    if upcase(Type) = \"CHAR\" then put \"$\" @;\n    put Length \" label='\" Variable_Label \"'\";\n    \n    if last_obs then do;\n        put \"    ;\";\n        put \"    call missing(of _all_);  \n        put \"    stop;                    \n        put \"run;\";\n    end;\nrun;\n%mend;\n\n/* 各データセットの空箱を生成 */\n%create_shell(ADSL);\n%create_shell(ADAE);\n%create_shell(ADEF);\n\n2.9.1 %includeによる読み込み\n生成されたSASコードは%include文で読み込み、任意のデータステップで使用できます。\nメリット\n1. 効率性: 手動でのattrib文記述が不要\n2. 正確性: コピペミスやタイプミスを防止\n3. 保守性: 仕様変更時はExcelファイルの更新のみで対応\n4. 再利用性: 複数のプログラムで同じattrib文を使用可能\n5. 検証支援: 独立プログラミングでも同じ仕様書を活用"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#excelコードリストの読み込みと-proc-format用データセット変換",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#excelコードリストの読み込みと-proc-format用データセット変換",
    "title": "解析用データセット作成の流れ2",
    "section": "3.1 Excelコードリストの読み込みと PROC FORMAT用データセット変換",
    "text": "3.1 Excelコードリストの読み込みと PROC FORMAT用データセット変換\nExcelコードリストの読み込み\n/* Excelファイルからコードリストを読み込み */\nproc import datafile=\"C:\\path\\to\\codelist.xlsx\"\n    out=Codelist\n    dbms=xlsx\n    replace;\n    sheet=\"Sheet1\";\n    getnames=yes;\nrun;\n\n/* CNTLIN形式に変換 */\ndata _Fmt;\n    set Codelist;\n    \n    FMTNAME = Codelist_Name;  /* フォーマット名 */\n    START = Value;            /* 開始値 */\n    LABEL = Label;            /* ラベル */\n    \n    /* 数値・文字型の判定 */\n    if Type = 'Num' then TYPE = 'N';\n    else TYPE = 'C';\n    \n    /* 不要な変数を除外 */\n    keep FMTNAME START LABEL TYPE;\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#フォーマット作成出力",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#フォーマット作成出力",
    "title": "解析用データセット作成の流れ2",
    "section": "3.2 フォーマット作成・出力",
    "text": "3.2 フォーマット作成・出力\n/*-------------------------*/\n/*   作業用フォーマット作成           */\n/*-------------------------*/\nproc format lib=work cntlin=_Fmt;\nrun;\n\n/*-------------------------*/\n/*   フォーマットの出力               */\n/*-------------------------*/\nproc format lib=Output cntlin=_Fmt;\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#用途例",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#用途例",
    "title": "解析用データセット作成の流れ2",
    "section": "3.3 用途例",
    "text": "3.3 用途例\n/* 臨床試験の被験者背景表 */\nproc tabulate data=trial_data;\n    class sex agegrp;\n    table sex, agegrp*n*f=8.0;\n    format sex $SEX. agegrp $AGEGRP.;\n    title \"被験者背景（性別・年齢群別）\";\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#specificationシートでの記載例",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#specificationシートでの記載例",
    "title": "解析用データセット作成の流れ2",
    "section": "3.4 Specificationシートでの記載例",
    "text": "3.4 Specificationシートでの記載例\n目的: Excelで管理されているコードリストマスターから自動的にSAS formatを生成し、データの標準化を図る 処理フロー:\n\nExcelコードリストをSASデータセットとして読み込み\nPROC FORMAT用のCNTLIN形式に変換（数値・文字型の自動判定含む）\n作業用ライブラリでフォーマット作成・検証\n本番ライブラリに最終出力\n\n入力ファイル: codelist.xlsxと出力フォーマットである。\n\nSEX（文字型）: M=Male, F=Female, U=Unknown\nSEXN（数値型）: 1=男性, 2=女性\nNYFL（文字型）: Y=Yes, N=No\nAGEU（文字型）: YEARS=Years, MONTHS=Months\n\n\n3.4.1 実装のメリット\n\nExcel管理の利便性: 非プログラマーでもコードリストの更新が容易で、チーム間での共有も簡単です。\n混在データ型対応: 数値型と文字型のフォーマットが混在していても、自動的に適切な形式で生成されます。\n自動化による効率性: 手動でのフォーマット定義が不要になり、Excelファイルの更新が即座にSASフォーマットに反映されます。\n品質向上: ヒューマンエラーが削減され、データの標準化が徹底されます。\n拡張性: 新しいコードリストの追加がExcelシートへの行追加だけで完了し、CDISC標準などへの準拠も効率的に行えます。"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#なぜアンダースコアを付けるのか",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#なぜアンダースコアを付けるのか",
    "title": "解析用データセット作成の流れ2",
    "section": "4.1 なぜアンダースコアを付けるのか？",
    "text": "4.1 なぜアンダースコアを付けるのか？\nSASでデータ処理をしていると、変数名の管理が重要になってきます。特に：\n\nデータマージ時の名前衝突を防ぐ\n生データと加工データの区別\n一時的な変数の識別\n\n例えば、ageという変数があるデータに対して、_ageのように接頭辞を付けることで、元の生データであることを明確にできます。"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#マクロの仕組み",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#マクロの仕組み",
    "title": "解析用データセット作成の流れ2",
    "section": "4.2 マクロの仕組み",
    "text": "4.2 マクロの仕組み\nこのマクロは3つのステップで動作します：\n\n4.2.1 ステップ1：変数情報の取得\n象データセットの変数一覧を取得します。\nproc contents data = work.&raw. out = work.VAR noprint ;\n\n\n4.2.2 ステップ2：マクロ変数の動的生成\n各変数名をマクロ変数として保存：\n\nVAR1_ → 1番目の変数名\nVAR2_ → 2番目の変数名\nMAXV → 総変数数\n\ndata _null_ ;\n  set work.VAR end = eof ;\n  call symputx(\"VAR\"||strip(put(VARNUM, 8.))||\"_\", NAME, 'G') ;\n  if eof then call symputx('MAXV', _N_) ;\nrun ;\n\n\n4.2.3 ステップ3：一括リネーム\nrename\n%do i = 1 %to &MAXV. ;\n  &&VAR&i._ = _&&VAR&i._\n%end ;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#実行例",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#実行例",
    "title": "解析用データセット作成の流れ2",
    "section": "4.3 実行例",
    "text": "4.3 実行例\n%macro rawdata(raw=, sort=, out=&raw);\nproc contents data = work.&raw. out = work.VAR noprint ;\nrun ;\nproc sort data = work.VAR ; \n  by VARNUM ;\nrun ;\ndata _null_ ;\n  set work.VAR end = eof ;\n  call symputx(\"VAR\"||strip(put(VARNUM, 8.))||\"_\", NAME, 'G') ;\n  if eof then call symputx('MAXV', _N_) ;\nrun ;\ndata work.&out. ;\n  set work.&raw. ;\n  rename\n  %do i = 1 %to &MAXV. ;\n    &&VAR&i._ = _&&VAR&i._\n  %end ;\n  ;\nrun ;\n%mend rawdata ;\n\n/* まずirisデータをworkライブラリにコピー */\ndata work.iris;\n  set sashelp.iris;\nrun;\n\n/* マクロを実行 */\n%rawdata(raw=iris, out=iris_renamed)\n\n/* データの中身を確認 */\nproc print data=work.iris_renamed(obs=5);\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ2.html#adamデータセット作成における変数名管理の重要性",
    "href": "posts/statistics/2025/解析用データセット作成の流れ2.html#adamデータセット作成における変数名管理の重要性",
    "title": "解析用データセット作成の流れ2",
    "section": "4.4 ADaMデータセット作成における変数名管理の重要性",
    "text": "4.4 ADaMデータセット作成における変数名管理の重要性\nADaMにおける変数命名の課題 ADaM（Analysis Data Model）データセット作成では、複数のソースから様々な変数を統合する必要があります：\n\nSDTMデータ（生データ）\n派生変数（計算結果）\n解析用変数（統計処理用）\nメタデータ（フラグや分類変数）\nこれらが混在すると、データの出自が不明確になり、品質管理やバリデーションが困難になります。"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html",
    "href": "posts/statistics/2025/解析用データセット仕様書.html",
    "title": "解析用データセット仕様書",
    "section": "",
    "text": "臨床試験や統計解析プロジェクトにおいて、解析用データセット仕様書は分析の設計図とも言える重要な文書です。本記事では、実用的な解析用データセット仕様書の構成と記載内容について、主要な3つのデータセット（ADSL、ADLB、ADTTE）を例に解説し、さらに仕様書に基づくデータセット作成とバリデーションプロセスについても説明します。探索的解析であっても事前に解析用データセット仕様書を作成することを推奨する。\n\n\n\n\nSASユーザー総会2014年度：SASとExcelを用いたCDISCADaM標準における作業効率化の試み（武田薬品、高浪さん：PDFのP351）\nSASユーザー総会2014年度：PMDAへの承認申請時 CDISC標準電子データ提出に向けた社内標準のリモデリング（塩野義製薬：）\nSASユーザー総会2013年度：ライブラリ参照と名前の定義を利用して EXCELファイルへの柔軟なデータ入出力を実現する 解析結果のレポーティングからセルオートマトンまで\nSASユーザー総会2010年度：“Standard Template Programs”の開発\n生存時間解析用ADaMデータセット（ADTTE）のソースコード紹介"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#はじめに",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#はじめに",
    "title": "解析用データセット仕様書",
    "section": "",
    "text": "臨床試験や統計解析プロジェクトにおいて、解析用データセット仕様書は分析の設計図とも言える重要な文書です。本記事では、実用的な解析用データセット仕様書の構成と記載内容について、主要な3つのデータセット（ADSL、ADLB、ADTTE）を例に解説し、さらに仕様書に基づくデータセット作成とバリデーションプロセスについても説明します。探索的解析であっても事前に解析用データセット仕様書を作成することを推奨する。"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#参考文献",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#参考文献",
    "title": "解析用データセット仕様書",
    "section": "",
    "text": "SASユーザー総会2014年度：SASとExcelを用いたCDISCADaM標準における作業効率化の試み（武田薬品、高浪さん：PDFのP351）\nSASユーザー総会2014年度：PMDAへの承認申請時 CDISC標準電子データ提出に向けた社内標準のリモデリング（塩野義製薬：）\nSASユーザー総会2013年度：ライブラリ参照と名前の定義を利用して EXCELファイルへの柔軟なデータ入出力を実現する 解析結果のレポーティングからセルオートマトンまで\nSASユーザー総会2010年度：“Standard Template Programs”の開発\n生存時間解析用ADaMデータセット（ADTTE）のソースコード紹介"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#例",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#例",
    "title": "解析用データセット仕様書",
    "section": "2.1 例",
    "text": "2.1 例\nAnalysis Dataset Metadata シート：データセットの一覧\n\n\n\n\n\n\n\n\n\n\n\n\nDataset Name\nDataset Description\nDataset Location\nDataset Structure\nKey Variables of Interest\nClass of Dataset\nDocumentation\n\n\n\n\nADSL\nSubject Population, demographic and baseline characteristics\nADSL.xpt\none record per subject\nUSUBJID\nADSL\nSAP, ADSL.sas\n\n\nADAE\nAdverse Event Analysis Dataset\nADAE.xpt\none record per subject per each AE recorded\nUSUBJID, AESEQ\nADAE\nADAE.sas\n\n\nADEF\nAnalysis Dataset for Efficacy Disease Parameters\nADEF.xpt\n1 record per subject parameter\nUSUBJID, PARAMCD\nBDS\nDictionary used in MedDRA VOL.X\n\n\n\nADSL シート：ADSL データセットの変数一覧（ADAE、ADEF も同様に作成）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nVariable Name\nVariable Label\nType\nLength\nDisplay Format\nCodelist/Controlled Term\nCodelist Name\nOrigin\nSource/Derivation\n\n\n\n\nADSL\nSTUDYID\nStudy Identifier\ntext\n200\n$20\n\n\nPredecessor\nDM.STUDYID\n\n\nADSL\nUSUBJID\nUnique Subject Identifier\ntext\n200\n$20\n\n\nPredecessor\nDM.USUBJID\n\n\nADSL\nSUBJID\nSubject Identifier\ntext\n200\n$20\n\n\nPredecessor\nDM.SUBJID\n\n\nADSL\nSITEID\nStudy Site Identifier\ntext\n200\n$3\n\n\nPredecessor\nDM.SITEID\n\n\nADSL\nAGE\nAge\ninteger\n8\n8.0\nAGEU\nAGEU\nPredecessor\nDM.AGE\n\n\nADSL\nAGEU\nAge Units\ntext\n200\n$20\nAGEU\nAGEU\nPredecessor\nDM.AGEU\n\n\nADSL\nSEX\nSex\ntext\n1\n$1\nSEX\nSEX\nPredecessor\nDM.SEX\n\n\nADSL\nSEXN\nSex (N)\ntext\n8\n8.0\nSEXN\nSEXN\nPredecessor\n1=F,FEMALE=M,2=M\n\n\nADSL\nRACE\nRace\ntext\n200\n$200\nRACE\nRACE\nPredecessor\nDM.RACE\n\n\nADSL\nRACEN\nRace (N)\ninteger\n8\n8.0\nRACEN\nRACEN\nAssigned\n1=DM.RACE=“ASIAN”\n\n\n\nCodelist シート：コードリストの一覧\n\n\n\nName\nCodeValue\nCodeText\nData Type\n\n\n\n\nSEX\nF\nFemale\ntext\n\n\nSEX\nM\nMale\ntext\n\n\nSEXN\n1\nMale\ninteger\n\n\nSEXN\n2\nFemale\ninteger\n\n\nAGEU\nYEARS\n\ntext\n\n\nARM\nDrug A\nDrug A\ntext\n\n\nARM\nDrug B\nDrug B\ntext\n\n\nARM\nScreen Failure\nScreen Failure\ntext\n\n\nTRT\nDrug A\nDrug A\ntext\n\n\nTRT\nDrug B\nDrug B\ntext\n\n\nTRTN\n1\nDrug A\ninteger\n\n\nTRTN\n2\nDrug B\ninteger\n\n\n\nValue List シート：解析パラメータの一覧\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDataset Name\nVariable Name\nVariable Label\nParameter_variable\nComparator\nParameters\nVariable Type\nLength\nDisplay Format\n\n\n\n\n\nADEF\nAVAL\nAnalysis Value\nPARAMCD\nIN\nHEAL\ninteger\n8\n1.0\n\n\n\nAnalysis Results Metadata シート：解析結果メタデータ\n前向きの臨床試験であれば、ここまでAnalysis Result Metadataを作成することは可能であろう。ただし、探索的にデータを解析する場合は事前に解析方法を定義することは難しいように思われる。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplay Identifier\nDisplay Name\nAnalysis\nPopulation\nDataset\nParameter\nReason\nSelection Criteria\nDocumentation\nProgramming Statements\n\n\n\n\nTable14.2.1\n主要評価項目\nFAS\nADEFF\nAVAL\nPre-specified in SAP\nFASFL=“Y” and PARAMCD = 1 and AVISTN = 4\nSAP\nproc format; value TRTFMT 1 = “Drug A” 2 = “Drug B”; run; proc freq data=ADSL ADEF; where FASFL = “Y” and PARAMCD = 1; table AVISTN　TRT AVAL/chisq nocol nopct format TRT01N TRT1FMT.;run;\n\n\nTable2\n副次評価項目\nFAS\nADEFF\nAVAL\nPre-specified in SAP\nPPROTFL=“Y” and PARAMCD = 1 and AVISTN = 4\nSAP\nproc format; value TRTFMT 1 = “Drug A” 2 = “Drug B”; run; proc freq data=ADSL ADEF; where PPROTFL = “Y” and PARAMCD = 1; table AVISTN TRT AVAL/chisq nocol nopct format TRT01N TRT1FMT.;run;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#解析用データセット仕様書の基本構成",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#解析用データセット仕様書の基本構成",
    "title": "解析用データセット仕様書",
    "section": "2.2 解析用データセット仕様書の基本構成",
    "text": "2.2 解析用データセット仕様書の基本構成\n解析用データセット仕様書は、主に以下の要素で構成されます：\n\n表紙・改訂履歴：文書管理情報\nDataset Definition：データセットの基本定義\nSpecification：変数の詳細仕様\nADLB_PARAM / ADTTE_PARAM：パラメータ定義表（ADLB、ADTTE用）\nCodelist：コード値の定義\n\nこれらの情報をExcelファイルの複数シートに整理することで、管理しやすく実用的な仕様書が作成できます。"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#specificationシートの構成",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#specificationシートの構成",
    "title": "解析用データセット仕様書",
    "section": "2.3 Specificationシートの構成",
    "text": "2.3 Specificationシートの構成\n変数仕様を記載するSpecificationシートでは、以下の項目を含めることを推奨します：\n\n2.3.1 基本項目\n\nVarnum：変数番号（並び順）\nDomain：データセット名（ADSL、ADLB、ADTTE等）\nVariable Name：変数名\nVariable Label：変数ラベル\nType：データタイプ（Char/Num）\nLength：変数長\nDisplay Format：表示フォーマット\nCodelist：コードリスト参照名\nCore：必須度（Req=必須、Perm=任意、Cond=条件付き）\nDefinition：定義・導出方法\n\n\n\n2.3.2 ADSLのSpecificationシート例\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVarnum\nDomain\nVariable Name\nVariable Label\nType\nLength\nDisplay Format\nCodelist\nCore\nDefinition\n\n\n\n\n1\nADSL\nSTUDYID\nStudy Identifier\nChar\n12\n\n\nReq\nDM.STUDYID\n\n\n2\nADSL\nUSUBJID\nUnique Subject Identifier\nChar\n40\n\n\nReq\nDM.USUBJID\n\n\n3\nADSL\nSUBJID\nSubject Identifier\nChar\n20\n\n\nReq\nDM.SUBJID\n\n\n4\nADSL\nAGE\nAge\nNum\n8\n\n\nReq\nDM.AGE\n\n\n5\nADSL\nSEX\nSex\nChar\n1\n\nSEX\nReq\nDM.SEX\n\n\n6\nADSL\nTRT01P\nPlanned Treatment for Period 1\nChar\n200\n\n\nReq\nARM\n\n\n7\nADSL\nTRT01A\nActual Treatment for Period 1\nChar\n200\n\n\nReq\nACTARM\n\n\n8\nADSL\nTRT01PN\nPlanned Treatment for Period 1 (N)\nNum\n8\n\n\nReq\nDerived from TRT01P\n\n\n9\nADSL\nTRT01AN\nActual Treatment for Period 1 (N)\nNum\n8\n\n\nReq\nDerived from TRT01A\n\n\n10\nADSL\nFASFL\nFull Analysis Set Flag\nChar\n1\n\nNYFL\nReq\nExternal derivation\n\n\n\n\n\n2.3.3 ADLBのSpecificationシート例\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVarnum\nDomain\nVariable Name\nVariable Label\nType\nLength\nDisplay Format\nCodelist\nCore\nDefinition\n\n\n\n\n1\nADLB\nSTUDYID\nStudy Identifier\nChar\n12\n\n\nReq\nDM.STUDYID\n\n\n2\nADLB\nUSUBJID\nUnique Subject Identifier\nChar\n40\n\n\nReq\nDM.USUBJID\n\n\n3\nADLB\nSUBJID\nSubject Identifier\nChar\n20\n\n\nReq\nDM.SUBJID\n\n\n4\nADLB\nAGE\nAge\nNum\n8\n\n\nReq\nDM.AGE\n\n\n5\nADLB\nSEX\nSex\nChar\n1\n\nSEX\nReq\nDM.SEX\n\n\n6\nADLB\nTRT01P\nPlanned Treatment for Period 1\nChar\n200\n\n\nReq\nARM\n\n\n7\nADLB\nTRT01A\nActual Treatment for Period 1\nChar\n200\n\n\nReq\nACTARM\n\n\n8\nADLB\nPARAM\nParameter\nChar\n200\n\n\nReq\nParameter description\n\n\n9\nADLB\nPARAMCD\nParameter Code\nChar\n8\n\n\nReq\nParameter short name\n\n\n10\nADLB\nPARAMN\nParameter (N)\nNum\n8\n\n\nReq\nNumeric representation of PARAM\n\n\n11\nADLB\nAVISIT\nAnalysis Visit\nChar\n200\n\n\nReq\nAnalysis visit description\n\n\n12\nADLB\nAVISITN\nAnalysis Visit (N)\nNum\n8\n\n\nReq\nNumeric representation of AVISIT\n\n\n13\nADLB\nAVAL\nAnalysis Value\nNum\n8\n\n\nReq\nNumeric analysis value\n\n\n14\nADLB\nAVALC\nAnalysis Value (C)\nChar\n200\n\n\nCond\nCharacter analysis value\n\n\n15\nADLB\nBASE\nBaseline Value\nNum\n8\n\n\nPerm\nBaseline analysis value\n\n\n16\nADLB\nBASEC\nBaseline Value (C)\nChar\n200\n\n\nPerm\nBaseline character value\n\n\n17\nADLB\nCHG\nChange from Baseline\nNum\n8\n\n\nPerm\nAVAL - BASE\n\n\n18\nADLB\nPCHG\nPercent Change from Baseline\nNum\n8\n\n\nPerm\n((AVAL-BASE)/BASE)*100\n\n\n19\nADLB\nAVALCAT1\nAnalysis Value Category 1\nChar\n200\n\n\nPerm\nCategorization of AVAL\n\n\n20\nADLB\nAVALCAT1N\nAnalysis Value Category 1 (N)\nNum\n8\n\n\nPerm\nNumeric representation of AVALCAT1\n\n\n21\nADLB\nPCHGCAT1\nPercent Chg from Baseline Category 1\nChar\n200\n\n\nPerm\nCategorization of PCHG\n\n\n22\nADLB\nPCHGCAT1N\nPercent Chg from Baseline Category 1 (N)\nNum\n8\n\n\nPerm\nNumeric representation of PCHGCAT1\n\n\n\n\n\n2.3.4 ADTTEのSpecificationシート例\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVarnum\nDomain\nVariable Name\nVariable Label\nType\nLength\nDisplay Format\nCodelist\nCore\nDefinition\n\n\n\n\n1\nADTTE\nSTUDYID\nStudy Identifier\nChar\n12\n\n\nReq\nDM.STUDYID\n\n\n2\nADTTE\nUSUBJID\nUnique Subject Identifier\nChar\n40\n\n\nReq\nDM.USUBJID\n\n\n3\nADTTE\nSUBJID\nSubject Identifier\nChar\n20\n\n\nReq\nDM.SUBJID\n\n\n4\nADTTE\nAGE\nAge\nNum\n8\n\n\nReq\nDM.AGE\n\n\n5\nADTTE\nSEX\nSex\nChar\n1\n\nSEX\nReq\nDM.SEX\n\n\n6\nADTTE\nTRT01P\nPlanned Treatment for Period 1\nChar\n200\n\n\nReq\nARM\n\n\n7\nADTTE\nTRT01A\nActual Treatment for Period 1\nChar\n200\n\n\nReq\nACTARM\n\n\n8\nADTTE\nPARAM\nParameter\nChar\n200\n\n\nReq\nAnalysis parameter description\n\n\n9\nADTTE\nPARAMCD\nParameter Code\nChar\n8\n\n\nReq\nAnalysis parameter short name\n\n\n10\nADTTE\nPARAMN\nParameter (N)\nNum\n8\n\n\nReq\nNumeric representation of PARAM\n\n\n11\nADTTE\nAVAL\nAnalysis Value\nNum\n8\n\n\nReq\nTime to event in days\n\n\n12\nADTTE\nSTARTDT\nTime to Event Origin Date\nNum\n8\ne8601da.\n\nReq\nAnalysis start date\n\n\n13\nADTTE\nADT\nAnalysis Date\nNum\n8\ne8601da.\n\nReq\nEvent or censoring date\n\n\n14\nADTTE\nCNSR\nCensor\nNum\n8\n\n\nReq\nCensoring flag (0=event, 1=censor)\n\n\n15\nADTTE\nEVNTDESC\nEvent or Censoring Description\nChar\n200\n\n\nPerm\nDescription of event or censoring\n\n\n16\nADTTE\nSRCDOM\nSource Data\nChar\n8\n\n\nPerm\nSource domain\n\n\n17\nADTTE\nSRCVAR\nSource Variable\nChar\n8\n\n\nPerm\nSource variable\n\n\n18\nADTTE\nSRCSEQ\nSource Sequence Number\nNum\n8\n\n\nPerm\nSource sequence number"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#paramシート",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#paramシート",
    "title": "解析用データセット仕様書",
    "section": "2.4 PARAMシート",
    "text": "2.4 PARAMシート\nADLB、ADTTEなどのBDS形式データセットでは、パラメータの定義情報を管理するための専用シートを作成します。\n\n2.4.1 ADLB_PARAMシート例\n\n\n\nPARAM\nPARAMCD\nPARAMN\nAVAL\n\n\n\n\nHbA1c(%)\nHBA1C\n1\ninput(LABデータ)\n\n\nBlood glucose (mg/dL)\nGLU\n2\ninput(LABデータ)\n\n\nBUN (mg/dL)\nBUN\n3\ninput(LABデータ)\n\n\nCreatinine (mg/dL)\nCREAT\n4\ninput(LABデータ)\n\n\nALT (U/L)\nALT\n5\ninput(LABデータ)\n\n\n\n\n\n2.4.2 ADTTE_PARAMシート例\n\n\n\n\n\n\n\n\n\nPARAM\nPARAMCD\nPARAMN\nAVAL\n\n\n\n\nWeight/Waist Circumference Loss\nWTWCLOSS\n1\nADT - STARTDT + 1\n\n\nOverall Survival\nOS\n2\nADT - STARTDT + 1\n\n\nProgression Free Survival\nPFS\n3\nADT - STARTDT + 1\n\n\nTime to Progression\nTTP\n4\nADT - STARTDT + 1"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#code-listのsheetについて",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#code-listのsheetについて",
    "title": "解析用データセット仕様書",
    "section": "2.5 Code ListのSheetについて",
    "text": "2.5 Code ListのSheetについて\nProc FormatでFormatを作るvalueとLabelを事前に仕様書に用意しておく。\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo.\nCodelist_Name\nValue\nLabel\nType\n説明\nSynonym\n備考\n\n\n\n\n1\nSEX\nM\nMale\nChar\n男性\n男\n\n\n\n2\nSEX\nF\nFemale\nChar\n女性\n女\n\n\n\n3\nSEX\nU\nUnknown\nChar\n不明\n未知\n\n\n\n4\nSEXN\n1\n男性\nNum\n男性\nMale\n\n\n\n5\nSEXN\n2\n女性\nNum\n女性\nFemale\n\n\n\n6\nNYFL\nY\nYes\nChar\nはい\nあり\n\n\n\n7\nNYFL\nN\nNo\nChar\nいいえ\nなし\n\n\n\n8\nAGEU\nYEARS\nYears\nChar\n年\n年\n\n\n\n9\nAGEU\nMONTHS\nMonths\nChar\n月\nヶ月"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#adlbデータセットの重要な変数",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#adlbデータセットの重要な変数",
    "title": "解析用データセット仕様書",
    "section": "2.6 ADLBデータセットの重要な変数",
    "text": "2.6 ADLBデータセットの重要な変数\n\n2.6.1 解析値とカテゴリ変数\nADLBでは数値解析値（AVAL）に基づいて、様々なカテゴリ変数を作成します：\n\n2.6.1.1 AVALCAT1 / AVALCAT1N\n解析値のカテゴリ化変数で、例えば正常範囲の判定などに使用： - “NORMAL” / “ABNORMAL” - “LOW” / “NORMAL” / “HIGH”\n\n\n2.6.1.2 PCHGCAT1 / PCHGCAT1N\nベースラインからの変化率のカテゴリ化変数： - “&gt;30% increase” / “±30%” / “&gt;30% decrease”\n\n\n\n2.6.2 時点変数（AVISIT / AVISITN）\n解析用の時点定義で、スケジュール通りの時点名： - “Baseline” (AVISITN=0) - “Week 4” (AVISITN=4) - “Week 12” (AVISITN=12) - “End of Treatment” (AVISITN=99)"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#adtteデータセットの詳細",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#adtteデータセットの詳細",
    "title": "解析用データセット仕様書",
    "section": "2.7 ADTTEデータセットの詳細",
    "text": "2.7 ADTTEデータセットの詳細\n\n2.7.1 ADTTEの特徴\nADTTEは生存時間解析用のデータセットで、以下の特徴があります：\n\n1被験者1パラメータにつき1レコードの構造\n事象の発生時間または打ち切り時間を記録\n複数の解析エンドポイントを1つのデータセットで管理\n\n\n\n2.7.2 ADTTE作成時の重要な考慮事項\n\n2.7.2.1 1. 事象の優先順位\n複数の事象や打ち切りが同日に発生した場合の優先順位を明確に定義する必要があります。\n\n\n2.7.2.2 2. AVALの計算方法\nAVALは通常、以下の式で計算されます：\nここで： - STARTDT：解析開始日（通常は治療開始日：ADSL.TRTSDT） - ADT：解析日（事象発生日または打ち切り日） - +1：0日目を避けるための調整\n\n\n2.7.2.3 3. 事象データの取得元\n\n事象発生：ADVS.ADTから日付を取得\n打ち切り：ADSL.EOSDTから日付を取得\n\n\n\n2.7.2.4 4. CNSRフラグの設定\n\nCNSR=0：事象発生\nCNSR=1：打ち切り\n\n\n\n2.7.2.5 5. ソースデータの追跡（SRCDOM, SRCVAR, SRCSEQ）\nADTTEでは元データの追跡可能性が重要です：\n事象採用時： - SRCDOM=“ADVS” - SRCVAR=“ADT” - SRCSEQ=ADVS.ASEQ\n打ち切り時： - SRCDOM=“ADSL” - SRCVAR=“EOSDT” - SRCSEQ=（ブランク）\n\n\n\n2.7.3 ADTTEプログラミングのポイント\n\nデータセットの組み合わせ：複数のデータセット（ADSL、ADVS等）を適切に結合\n日付の妥当性チェック：論理的に矛盾する日付の検出と処理\n追跡可能性の確保：各レコードの導出元を明確に記録\n複数事象の処理：同一被験者で複数の解析パラメータを生成"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#データセット作成プロセス",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#データセット作成プロセス",
    "title": "解析用データセット仕様書",
    "section": "2.8 データセット作成プロセス",
    "text": "2.8 データセット作成プロセス\n\n2.8.1 仕様書に基づくプログラム作成\n解析用データセット仕様書を読み込み、Specificationシートの定義に従ってSASプログラムを作成します。ADxx_Conversionシートを参照してパラメータ変換処理も実装します。まずは、プロトコール、統計解析計画書、図表計画書、Rawデータ/SDTM仕様書を基に、解析用データ仕様書をテンプレートを基に手作業で作成する。ここは泥臭いが、手作業でやるしかない。\n\n\n2.8.2 チェックリストの作成\n解析用データセット仕様書について、事前に組織内で規定されているチェックリストに基づいて、目視チェックをする。ここが人的作業であるためミスが起こりえる！"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット仕様書.html#section",
    "href": "posts/statistics/2025/解析用データセット仕様書.html#section",
    "title": "解析用データセット仕様書",
    "section": "2.9 ",
    "text": "2.9"
  },
  {
    "objectID": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html",
    "href": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html",
    "title": "SAS：要約統計量作成マクロ",
    "section": "",
    "text": "第3回：演題4「要約統計量マクロ」坂尻大樹さん\n\n要約統計量マクロ\n\n\n/* 疑似データの作成 */\ndata AAA;\n   input Name $ Age Height Treatment $;\n   datalines;\nアルフレッド 14 69.2 A群\nアリス 13 56.8 B群\nバーバラ 13 65.5 A群\nキャロル 14 62.9 B群\nヘンリー 14 63.7 A群\nジェームズ 12 . B群\nジェーン 12 59.9 A群\nジャネット 15 62.7 B群\nジェフリー 13 62.6 A群\nジョン 12 59.1 B群\n;\nrun;\n\n/* 分析設定 */\n%let DS = AAA;\n%let VAR = HEIGHT;\n%let CVAR = TREATMENT;\n\n/* ステップ1: 原データから桁数を取得 */\ndata SUMMARY_KETA1;\n   set &DS.;\n   if &VAR. ^= . then KETA = length(cats(&VAR. - int(&VAR.)));\nrun;\n\nproc summary data=SUMMARY_KETA1;\n   var KETA;\n   output out=SUMMARY_KETA2 max=MAXKETA;\nrun;\n\ndata SUMMARY_KETA3;\n   set SUMMARY_KETA2;\n   if MAXKETA = 1 then KETA = 0;  /* 小数点第0桁=整数値 */\n   else KETA = MAXKETA - 2;        /* 小数点第X桁を格納 */\n   \n   call symputx(\"KETA\", KETA);\nrun;\n\n/* ステップ2: クラスデータを作成 */\ndata SUMMARY_CLASS;\n   length &CVAR. $8.;\n   &CVAR. = \"A群\"; output;\n   &CVAR. = \"B群\"; output;\nrun;\n\n/* ステップ3: 要約統計量を算出 */\nproc summary data=&DS. classdata=SUMMARY_CLASS exclusive nway;\n   class &CVAR.;\n   var &VAR.;\n   output out=SUMMARY_1 N= MEAN= STD= MIN= MEDIAN= MAX= NMISS= Q1= Q3= /autoname;\nrun;\n\ndata SUMMARY_2;\n   length PREOUT1-PREOUT9 $12.;\n   set SUMMARY_1;\n   \n   /* 表示桁数の設定 */\n   PREOUT1 = cats(&VAR._N);\n   \n   /* 平均値 */\n   if &VAR._MEAN ^= . then \n       PREOUT2 = cats(put(round(&VAR._MEAN, 0.1), 8.1));\n   else \n       PREOUT2 = \"-\";\n   \n   /* 標準偏差 */\n   if &VAR._STD ^= . then \n       PREOUT3 = cats(put(round(&VAR._STD, 0.01), 8.2));\n   else \n       PREOUT3 = \"-\";\n   \n   /* 最小値 */\n   if &VAR._MIN ^= . then \n       PREOUT4 = cats(put(round(&VAR._MIN, 1), 8.0));\n   else \n       PREOUT4 = \"-\";\n       \n   /* 第一四分位値 */\n   if &VAR._Q1 ^= . then \n       PREOUT5 = cats(put(round(&VAR._Q1, 0.1), 8.1));\n   else \n       PREOUT5 = \"-\";\n       \n   /* 中央値 */\n   if &VAR._MEDIAN ^= . then \n       PREOUT6 = cats(put(round(&VAR._MEDIAN, 0.1), 8.1));\n   else \n       PREOUT6 = \"-\";\n       \n   /* 第三四分位値 */\n   if &VAR._Q3 ^= . then \n       PREOUT7 = cats(put(round(&VAR._Q3, 0.1), 8.1));\n   else \n       PREOUT7 = \"-\";\n       \n   /* 最大値 */\n   if &VAR._MAX ^= . then \n       PREOUT8 = cats(put(round(&VAR._MAX, 1), 8.0));\n   else \n       PREOUT8 = \"-\";\n       \n   /* 欠測数 */\n   PREOUT9 = cats(&VAR._NMISS);\n   \n   /* ID変数 */\n   ID + 1;\n   keep ID &CVAR. PREOUT1-PREOUT9;\nrun;\n\n/* ステップ4: 整形&出力 */\nproc transpose data=SUMMARY_2 out=SUMMARY_3 (drop=_NAME_)\n   prefix=OUT;\n   var PREOUT1-PREOUT9;\n   id ID;\nrun;\n\n/* 統計項目ラベルを追加 */\ndata SUMMARY_FINAL;\n   length STAT_ITEM $12.;\n   set SUMMARY_3;\n   select(_N_);\n       when(1) STAT_ITEM = \"症例数\";\n       when(2) STAT_ITEM = \"平均値\";\n       when(3) STAT_ITEM = \"標準偏差\";\n       when(4) STAT_ITEM = \"最小値\";\n       when(5) STAT_ITEM = \"第一四分位値\";\n       when(6) STAT_ITEM = \"中央値\";\n       when(7) STAT_ITEM = \"第三四分位値\";\n       when(8) STAT_ITEM = \"最大値\";\n       when(9) STAT_ITEM = \"欠測数\";\n   end;\nrun;\n\n/* 結果出力 */\nproc print data=SUMMARY_FINAL noobs;\n   title \"統計量サマリー（&CVAR.別 - &VAR.）\";\n   var STAT_ITEM OUT1 OUT2;\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#大阪sas-user総会にて紹介されていたプログラムの紹介",
    "href": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#大阪sas-user総会にて紹介されていたプログラムの紹介",
    "title": "SAS：要約統計量作成マクロ",
    "section": "",
    "text": "第3回：演題4「要約統計量マクロ」坂尻大樹さん\n\n要約統計量マクロ\n\n\n/* 疑似データの作成 */\ndata AAA;\n   input Name $ Age Height Treatment $;\n   datalines;\nアルフレッド 14 69.2 A群\nアリス 13 56.8 B群\nバーバラ 13 65.5 A群\nキャロル 14 62.9 B群\nヘンリー 14 63.7 A群\nジェームズ 12 . B群\nジェーン 12 59.9 A群\nジャネット 15 62.7 B群\nジェフリー 13 62.6 A群\nジョン 12 59.1 B群\n;\nrun;\n\n/* 分析設定 */\n%let DS = AAA;\n%let VAR = HEIGHT;\n%let CVAR = TREATMENT;\n\n/* ステップ1: 原データから桁数を取得 */\ndata SUMMARY_KETA1;\n   set &DS.;\n   if &VAR. ^= . then KETA = length(cats(&VAR. - int(&VAR.)));\nrun;\n\nproc summary data=SUMMARY_KETA1;\n   var KETA;\n   output out=SUMMARY_KETA2 max=MAXKETA;\nrun;\n\ndata SUMMARY_KETA3;\n   set SUMMARY_KETA2;\n   if MAXKETA = 1 then KETA = 0;  /* 小数点第0桁=整数値 */\n   else KETA = MAXKETA - 2;        /* 小数点第X桁を格納 */\n   \n   call symputx(\"KETA\", KETA);\nrun;\n\n/* ステップ2: クラスデータを作成 */\ndata SUMMARY_CLASS;\n   length &CVAR. $8.;\n   &CVAR. = \"A群\"; output;\n   &CVAR. = \"B群\"; output;\nrun;\n\n/* ステップ3: 要約統計量を算出 */\nproc summary data=&DS. classdata=SUMMARY_CLASS exclusive nway;\n   class &CVAR.;\n   var &VAR.;\n   output out=SUMMARY_1 N= MEAN= STD= MIN= MEDIAN= MAX= NMISS= Q1= Q3= /autoname;\nrun;\n\ndata SUMMARY_2;\n   length PREOUT1-PREOUT9 $12.;\n   set SUMMARY_1;\n   \n   /* 表示桁数の設定 */\n   PREOUT1 = cats(&VAR._N);\n   \n   /* 平均値 */\n   if &VAR._MEAN ^= . then \n       PREOUT2 = cats(put(round(&VAR._MEAN, 0.1), 8.1));\n   else \n       PREOUT2 = \"-\";\n   \n   /* 標準偏差 */\n   if &VAR._STD ^= . then \n       PREOUT3 = cats(put(round(&VAR._STD, 0.01), 8.2));\n   else \n       PREOUT3 = \"-\";\n   \n   /* 最小値 */\n   if &VAR._MIN ^= . then \n       PREOUT4 = cats(put(round(&VAR._MIN, 1), 8.0));\n   else \n       PREOUT4 = \"-\";\n       \n   /* 第一四分位値 */\n   if &VAR._Q1 ^= . then \n       PREOUT5 = cats(put(round(&VAR._Q1, 0.1), 8.1));\n   else \n       PREOUT5 = \"-\";\n       \n   /* 中央値 */\n   if &VAR._MEDIAN ^= . then \n       PREOUT6 = cats(put(round(&VAR._MEDIAN, 0.1), 8.1));\n   else \n       PREOUT6 = \"-\";\n       \n   /* 第三四分位値 */\n   if &VAR._Q3 ^= . then \n       PREOUT7 = cats(put(round(&VAR._Q3, 0.1), 8.1));\n   else \n       PREOUT7 = \"-\";\n       \n   /* 最大値 */\n   if &VAR._MAX ^= . then \n       PREOUT8 = cats(put(round(&VAR._MAX, 1), 8.0));\n   else \n       PREOUT8 = \"-\";\n       \n   /* 欠測数 */\n   PREOUT9 = cats(&VAR._NMISS);\n   \n   /* ID変数 */\n   ID + 1;\n   keep ID &CVAR. PREOUT1-PREOUT9;\nrun;\n\n/* ステップ4: 整形&出力 */\nproc transpose data=SUMMARY_2 out=SUMMARY_3 (drop=_NAME_)\n   prefix=OUT;\n   var PREOUT1-PREOUT9;\n   id ID;\nrun;\n\n/* 統計項目ラベルを追加 */\ndata SUMMARY_FINAL;\n   length STAT_ITEM $12.;\n   set SUMMARY_3;\n   select(_N_);\n       when(1) STAT_ITEM = \"症例数\";\n       when(2) STAT_ITEM = \"平均値\";\n       when(3) STAT_ITEM = \"標準偏差\";\n       when(4) STAT_ITEM = \"最小値\";\n       when(5) STAT_ITEM = \"第一四分位値\";\n       when(6) STAT_ITEM = \"中央値\";\n       when(7) STAT_ITEM = \"第三四分位値\";\n       when(8) STAT_ITEM = \"最大値\";\n       when(9) STAT_ITEM = \"欠測数\";\n   end;\nrun;\n\n/* 結果出力 */\nproc print data=SUMMARY_FINAL noobs;\n   title \"統計量サマリー（&CVAR.別 - &VAR.）\";\n   var STAT_ITEM OUT1 OUT2;\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#各プログラムの工夫点",
    "href": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#各プログラムの工夫点",
    "title": "SAS：要約統計量作成マクロ",
    "section": "2 各プログラムの工夫点",
    "text": "2 各プログラムの工夫点"
  },
  {
    "objectID": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#桁数自動取得の工夫",
    "href": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#桁数自動取得の工夫",
    "title": "SAS：要約統計量作成マクロ",
    "section": "3 1. 桁数自動取得の工夫",
    "text": "3 1. 桁数自動取得の工夫\ndata SUMMARY_KETA1;\n    set &DS.;\n    if &VAR. ^= . then KETA = length(cats(&VAR. - int(&VAR.)));\nrun;\n工夫点:\n\n小数点以下の桁数を自動検出: &VAR. - int(&VAR.)で小数点以下のみを抽出\n文字列変換による桁数計算: cats()で文字列に変換し、length()で桁数を取得\n例: 62.75の場合 → 62.75 - 62 = 0.75 → “0.75” → length = 4文字 → 小数点第2位\n\ndata SUMMARY_KETA3;\n    set SUMMARY_KETA2;\n    if MAXKETA = 1 then KETA = 0;  /* 小数点第0桁=整数値 */\n    else KETA = MAXKETA - 2;        /* 小数点第X桁を格納 */\n    \n    call symputx(\"KETA\", KETA);\nrun;\n工夫点:\n\n整数判定: MAXKETA=1は”0”（整数）を意味するため、KETA=0に設定\n桁数計算: “0.75”なら4文字-2=“.”と”0”を除く=2桁\nマクロ変数への格納: call symputx()でグローバルマクロ変数に設定"
  },
  {
    "objectID": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#クラスデータ作成の工夫",
    "href": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#クラスデータ作成の工夫",
    "title": "SAS：要約統計量作成マクロ",
    "section": "4 2. クラスデータ作成の工夫",
    "text": "4 2. クラスデータ作成の工夫\n\n4.1 数値変数（年齢）の場合:\ndata SUMMARY_CLASS;\n    if _N_ = 0 then set &DS.(keep=&CVAR.);\n    do &CVAR. = &CSTR. to &CEND.;\n        output;\n    end;\n    stop;\nrun;\n工夫点:\n\n変数定義の継承: if _N_ = 0 then setで元データの変数属性を継承\n連続値の生成: doループで指定範囲の全ての値を生成\n効率的な処理: stopで無限ループを防止\n\n\n\n4.2 文字変数（治療群）の場合:\ndata SUMMARY_CLASS;\n    length &CVAR. $8.;\n    &CVAR. = \"A群\"; output;\n    &CVAR. = \"B群\"; output;\nrun;\n工夫点:\n\n文字変数の明示的定義: lengthで文字変数の長さを指定\n必要な値のみ生成: 存在する治療群のみを明示的に作成"
  },
  {
    "objectID": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#統計量計算での工夫",
    "href": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#統計量計算での工夫",
    "title": "SAS：要約統計量作成マクロ",
    "section": "5 3. 統計量計算での工夫",
    "text": "5 3. 統計量計算での工夫\nproc summary data=&DS. classdata=SUMMARY_CLASS exclusive nway;\n    class &CVAR.;\n    var &VAR.;\n    output out=SUMMARY_1 N= MEAN= STD= MIN= MEDIAN= MAX= NMISS= Q1= Q3= /autoname;\nrun;\n工夫点:\n\nCLASSDATA使用: 存在しないクラス値も強制的に出力（0件でも表示）\nEXCLUSIVE: 元データにないクラス値も処理対象に含める\nNWAY: 最下位レベルのクロス集計のみ出力\nAUTONAME: 変数名を自動生成（HEIGHT_N, HEIGHT_MEAN等）"
  },
  {
    "objectID": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#数値フォーマットの工夫",
    "href": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#数値フォーマットの工夫",
    "title": "SAS：要約統計量作成マクロ",
    "section": "6 4. 数値フォーマットの工夫",
    "text": "6 4. 数値フォーマットの工夫\nif &VAR._MEAN ^= . then \n    PREOUT2 = cats(put(round(&VAR._MEAN, 0.1), 8.1));\nelse \n    PREOUT2 = \"-\";\n工夫点:\n\n統計量別の桁数制御:\n\n平均値: 小数点第1位（0.1で四捨五入、8.1で表示）\n標準偏差: 小数点第2位（0.01で四捨五入、8.2で表示）\n最小値・最大値: 整数（1で四捨五入、8.0で表示）\n\n欠損値の統一処理: 欠損値は全て”-“で表示\n文字列への変換: cats()で余分な空白を除去"
  },
  {
    "objectID": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#データ構造変換の工夫",
    "href": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#データ構造変換の工夫",
    "title": "SAS：要約統計量作成マクロ",
    "section": "7 5. データ構造変換の工夫",
    "text": "7 5. データ構造変換の工夫\nproc transpose data=SUMMARY_2 out=SUMMARY_3 (drop=_NAME_)\n    prefix=OUT;\n    var PREOUT1-PREOUT9;\n    id ID;\nrun;\n工夫点:\n\n行列の転置: 統計量（行）×群（列）の表形式に変換\n列名の制御: prefix=OUTで列名をOUT1, OUT2…に統一\n不要変数の削除: drop=_NAME_で転置時の不要変数を除去"
  },
  {
    "objectID": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#最終出力での工夫",
    "href": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#最終出力での工夫",
    "title": "SAS：要約統計量作成マクロ",
    "section": "8 6. 最終出力での工夫",
    "text": "8 6. 最終出力での工夫\ndata SUMMARY_FINAL;\n    length STAT_ITEM $12.;\n    set SUMMARY_3;\n    select(_N_);\n        when(1) STAT_ITEM = \"症例数\";\n        when(2) STAT_ITEM = \"平均値\";\n        /* ... */\n    end;\nrun;\n工夫点:\n\n日本語ラベル: 統計量に分かりやすい日本語名を付与\n順序の制御: select(_N_)で行番号に基づいた処理\n表示用の最終調整: ユーザーフレンドリーな出力形式"
  },
  {
    "objectID": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#マクロ設計の工夫",
    "href": "posts/statistics/2025/要約統計量マクロ_大阪SAS_坂尻さん.html#マクロ設計の工夫",
    "title": "SAS：要約統計量作成マクロ",
    "section": "9 7. マクロ設計の工夫",
    "text": "9 7. マクロ設計の工夫\n%let DS = AAA;\n%let VAR = HEIGHT;\n%let CVAR = AGE;  /* または Treatment *\n工夫点:\n\nパラメータ化: データセット名、変数名を簡単に変更可能\n汎用性: 数値・文字変数どちらでも対応\n再利用性: マクロ変数を変更するだけで異なる分析に適用可能\n\nこれらの工夫により、元の複雑な%SUMMARYマクロの機能を再現しつつ、理解しやすく保守しやすいコードになっています。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_09_否定表現.html",
    "href": "posts/statistics/2025/英文法_09_否定表現.html",
    "title": "否定表現",
    "section": "",
    "text": "否定表現は多彩です。not/never/noといった定番の否定表現だけでなく、種々の否定表現のバリエーションを身に付けることにより、奥行きのある文章を書けるようになります。\nこの章では、否定表現の基礎知識として、普通の否定、強い否定のほかに、準否定・部分否定・二重否定などを説明します。また、論文でも用いられる、否定語を用いない否定的表現、否定の意味を添える接頭語について解説します。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_09_否定表現.html#否定表現",
    "href": "posts/statistics/2025/英文法_09_否定表現.html#否定表現",
    "title": "否定表現",
    "section": "",
    "text": "否定表現は多彩です。not/never/noといった定番の否定表現だけでなく、種々の否定表現のバリエーションを身に付けることにより、奥行きのある文章を書けるようになります。\nこの章では、否定表現の基礎知識として、普通の否定、強い否定のほかに、準否定・部分否定・二重否定などを説明します。また、論文でも用いられる、否定語を用いない否定的表現、否定の意味を添える接頭語について解説します。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_09_否定表現.html#否定表現の基礎知識",
    "href": "posts/statistics/2025/英文法_09_否定表現.html#否定表現の基礎知識",
    "title": "否定表現",
    "section": "2 否定表現の基礎知識",
    "text": "2 否定表現の基礎知識\n普通の否定にはnotが用いられます。強い否定にはnever,noが用いられます。そのほかに、準否定、部分否定、二重否定があります。\n\n2.1 準否定\n準否定語には以下があります。\n\nseldom/rarely：めったに～ない【頻度】\nhardly/scarcely：ほとんど～ない【頻度】\nfew/little：ほとんど～ない【数量】\n\nなお、barelyは「かろうじて～する」、「どういか～する」という意味で肯定です。\n例：Only a few these therapies are barely translated into clinical practices.\n\n\n2.2 部分否定\nall,both always,necessarily,completely,entirelyなどの前にnotがあると、部分否定になる。\n\nnot all：すべてが～というわけではない\nnot always：いつも～というわけではない\nnot necessarily：必ずしも～というわけではない\n\n例：Not all of the studies detailed how caries was defined\n\n\n2.3 二重否定\n二重否定は弱い肯定の意味になります。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_09_否定表現.html#論文における否定表現",
    "href": "posts/statistics/2025/英文法_09_否定表現.html#論文における否定表現",
    "title": "否定表現",
    "section": "3 論文における否定表現",
    "text": "3 論文における否定表現\n\n3.1 否定語は文中のなるべく前の方に置く\n\n\n3.2 否定語を用いない否定表現\n\n\n3.3 否定の意味を添える接頭語"
  },
  {
    "objectID": "posts/statistics/2025/英文法_07_前置詞.html",
    "href": "posts/statistics/2025/英文法_07_前置詞.html",
    "title": "前置詞",
    "section": "",
    "text": "前置詞は、通常は名詞や代名詞の前に置かれ、場所・空間、時間などのほか、さまざまな意味を添えて、語句と語句を繋げる役割がある。1つの前置詞が多様な意味を持つことが多く、使い分けが難しい。論文における前置詞の用例をみよう"
  },
  {
    "objectID": "posts/statistics/2025/英文法_07_前置詞.html#前置詞",
    "href": "posts/statistics/2025/英文法_07_前置詞.html#前置詞",
    "title": "前置詞",
    "section": "",
    "text": "前置詞は、通常は名詞や代名詞の前に置かれ、場所・空間、時間などのほか、さまざまな意味を添えて、語句と語句を繋げる役割がある。1つの前置詞が多様な意味を持つことが多く、使い分けが難しい。論文における前置詞の用例をみよう"
  },
  {
    "objectID": "posts/statistics/2025/英文法_07_前置詞.html#前置詞の基礎知識",
    "href": "posts/statistics/2025/英文法_07_前置詞.html#前置詞の基礎知識",
    "title": "前置詞",
    "section": "2 前置詞の基礎知識",
    "text": "2 前置詞の基礎知識\n\n2.1 （1）前置詞の意味別一覧\n\n\n2.2 （2）場所・空間を表す主な前置詞\n\n位置\n\natは，広がりのない地点を示します。\n\n例：Thousands of patients with trauma were seen at a large hospital in Tokyo.\n\ninは、広がりのある場所や建物などの内部を示します。\n\n例：We recorded treatments that patients received in hospital.\n\nonは、面に接している状態を示します。\n\n例：on the road, writing on the wall\n\n\n上下\n\noverは、「～の上」であり、接していても離れていても用いられます。\nunderは、「～の下」であり、接していても離れていても用いられます。\nupは、「上に向かって」を意味する。\n\ngo up the stairs：階段を上る\n\ndownは、「下に向かって」を意味する。\n\nwalk down the slope：斜面を下る\n\n前置詞としてのaboveは、「～の上方」、belowは「～の下方」という意味になる。どちらも副詞としても用いられ、aboveは「上方に」、belowは「下方に」という意味になる。\n\n例：The body mass index classifications are listed below.\n\n\n\n\n\n2.3 （3）時間を表す主な前置詞\n\n時点\n\n時点を表す前置詞には、at,on,inがある。\natは時刻を示す。\nonは曜日、日付などを示す。\ninは月・季節・年・世紀などを示す。\n\n\n始点・終点を表す前置詞には、from, to , by ,before , after ,since ,till\n期間を表す前置詞には、for , during , through , over , within\n\n\n2.4 （4）その他の意味をもつ前置詞\n「～についての」という内容を示す前置詞として、一般的にはaboutが用いられる。論文では、onも用いられる\n\n例\n\na study on ：～についての研究\ndata on ：～についてのデータ\ninformation on：～についての情報"
  },
  {
    "objectID": "posts/statistics/2025/英文法_07_前置詞.html#論文における前置詞",
    "href": "posts/statistics/2025/英文法_07_前置詞.html#論文における前置詞",
    "title": "前置詞",
    "section": "3 論文における前置詞",
    "text": "3 論文における前置詞\n\n3.1 （1）前置詞の使い分け\n\ndie ofとdie from\ncompare toとcompare with\n\ncompare toは、原則として、本質的に異なるものの間に類似点がある場合に、「例える」「～と照らしあわせてみる」という意味で用いられます。\ncompare withは、原則として、本質的に類似しているもの同士をさらに比較・分析する場合に、「比較する」という意味で用いられます。しかし、これらの原則は実際にはあまり守られておらず、compare withの代わりにcompare toが用いられるケースもよくみられている。なお、as compare to/withという形も見られえてる。\n\n例：We aimed to evaluate whether conservative oxygenation would reduce duration of organ support or incidence of death compared to standard care.\n例：A meta-analysis suggested an increased mortality risk for paclitaxel-coated devices compared with uncoated control devices.\n例：We conducted a phase 3 trial of erdafitinib as compared with chemotherapy in patients with metastatic urothelial carcinoma.\n\n\nAとBの関連\n\nA is associated with Bは、論文における頻出表現です。論文のタイトルでは、Association between A and Bという形がよぅみられます。\n\n例：Association between care experience and mental health hospitalization among children in Scotland.\n\nAssociation between A and Bは、「Association of A with B」と言い換えても同義です。Association between A and Bを用いる場合、AまたはBを構成する語句の中にandが入っていると、betweenの後にandが2回出現するため、読みにくくなってしまいます。その場合は、「association of A and B」を用いるとよいでしょう。\n\nAssociation of early life physical and sexual abuse with premature mortality\n\n\n\n\n\n3.2 （2）前置詞を含む成句\n\nofを含む成句\nasを含む成句\ninを含む成句\ntoを含む成句"
  },
  {
    "objectID": "posts/statistics/2025/英文法_05_形容詞と副詞.html",
    "href": "posts/statistics/2025/英文法_05_形容詞と副詞.html",
    "title": "形容詞・副詞",
    "section": "",
    "text": "形容詞は主に名詞を修飾し、副詞は主に動詞や形容詞を修飾することで、文章全体を豊かな内容にすることができます。本章では、最初に形容詞・副詞の基礎知識をおさらいします。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_05_形容詞と副詞.html#形容詞副詞",
    "href": "posts/statistics/2025/英文法_05_形容詞と副詞.html#形容詞副詞",
    "title": "形容詞・副詞",
    "section": "",
    "text": "形容詞は主に名詞を修飾し、副詞は主に動詞や形容詞を修飾することで、文章全体を豊かな内容にすることができます。本章では、最初に形容詞・副詞の基礎知識をおさらいします。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_05_形容詞と副詞.html#形容詞副詞の基礎知識",
    "href": "posts/statistics/2025/英文法_05_形容詞と副詞.html#形容詞副詞の基礎知識",
    "title": "形容詞・副詞",
    "section": "2 形容詞・副詞の基礎知識",
    "text": "2 形容詞・副詞の基礎知識\n\n2.1 形容詞の位置\n\n\n2.2 数量形容詞\n\n\n2.3 副詞の位置\n\n\n2.4 比較級・最大級"
  },
  {
    "objectID": "posts/statistics/2025/英文法_05_形容詞と副詞.html#論文における形容詞副詞",
    "href": "posts/statistics/2025/英文法_05_形容詞と副詞.html#論文における形容詞副詞",
    "title": "形容詞・副詞",
    "section": "3 論文における形容詞・副詞",
    "text": "3 論文における形容詞・副詞\n\n3.1 論文では感情表現を用いない\n論文では、以下のような感情表現を表す形容詞（およびその副詞形）を用いるべきではありません。\n\nabsolute,amazing,astonishing,astounding,…\n\n書き手が勝手に抱いた感情に過ぎず、客観性を欠いているからです。特にResultにおいてこれらを用いるのは不適切です。Discussionでこれらの単語が使われているケースを少ないながら見受けられますが、決して推奨されません。\n\n\n3.2 論文に頻出する形容詞・副詞\n\n有意\n\nsignificant\n\n正確、適切、妥当\n\ncorrect：正確な、（一般原則や基準に合致して）誤りがない\nExact：正確な、（事実や真理に合致して）誤りがない\naccurate：正確な、（注意や努力が払われて）誤りがない\nprecise：正確な、（細部に至るまで）精密な\nappropriate：（目的に合って）適当な、適切な\nadequate：（目的に）足りる、適切な\nValid：妥当な、根拠の確実な\n\n時期を表す形容詞\n\npast,previous,prior\nlatest,recent\ncurrent,present\n\nそれぞれ\nとても重要\n明らかでない\n矛盾しない、一致する。\n矛盾する、相反する、競争の的である。\n同様の\n同時に起こる\n影響を受けやすい\nより～しやすい、より～しにくい"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html",
    "title": "能動態と受動態",
    "section": "",
    "text": "論文では、なるべく能動態を用いる方がよいでしょう。なぜなら、一般的に能動態の方が少ない単語数で表現できるからである。本章ではまず、能動態と受動態の基礎を解説します。次に、能動態を原則とする、論文の文章の書き方を解説する。さらに、論文において受動態を用いてもよい場面、むしろ受動態を用いる方が望ましい場面を説明する。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#能動態",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#能動態",
    "title": "能動態と受動態",
    "section": "",
    "text": "論文では、なるべく能動態を用いる方がよいでしょう。なぜなら、一般的に能動態の方が少ない単語数で表現できるからである。本章ではまず、能動態と受動態の基礎を解説します。次に、能動態を原則とする、論文の文章の書き方を解説する。さらに、論文において受動態を用いてもよい場面、むしろ受動態を用いる方が望ましい場面を説明する。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#受動態の基礎知識",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#受動態の基礎知識",
    "title": "能動態と受動態",
    "section": "2 受動態の基礎知識",
    "text": "2 受動態の基礎知識"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#受動態に変換できない文章",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#受動態に変換できない文章",
    "title": "能動態と受動態",
    "section": "3 受動態に変換できない文章",
    "text": "3 受動態に変換できない文章"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#目的語がthat節の場合",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#目的語がthat節の場合",
    "title": "能動態と受動態",
    "section": "4 目的語がthat節の場合",
    "text": "4 目的語がthat節の場合"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#句動詞を用いた文章の受動態",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#句動詞を用いた文章の受動態",
    "title": "能動態と受動態",
    "section": "5 句動詞を用いた文章の受動態",
    "text": "5 句動詞を用いた文章の受動態"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#第4文型の受動態",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#第4文型の受動態",
    "title": "能動態と受動態",
    "section": "6 第4文型の受動態",
    "text": "6 第4文型の受動態"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#論文における能動態と受動態",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#論文における能動態と受動態",
    "title": "能動態と受動態",
    "section": "7 論文における能動態と受動態",
    "text": "7 論文における能動態と受動態"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#能動態を原則とする",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#能動態を原則とする",
    "title": "能動態と受動態",
    "section": "8 能動態を原則とする。",
    "text": "8 能動態を原則とする。\n論文で能動態を用いると、主語にwe（単著の場合はI）という一人称名詞が使われることが多くなります。現代では、自然科学だけでなく社会科学の論文でも、一人称代名詞の使用が一般的に許容されています。能動態の文章は読みやすく、冗長になりにくいでしょう。また、文の主体が著者自身であることが分かり、論文における著者の見解や貢献をたやすく理解できます。\n論文においてweが主語になる場合は以下の動詞がよく用いられます。\n\nfind, observe , identify , examine , investigate , verify , elucidate , clarify , hypothesize , use , perform , conduct ,analyze , evaluate , include ,exclude , categorize , divide , obtain , show , demonstrate , reveal , believe , consider , realize , assume , estimate , suggest , suppose , speculate"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#受動態を用いてもよい場合",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#受動態を用いてもよい場合",
    "title": "能動態と受動態",
    "section": "9 受動態を用いてもよい場合",
    "text": "9 受動態を用いてもよい場合"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#methodおよびresults",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#methodおよびresults",
    "title": "能動態と受動態",
    "section": "10 MethodおよびResults",
    "text": "10 MethodおよびResults\n実験・観察の方法や結果の記述において、実験者・観察者が著者ら（we ,the authors）であることは自明です。受動態にして、by usやby the authorsを省いてもよいでしょう。特に、能動態の文章の目的語がその文章の中心となる語句である場合、むしろ受動態に変換してその語句を文の先頭に配置した方がよいこともあります。\n例：\n\nThis retrospective cohort study was conducted using a nationwide inpatient database in Japan.\nHospital volume was defined as the number of xxx per year in each hospital , and was categorized into quartiles.\nThe Cochran-Armitage trend test was used to evaluate the change in the proportion of patients undergoing early extubation over the study period.\nThis study was approved by the IRB .\n\n実験者・観察者が著者らでweダルことが自明の場合でも、weを主語とする能動態で書いても問題はない。むしろ能動態にして、主語と動詞をなるべく文章の前に配置する方が望ましいです。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#行為の主体が不明な場合",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#行為の主体が不明な場合",
    "title": "能動態と受動態",
    "section": "11 行為の主体が不明な場合",
    "text": "11 行為の主体が不明な場合"
  },
  {
    "objectID": "posts/statistics/2025/英文法_03_能動態と受動態.html#section",
    "href": "posts/statistics/2025/英文法_03_能動態と受動態.html#section",
    "title": "能動態と受動態",
    "section": "12 ",
    "text": "12"
  },
  {
    "objectID": "posts/statistics/2025/英文法_01_英語表現_冠詞.html#冠詞の基礎知識",
    "href": "posts/statistics/2025/英文法_01_英語表現_冠詞.html#冠詞の基礎知識",
    "title": "冠詞について",
    "section": "2 冠詞の基礎知識",
    "text": "2 冠詞の基礎知識\n\n可算名詞と不可算名詞\n\n可算名詞の分類\n\n普通名詞\n\n例：Patient\n\n集合名詞\n\n例：family\n\n\n不可算名詞\n\n物質名詞\n\n例：blood\n\n抽象名詞\n\n例：chemo-therapy\n概念を表すときは、不可算名詞になるが、個々の具体例を表す場合は可算名詞にもなりえる。\n\n固有名詞\n\n例：Japan\n\n\n\n不定冠詞と定冠詞\n\n不定冠詞はa[an]は可算名詞につけられる。\n定冠詞theは、可算名詞と、不可算名詞のうち物質名詞・抽出名詞につけられる。\n固有名詞は、通常、無冠詞である。\n\n\n可算名詞にも不可算名詞にもなる名詞がある。例えば、a stoneは「石」（可算名詞の普通名詞）であるが、無冠詞のstoneは「石材」（不可算名詞の物質名詞）となる。 a paperは「新聞」または「論文」（可算名詞の普通名詞）、無冠詞のpaperは「紙」（不可算名詞の物質名詞）となる。また、 a roomは「部屋」（可算名詞の普通名詞）、無冠詞のroomは「余地」（不可算名詞の抽象名詞）となる。\n例： This house is made of stone. （stoneは無可算名詞の物質を表す）\n例：A rolling stone gathers no moss （a stoneは可算名詞の普通名詞なので冠詞がいる）"
  },
  {
    "objectID": "posts/statistics/2025/英文法_01_英語表現_冠詞.html#不定冠詞と定冠詞",
    "href": "posts/statistics/2025/英文法_01_英語表現_冠詞.html#不定冠詞と定冠詞",
    "title": "冠詞について",
    "section": "3 不定冠詞と定冠詞",
    "text": "3 不定冠詞と定冠詞\n\n3.1 不定冠詞と定冠詞の基礎\n可算名詞である場合、常に冠詞 or 定冠詞は必要であるが、名詞の前に以下がある場合は冠詞が省略される。\n\n指示代名詞（this / that / these / those）\n不定代名詞（some / any / each / another / either / no / neither )\n人称代名詞の所有格（ my / our / your / his / her / its / their / whose）\n\nしかし、原則として、不定冠詞a[an]は不特定の事柄、定冠詞theは、特定の事柄につけます。つまり、a[an]は、「たくさんのあるもののうちの一つ（one of many）」で、the は「唯一無二（one and only）」を示す。\n原則として、初めて話題に上がる単数の可算名詞には、a[an]がついて、複数形の場合は、無冠詞となる。\nThe をつけるのは、以下のような場合である。\n\n同じものが再登場した場合\nその場の状況から初出であっても特定できる場合\n修飾語句がついて特定できる場合\n唯一無の存在\n必ずtheがつく単語の場合（the sameなど）\n\n例：There is a clinic around the corner.\n解説：clinicは初出なので、単数の可算名詞なのでaをつけます。cornerは初出であっても、clinicの角は一意に定まるので（会話の中でも）one and onlyということで、theをつけています。なお、 a clinic is around the conerは文法的にOKですが使われません。注意しましょう。既にclinicも話題に上がっていれば、There is the clinic around the cornerが正しそうにめますが、これは言いません。その場合は、The clinic is around the corner.が正しい表現です。英語は難しいですね。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_01_英語表現_冠詞.html#総称用法",
    "href": "posts/statistics/2025/英文法_01_英語表現_冠詞.html#総称用法",
    "title": "冠詞について",
    "section": "4 総称用法",
    "text": "4 総称用法\n不定冠詞の総称用法とは、不定冠詞を用いて不特定の個体を想定し、それを代表例として登場させ、その種が一般的にもつ特性を述べる用法です。「～というもの」となる。\n例：A cat is a pet animal\n訳：猫（というもの）は愛玩動物である。\nところが、主語ではA catでいいが、目的語では使いません。目的語の場合は catsとして、I like cats.が正しい表現である。\nまた、個体全体をイメージして論じる場合は、aではなく定冠詞theの総称用法を用います。\n例：The chimpanzee is listed as dangered.\n\n4.1 唯一無二のthe\n地球や月は唯一無二のため、定冠詞をつけて、the earth、the moonとします。ただし、The をつけずに、University of Okayamaとしても文法上としては正しいです。なので、The をつけてもいいし、つけなくてもいい。難しいですね。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_01_英語表現_冠詞.html#論文における冠詞",
    "href": "posts/statistics/2025/英文法_01_英語表現_冠詞.html#論文における冠詞",
    "title": "冠詞について",
    "section": "5 論文における冠詞",
    "text": "5 論文における冠詞\n\n5.1 論文における可算名詞と不可算名詞の区別\n\nresearchとstudy\n\nどちらも「研究」という意味があります。researchは常に不可算名詞であるので、researchesのように複数形になることはありません。なので、 a researchのように不定冠詞をつけることも誤りになります。Studyは「勉強」、「学問」という意味では不可算名詞になりますが、「研究」という意味では可算名詞です。a studyは「1つの研究」、studiesは「複数の研究です」\n例：Previous research showed that physical activity may be effective in reducing burnout.\n例：Numerous studies showed that treatment with hypertension can reduce the risk of cardiovascular disease.\n\n\n5.2 data,criteria,index\ndataは、datnumという可算名詞の複数形です。なお、data-baseは可算名詞であり、a database、databasesが正しいです。\n例：Data were collected from a large inpatient database.\ncriteriaはCriterionという可算名詞の複数形です。\n例：The exclusion criteria were as follows: recent cardiopulmonary resuscitation; pregnancy.\n\n\n5.3 抽象名詞の可算・不可算\n以下の名詞は常に不可算となります。\n\nevidence\ninformation\nknowledge\nprogress\nadvice\ndamage\nharm\npresence\nabsence\nsuperiority\ninferiority\nsmoking\nseverity\n\n概念を表すときは不可算名詞、個々の具体例を示す場合には可算名詞になる抽象名詞は、論文ではかなり多く認められます。\n例えば、recordは、「記録すること」を意味する場合は不可算名詞、「個々の具体的な記録」を表す場合は可算名詞（records）です。\n例：Lipid profile was obtained from electronic medical records.\n他にも、surgeryは通常不可算名詞で、「外科」を示します。しかし、「個々の手術や術式」を示す場合は可算名詞（a surgery, surgeries）となる。\n例：Preserving elective surgeries was difficult in the Covid-19 pandemic.\n以下の名詞も同様に、通常は不可算となるが、個々の具体例を示す場合は可算名詞になることもある。\n\ndisease、diagnosis、treatment\n\n\n\n5.4 病名、症状、兆候、薬品名\n「疾患・病気」を表すdisaease、illnessという単語は、\n動詞の名詞系は通お嬢、不可算名詞です。例えば、initiation、recommendation、selection。しかし、一部は、概念を表す場合は不可算名詞となる。例えば、choice、investigation、onservation\n\n疫学・統計用語の多くは可算名詞です。\n\nhypothesis\nmean\nstandard deviation\nmedian\nrate\nproportion\ntest\nodds ratio\n\n不可算名詞もある\n\neffectiveness\nefficacy\nmatching"
  },
  {
    "objectID": "posts/statistics/2025/英文法_01_英語表現_冠詞.html#ネイティブチェックに引っかかった冠詞の例",
    "href": "posts/statistics/2025/英文法_01_英語表現_冠詞.html#ネイティブチェックに引っかかった冠詞の例",
    "title": "冠詞について",
    "section": "6 ネイティブチェックに引っかかった冠詞の例",
    "text": "6 ネイティブチェックに引っかかった冠詞の例\n康永先生は、大学で指導している研究者らが論文を執筆したときは、必ず、英文校正会社に原稿のネイティブチェックを依頼しているそうです。その例を示す。\n例①\n\n修正前：We applied self-controlled case series (SCCS) design to evaluate change in composite outcome of hospital admissions and emergency department visits.\n修正後：We applied a self-controlled case series (SCCS) design to evaluate changes in the composite outcome of hospital admissions and emergency department visits.\n\nここで、design、change、outcomeはいずれも可算名詞であるので、aやtheをつける必要がある。designは初出の単数の可算名詞なのでaをつけている。changeは複数形にして、初出なので無冠詞とした。outdomeについては修飾語であるcompositeがついているので、特定できる、ということでtheをつけている。むず！！\n例②\n\n修正前：Baseline characteristics of eligible patients for SCCS are presented in Table1.\n修正後：The baseline characteristics of eligible patients for the SCCS are presented in Table1.\n\nここで、CharacteristicsにはBaselineという修飾語がついているので、特定可能となるためTheをつけている。SCCSについては、すでにMethodで説明しているので、特定可能なのでTheとしている。\n例③\n\n修正前：All groups had significant decline in the composite outcome in subgroup analyses.\n修正後：All groups had a significant decline in the composite outcome in the subgroup\n\ndeclineについては、この場合は可算名詞なので、aをつけている。subgroup analyesesはMethodsで説明済なので、theをつけている。\n例④\n\n修正前：P value &lt; 0.05 was considered statistically significant.\n修正後：A p value &lt; 0.05 was considered statistically significant.\n\nvalueは「価値」という抽象名詞で不可算になりますが、ここでは「数値」という意味で可算名詞になります。\n例⑤\n\nThe patients backgrounds are shown in Table1.\n\n論文の図表において、Table1, Figure1などは大文字で表記します。これらは本文中で初出であっても特定できます。Theをつけたくなりますが、慣習上、theをつける必要はありません。"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験の中間事象.html",
    "href": "posts/statistics/2025/臨床試験の中間事象.html",
    "title": "臨床試験の中間事象",
    "section": "",
    "text": "中間事象を扱う視点として、当該事象は治療の一部か、アウトカムの一部か、頻度は群間で異なるか、当該事象が生じた理由は何か、などが重要である。"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験の中間事象.html#臨床試験の中間事象",
    "href": "posts/statistics/2025/臨床試験の中間事象.html#臨床試験の中間事象",
    "title": "臨床試験の中間事象",
    "section": "",
    "text": "中間事象を扱う視点として、当該事象は治療の一部か、アウトカムの一部か、頻度は群間で異なるか、当該事象が生じた理由は何か、などが重要である。"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験の中間事象.html#中間事象とは",
    "href": "posts/statistics/2025/臨床試験の中間事象.html#中間事象とは",
    "title": "臨床試験の中間事象",
    "section": "2 中間事象とは？",
    "text": "2 中間事象とは？\nランダム化は、交絡に伴うバイアスを回避するための有力な手段であるが、ランダム化比較試験であってもあらゆる問題がないわけではない。たとえば、治療開始後に、以下のような事象が生じることがある。\n\n対象者を選ぶための適格基準に違反することが、事後的に明らかになる。\nプロトコールに規定された治療計画から逸脱する。例えば、治療の中止、治療の切り替え、代替え治療の開始\n有害事象や死亡のため、アウトカムの一部が観測できなかったり、その解釈に影響を与えたりする。\n\nこれらの出来事を中間事象と呼ぶ。中間事象の取り扱いは、その試験でどのようなパラメータを推定しようとするかを左右するため、試験計画の段階で考慮すべきである。"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験の中間事象.html#中間事象は治療の一部かアウトカムの一部か",
    "href": "posts/statistics/2025/臨床試験の中間事象.html#中間事象は治療の一部かアウトカムの一部か",
    "title": "臨床試験の中間事象",
    "section": "3 中間事象は治療の一部か？アウトカムの一部か？",
    "text": "3 中間事象は治療の一部か？アウトカムの一部か？\n指針として、それが広い意味で治療の一部とみなされるのか、アウトカムの結果と解釈すべきかどうかは大事であろう。\n例として、試験薬の投与中止が考えられる。もし、治療を開始する前に、試験への参加拒否を理由に、患者が投与中止を申し出たとしよう。このケースでは、偶発的なもので、解析結果への影響は軽微と判断して、解析対象集団から除外することが一般的である。\n別の例として、なんらかの有害事象のため投与が中止され、有効性アウトカムが観測されなかったとしよう。このケースで、解析対象集団から除外すると、明らかに治験薬の安全性を過大推定してしまう。このケースでは、安全性解析対象集団には含めるべきだし、有効性の解釈においてこの患者が欠測のため解析に寄与しなかったとしても、そのことをきちんと報告すべきである。\n中間事象の有無で層別して、試験治療群とコントロール群の結果を記述したり、比較したりすることがある。中間事象がアウトカムの一部ともなされるようなケースでは注意が必要である。なぜなら、治療の結果が良い層と悪い層を分けて、それぞれの層で試験治療とコントロール治療を比較してることになるからである。"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験の中間事象.html#中間事象の発現頻度が群間で異なるか",
    "href": "posts/statistics/2025/臨床試験の中間事象.html#中間事象の発現頻度が群間で異なるか",
    "title": "臨床試験の中間事象",
    "section": "4 中間事象の発現頻度が群間で異なるか",
    "text": "4 中間事象の発現頻度が群間で異なるか\n中間事象の発現頻度が群間で異なると、例えば、中間事象の発現頻度が少ない方が、その扱いによって因果効果の違いが生じにくい。一方で、中間事象が多く発現しており、しかもどちらかの群で偏っていると、因果効果への影響は大きくなるし、治療の有効性や安全性について解釈が難しくなる。"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験の中間事象.html#中間事象が生じた理由は何か共変量によって説明できるか",
    "href": "posts/statistics/2025/臨床試験の中間事象.html#中間事象が生じた理由は何か共変量によって説明できるか",
    "title": "臨床試験の中間事象",
    "section": "5 中間事象が生じた理由は何か、共変量によって説明できるか",
    "text": "5 中間事象が生じた理由は何か、共変量によって説明できるか\n中間事象によってアウトカムの欠測が生じ、それによって因果効果の推定にバイアスが含まれることがある。特にバイアスが懸念されるのは、中間事象が生じるメカニズムがアウトカムと関連しているときである。外的要因による欠測ならば、MCARの仮定の下で、欠測の影響を無視可能となるかもしれない。\n欠測に伴うバイアスを調整するには、欠測メカニズムを説明するような共変量を解析で考慮しなければならない。そのためには、どのような中間事象があり得るかを予想し、それが生じた理由など中間事象に関連するデータを収集できるように、あらかじめ計画を立てておく必要がある。"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験の中間事象.html#ich-e9-r1の5つのストラテジー",
    "href": "posts/statistics/2025/臨床試験の中間事象.html#ich-e9-r1の5つのストラテジー",
    "title": "臨床試験の中間事象",
    "section": "6 ICH E9 (R1)の5つのストラテジー",
    "text": "6 ICH E9 (R1)の5つのストラテジー\n\n6.1 治療方針ストラテジー\n対象者iの観察されるアウトカムをY_i、ランダム割付の結果をZ_i(Z_i = 1なら試験治療群、Z_i = 0 ならコントロール群 として、ランダム化とアウトカムの途中に生じる中間事象をM_iで表すに。コントロール治療と試験治療のそれぞれの割付に対応するPotential OutcomeをY_i^{Z=0}とY_i^{Z=1}で表す。中間事象M_iの影響で、実際に行われる治療A_i はランダム化の通りではないとする。あくまでランダム化の効果をみているのであって、A_iはみてない。\nすなわち治療方針ストラテジーの推定目標は以下の、割付結果がアウトカムに与える平均因果効果と考えられる。\n\nE(Y_i^{Z=1} - Y_i^{Z=0})\n\nこの推定目標の特徴は2つある。まず、この推定目標は、治療自体の効果というより、ランダム化時点で決めた治療方針のどちらが有効か、という意味での効果を表している。もう一つの特徴は、（治療変数）とは異なり、割付変数についてはランダム化により非交絡性が保証されている点である。そのため、治療方針ストラテジーを取れば、妥当な統計的推測が可能となる。\n\n\n6.2 複合ストラテジー\n試験治療とコントロール治療に対応する潜在的な中間事象の値を、それぞれM_i^{Z = 1} とM_i^{Z = 0}と表す。複合ストラテジーの推定目標は、アウトカムyと中間事象mのなんらかの関数f(y,m) を用いて、以下のように表現できる。\n\nE[f(Y_i^{Z=1},M_i^{Z=1}) -  f(Y_i^{Z=0},M_i^{Z=0}) ]\n\n\n\n6.3 仮想ストラテジー\n中間事象が発現しなかった仮想的な状況を想定する。この場合、関心のある推定目標は、仮想的な状況において得られたであろうアウトカムのある種の予測値である。\n仮想ストラテジーでは、状況によって様々な推定目標が考えられる。ここでは、ランダムに割り付けられた治療を、対象者の一部が遵守しなかった状況を考えよう。そして、実際に受けた治療を、治療変数A_i で表す。推定目標として自然なのは、治療変数A_iがアウトカムY_iに与える因果効果である。試験治療とコントロール治療が仮に行われた状況に対応する潜在結果変数をそれぞれ、 Y^{A=0}、Y^{A=1}と表すと、A_iの平均因果効果は以下で表すことができる。\n\nE(Y_i^{A=1}-Y_i^{A=0})\n\nただし、この場合のA_iは直接ランダムに割り付けられたものではないこと、治療を遵守しなかった対象者のアウトカムは医学的判断で採用できない可能性がある（欠測で扱いになる）ことに注意が必要である。この問題の対処法の1つとして、アウトカムになんらかのモデルを仮定して、結果を予測するというアプローチが考えられる。\n\n\n6.4 主要層ストラテジー\n省略\n\n\n6.5 治療下ストラテジー\n中間事象が発現し、治療が中止したり変更したりする時点より前の治療への反応をアウトカムとする。このストラテジーが特に意味を持つのは、経時データと生存時間データの場合であろう。アウトカムが経時的に繰り返し測定されている場合、全時点の測定値を採用するか、治療下のものに測定値を限定するかを選ばなければならない。同様に、特定のイベントまでの時間をアウトカムとして解析するとき、治療中止や変更を無視することもあれば、その時点で打ち切り（censoring）として取り扱うこともある。後者の解析では、治療が継続している期間内のハザードを比較しており、治療下ストラテジーの一種と考えられる"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験の中間事象.html#例抗がん剤試験を想定するestimand",
    "href": "posts/statistics/2025/臨床試験の中間事象.html#例抗がん剤試験を想定するestimand",
    "title": "臨床試験の中間事象",
    "section": "7 例：抗がん剤試験を想定するEstimand",
    "text": "7 例：抗がん剤試験を想定するEstimand\nアウトカムを生存時間とすると、3パターン感がられる。\n\n\n\nストラテジー\n治療方針\n治療下\n仮想\n\n\n\n\n中間事象\n治療切り替え\n治療切り替え\n治療切り替え\n\n\n考慮の仕方\n無視\n打ち切り\nIPCW\n\n\n要約指標\nハザード比\nハザード比\nハザード比"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微積分_2024.html",
    "href": "posts/statistics/2025/統計のための線形代数_微積分_2024.html",
    "title": "統計のための線形代数・微分積分まとめ2",
    "section": "",
    "text": "detA=detA^T\n行列式と積は可換である。すなわち、A,Bをn次正方行列とするとき、以下が成立する。\ndetAB=detA detB\nAが正則のとき、\ndet(A)^{-1}=det(A^{-1})\n\n3のみ証明を与える。\n\n2を認めるならば、\nI = detI= det(AA^{-1})=detAdet(A^{-1})"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微積分_2024.html#行列式の重要な性質",
    "href": "posts/statistics/2025/統計のための線形代数_微積分_2024.html#行列式の重要な性質",
    "title": "統計のための線形代数・微分積分まとめ2",
    "section": "",
    "text": "detA=detA^T\n行列式と積は可換である。すなわち、A,Bをn次正方行列とするとき、以下が成立する。\ndetAB=detA detB\nAが正則のとき、\ndet(A)^{-1}=det(A^{-1})\n\n3のみ証明を与える。\n\n2を認めるならば、\nI = detI= det(AA^{-1})=detAdet(A^{-1})"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微積分_2024.html#section",
    "href": "posts/statistics/2025/統計のための線形代数_微積分_2024.html#section",
    "title": "統計のための線形代数・微分積分まとめ2",
    "section": "2 ",
    "text": "2"
  },
  {
    "objectID": "posts/statistics/2025/組織におけるSASプログラミングのMUST_DO_MUST_NOT_DO.html",
    "href": "posts/statistics/2025/組織におけるSASプログラミングのMUST_DO_MUST_NOT_DO.html",
    "title": "プログラミング一般的な考え方（MUST DO , MUST NOT DO)",
    "section": "",
    "text": "本記事では、組織でSASプログラミングを実行する際に、注意すべきことをまとめます。また初学者向けにPitfall and Bad Habitsについてまとめます。\n引用：\n\nGood Programming Practices in Clinical Trial – a Check Program\nHow Not to SAS: Avoiding Common Pitfalls and Bad Habits\n\nAbstract\n\nLarge pharmaceutical companies typically have many programmers, data managers and statisticians producing trial analysis and reporting programs. Each of these has their own style in programming. Unfortunately, such a large variety in program styles can lead to problems in the quality, readability, verifiability and maintainability of the program code.\nEstablishing a central standard, or guideline, for good programming practices (GPP) is therefore a necessary first step for large companies. By requiring all programs to meet the GPP guideline, we can insure that we produce high quality programs that are easily readable by others, can be verified easily and can be easily maintained.\n\nということで、MUST DO、MUST NOT DO、AVOID、RECCOMENTを組織として決めたら良い。\n\n\n\n\n\n\n\nカテゴリ\n説明\n\n\n\n\nMUST DO\nThese requirements for coding style and techniques must be met in the program.\n\n\nMUST NOT DO\nThe styles and techniques in this category must not be used in the program.\n\n\nAVOID\nThe styles and techniques items in this category should not be used in the program.\n\n\nRECOMMEND\nThe styles and techniques items in this category should be used in order to promote good readability, verifiability, maintainability and efficiency"
  },
  {
    "objectID": "posts/statistics/2025/組織におけるSASプログラミングのMUST_DO_MUST_NOT_DO.html#sasプログラミングtipsについてまとめます",
    "href": "posts/statistics/2025/組織におけるSASプログラミングのMUST_DO_MUST_NOT_DO.html#sasプログラミングtipsについてまとめます",
    "title": "プログラミング一般的な考え方（MUST DO , MUST NOT DO)",
    "section": "",
    "text": "本記事では、組織でSASプログラミングを実行する際に、注意すべきことをまとめます。また初学者向けにPitfall and Bad Habitsについてまとめます。\n引用：\n\nGood Programming Practices in Clinical Trial – a Check Program\nHow Not to SAS: Avoiding Common Pitfalls and Bad Habits\n\nAbstract\n\nLarge pharmaceutical companies typically have many programmers, data managers and statisticians producing trial analysis and reporting programs. Each of these has their own style in programming. Unfortunately, such a large variety in program styles can lead to problems in the quality, readability, verifiability and maintainability of the program code.\nEstablishing a central standard, or guideline, for good programming practices (GPP) is therefore a necessary first step for large companies. By requiring all programs to meet the GPP guideline, we can insure that we produce high quality programs that are easily readable by others, can be verified easily and can be easily maintained.\n\nということで、MUST DO、MUST NOT DO、AVOID、RECCOMENTを組織として決めたら良い。\n\n\n\n\n\n\n\nカテゴリ\n説明\n\n\n\n\nMUST DO\nThese requirements for coding style and techniques must be met in the program.\n\n\nMUST NOT DO\nThe styles and techniques in this category must not be used in the program.\n\n\nAVOID\nThe styles and techniques items in this category should not be used in the program.\n\n\nRECOMMEND\nThe styles and techniques items in this category should be used in order to promote good readability, verifiability, maintainability and efficiency"
  },
  {
    "objectID": "posts/statistics/2025/組織におけるSASプログラミングのMUST_DO_MUST_NOT_DO.html#must-do-and-must-not-do-categories",
    "href": "posts/statistics/2025/組織におけるSASプログラミングのMUST_DO_MUST_NOT_DO.html#must-do-and-must-not-do-categories",
    "title": "プログラミング一般的な考え方（MUST DO , MUST NOT DO)",
    "section": "2 Must Do and Must Not Do Categories",
    "text": "2 Must Do and Must Not Do Categories\n\n2.1 MUST DO\n\n\n\n\n\n\n項目\n\n\n\n\nMake a backup copy of the program prior to updates (Not checked by the program).\nプログラムはフォルダとして、yyyymmddで日付管理をしましょう。1つのフォルダで管理するのは間違っています。\n都度、プログラムを実行する際にフォルダを移動させましょう。注意点：プロジェクトのプログラムが開発が終了したら、最後は1つのプログラムで全プログラムが実行されて、.rtfで解析帳票が出てくるようにしましょう。途中でExcelに吐き出してExcelで結果を加工するのはご法度です。ありえません。\n\n\nUse program documentation, e.g. Headers and Comments. Write documentation in English. Imbedded comments within the body of the program code should detail the modular flow.\nこれも大事です。手動でHeadersをつけるの大変ですので、自動化させましょう。ただし、プログラム内にはコメントを残すのが良いです。\n\n\nTypically, write only one SAS® statement per line.\n\n\nUse indentation to arrange the code clearly.\n\n\nUse unique names for datasets and files within the program / macro.\nこれも大事です。var、tmpが楽でよいですが、きちんと名前をつけましょう。\n\n\nReference datasets and/or files explicitly in each step or proc.\n1つの帳票に対して原則1つのプログラムです。\n\n\nDisplay ‘Draft’ in the output before program validation.\nこれもいいですね。1人でやるのは悩ましいが。\n\n\nReset global options to original settings – if changed by the program / macro.\nglobal optionsやproc datasetでwork フォルダを空にしたりlogをresetしましょう。\n\n\nDelete all temporary datasets after program execution.\n上と同じ考えですね。\n\n\nUse ‘RUN;’ or ‘QUIT;’ at the end of each data step / proc.\nこれは基本です。\n\n\nUse defensive coding.\nよく分かりません。\n\n\nDisplay the name of the program on the output when running outside of RAGE (our document control system).\n\n\nOptimize the data. Do not re-read data.\n\n\n\n\n\n2.2 MUST NOT DO\n\n\n\n\n\n\n項目\n\n\n\n\nHard-Code data.\n手で記載するのをやめましょう。表のheaderやグラフにおけるY軸、X軸のlength等はマクロ変数、Rであれば事前にベクトルにしておきましょう。\n/* ハードコードの悪い例 */ data work.sales_analysis; set mylib.sales_data; where year = 2024 and region = ‘Tokyo’; /* 年度と地域を直接記述 */\n/* 固定値を直接計算に使用 */ bonus = salary * 0.15; /* ボーナス率15%をハードコード */\n/* ファイルパスをハードコード */ if _n_ = 1 then call symputx(‘output_path’, ‘C:\\Reports\\sales_2024.xlsx’); run;\n/* マクロ変数で設定値を管理 */ %let target_year = 2024; %let target_region = Tokyo; %let bonus_rate = 0.15; %let output_dir = C:\\Reports; %let library_name = mylib;\n/* または外部設定ファイルから読み込み */ %include “C:\\Config\\sas_config.sas”;\ndata work.sales_analysis; set &library_name..sales_data; where year = &target_year and region = “&target_region”;\n/* マクロ変数を使用 */ bonus = salary * &bonus_rate; run;\n/* 動的なファイル名生成 */ %let output_file = &output_dir.\\sales_&target_year..xlsx;\nproc export data=work.sales_analysis outfile=“&output_file” dbms=xlsx replace; run;\n\n\nManually edit output.\nこれは論外です。どんな理由であれ解析結果を手で直すことはあってはなりません。\n上のHard codingと同様に事前にマクロ変数やベクトルで準備しておきましょう。\n\n\nOverwrite input data.\nこれはlibname statementにおいて、readonly optionをつけておきましょうか。\n\n\nUse macro names that are already used by CARE Standard macros (our standardized reporting macros)\n既にあるマクロと同じ名前は利用してはいけません。\n\n\nUse of SAS® keywords for dataset and variable names.\n\n\n\nAny programmers that have supported clinical trial analysis and reporting understand the complexity of derived datasets that are ready to produce tables and graphs for efficacy and safety analysis. Taking data from several domains, many-to-many merging, transposing data, imputing dates and values, averaging within visits for duplicate results, and creating intermediate datasets for the final ADaM are more complex than we expect."
  },
  {
    "objectID": "posts/statistics/2025/組織におけるSASプログラミングのMUST_DO_MUST_NOT_DO.html#参考",
    "href": "posts/statistics/2025/組織におけるSASプログラミングのMUST_DO_MUST_NOT_DO.html#参考",
    "title": "プログラミング一般的な考え方（MUST DO , MUST NOT DO)",
    "section": "3 参考",
    "text": "3 参考\n\nChallenges in Implementing ADaM datasets: Balancing the Analysis-Ready and Traceability Concepts"
  },
  {
    "objectID": "posts/statistics/2025/潜在結果変数と統計的推測.html",
    "href": "posts/statistics/2025/潜在結果変数と統計的推測.html",
    "title": "潜在結果変数と統計的推測",
    "section": "",
    "text": "対象者iにについて、観測されるアウトカムを0または1の値をとる2値変数Y_iで表す。また、関心のある試験治療とコントロール治療に対応する潜在結果変数をY_i{^0}とY_i{^1}と定義する。試験治療がアウトカムに与える因果効果は、両者に差があるかどうか、つまり以下で定義される。\n\n\\tau_i = Y_i^1-Y_i^0\n\nこの、 \\tau_iを解析単位レベルの因果効果という。個人単位iで因果効果は推測不可能であるので、 N人からなる有限母集団を考え、統計的推測を行う。集団全体の潜在結果変数は以下の4パターンに分類できる。\n\n\\begin{aligned}\n・Y_i^0 =Y_i^1 = 0  \\\\\n・Y_i^0 = 0 , Y_i^1 = 1  \\\\\n・Y_i^0 =1, Y_i^1 = 0 \\\\\n・Y_i^0 =Y_i^1 = 1\n\\end{aligned}\n\nこれらのパターンの人数をそれぞれ、 n_{00},n_{01},n_{10},n_{11} とすると、対象者全員に試験治療を用いたときとコントロール治療を用いたときの比較は、有限母集団因果リスク差（finite-population causal risk difference）として、以下で表せる。\n\n\\bar \\tau = \\frac{\\sum_{i=1}^N Y_i^1}{N}-\\frac{\\sum_{i=1}^N Y_i^0}{N} =\\frac{n_{01}}{N}-\\frac{n_{10}}{N}\n\n通常のリスク差と異なり、分母が対象者全員である。\nまた、有限母集団と無限母集団の違いにも注意する。無限母集団とは、 N人全員の潜在結果変数の組 \\{Y_i^0,Y_i^1 ; i = 1,...,N\\}からランダムサンプルとみなすことがある。そのときの \\tau_iの期待値は、 Pr(Y_i^0 =1) = \\pi^0とPr(Y_i^0 =1) = \\pi^1の差になる。このように定義された以下を母集団因果リスク差（super-population causal risk difference）または、（連続データでは）母集団平均因果効果（super-population average causal effect）という。\n\n\\tau = E(Y_i^1-Y_i^0)=\\pi^1-\\pi^0\n\n別に因果リスク差だけでなく、因果リスク比、因果オッズ比と定義してもよい。\nさらに、実際に試験に組み込まれた N_1人に注目して、その集団に試験治療を用いたときとコントロール治療を用いたときの因果リスク差を定義することができる。それはそれぞれ、治療群における有限母集団因果リスク差（finite-population causal risk difference for the treated)、治療群における母集団因果リスク差（super-population causal risk difference for the treated）と呼ぶ。\n\n\\bar \\tau^T= \\frac{\\sum_{i:A_i=1}Y_i^1}{N_1}-\\frac{\\sum_{i:A_i=1}Y_i^0}{N_1}\n\n\n\\tau^T = E(Y_i^1-Y_i^0 | A_i=1)"
  },
  {
    "objectID": "posts/statistics/2025/潜在結果変数と統計的推測.html#潜在結果変数と因果リスク差",
    "href": "posts/statistics/2025/潜在結果変数と統計的推測.html#潜在結果変数と因果リスク差",
    "title": "潜在結果変数と統計的推測",
    "section": "",
    "text": "対象者iにについて、観測されるアウトカムを0または1の値をとる2値変数Y_iで表す。また、関心のある試験治療とコントロール治療に対応する潜在結果変数をY_i{^0}とY_i{^1}と定義する。試験治療がアウトカムに与える因果効果は、両者に差があるかどうか、つまり以下で定義される。\n\n\\tau_i = Y_i^1-Y_i^0\n\nこの、 \\tau_iを解析単位レベルの因果効果という。個人単位iで因果効果は推測不可能であるので、 N人からなる有限母集団を考え、統計的推測を行う。集団全体の潜在結果変数は以下の4パターンに分類できる。\n\n\\begin{aligned}\n・Y_i^0 =Y_i^1 = 0  \\\\\n・Y_i^0 = 0 , Y_i^1 = 1  \\\\\n・Y_i^0 =1, Y_i^1 = 0 \\\\\n・Y_i^0 =Y_i^1 = 1\n\\end{aligned}\n\nこれらのパターンの人数をそれぞれ、 n_{00},n_{01},n_{10},n_{11} とすると、対象者全員に試験治療を用いたときとコントロール治療を用いたときの比較は、有限母集団因果リスク差（finite-population causal risk difference）として、以下で表せる。\n\n\\bar \\tau = \\frac{\\sum_{i=1}^N Y_i^1}{N}-\\frac{\\sum_{i=1}^N Y_i^0}{N} =\\frac{n_{01}}{N}-\\frac{n_{10}}{N}\n\n通常のリスク差と異なり、分母が対象者全員である。\nまた、有限母集団と無限母集団の違いにも注意する。無限母集団とは、 N人全員の潜在結果変数の組 \\{Y_i^0,Y_i^1 ; i = 1,...,N\\}からランダムサンプルとみなすことがある。そのときの \\tau_iの期待値は、 Pr(Y_i^0 =1) = \\pi^0とPr(Y_i^0 =1) = \\pi^1の差になる。このように定義された以下を母集団因果リスク差（super-population causal risk difference）または、（連続データでは）母集団平均因果効果（super-population average causal effect）という。\n\n\\tau = E(Y_i^1-Y_i^0)=\\pi^1-\\pi^0\n\n別に因果リスク差だけでなく、因果リスク比、因果オッズ比と定義してもよい。\nさらに、実際に試験に組み込まれた N_1人に注目して、その集団に試験治療を用いたときとコントロール治療を用いたときの因果リスク差を定義することができる。それはそれぞれ、治療群における有限母集団因果リスク差（finite-population causal risk difference for the treated)、治療群における母集団因果リスク差（super-population causal risk difference for the treated）と呼ぶ。\n\n\\bar \\tau^T= \\frac{\\sum_{i:A_i=1}Y_i^1}{N_1}-\\frac{\\sum_{i:A_i=1}Y_i^0}{N_1}\n\n\n\\tau^T = E(Y_i^1-Y_i^0 | A_i=1)"
  },
  {
    "objectID": "posts/statistics/2025/潜在結果変数と統計的推測.html#ランダム化の下での因果リスク差の推定",
    "href": "posts/statistics/2025/潜在結果変数と統計的推測.html#ランダム化の下での因果リスク差の推定",
    "title": "潜在結果変数と統計的推測",
    "section": "2 ランダム化の下での因果リスク差の推定",
    "text": "2 ランダム化の下での因果リスク差の推定\n対象者iに試験治療を用いたかどうかをA_i(A_i =1なら試験治療、A_i=0ならコントロール治療）、アウトカムを Y_i(Y_i=0ならイベントなし、Y_i=1ならイベントあり)で表す。\nTable 3-2 観測データの記法\n\n\n\n\n\n\n\n\n\n\n試験治療( A_1=1 )\nコントロール治療( A_1=0 )\n合計\n\n\n\n\nイベントなし( Y_i=0)\nN_1-S_1\nN_0-S_0\n\n\n\nイベントなし( Y_i=1)\nS_1\nS_0\nS\n\n\n合計\nN_1\n_N0\nN\n\n\n\nTable 3-3 潜在結果変数の記法\n\n\n\n\n\n\n\n\n\n\n試験治療( A_1=1 )\nコントロール治療( A_1=0 )\n合計\n\n\n\n\n常にイベントなし(Y_i^0=Y_i^1=0)\nx_{00}\nn_{00}-x_{00}\nn_{00}\n\n\n試験治療のみイベントあり(Y_i^0= 0 ,Y_i^1=1)\nx_{01}\nn_{01}-x_{01}\nn_{ 01}\n\n\nコントロール治療のみイベントあり\n(Y_i^0= 1 ,Y_i^1=0)\nx_{10}\nn_{10}-x_{10}\n n_{10}\n\n\n常にイベントあり\n(Y_i^0=Y_i^1=1)\nx_{11}\nn_{11}-x_{11}\n n_{11}\n\n\n合計\nN_1\nN_0\nN\n\n\n\nここで、試験治療とコントロール治療がランダムに割り付けられていると仮定する。いわゆるリスク差は、以下となる。\n\n\\hat \\tau=\\frac{S_1}{N_1} - \\frac{S_0}{N_0}\n\nまた、潜在アウトカムの記法をTable3-3のように表す。この表はN組の Y_i^0とY_i^1から構成される有限母集団のパターンを表している。つまり各パターンの人数 n_{00},n_{01},n_{10},n_{11}が分かれば、（ただし、総和がNであるという制約はある。）この有限母集団で因果効果が判断できる。一方で、ランダム割付をすることで、x_{00},x_{01},x_{10},x_{11}は確率変数とみなすことができる。\n繰り返し、ランダム割付を行ってデータを発生させた仮想的な状況を考えてみる。ある個人がどちらの治療に割付られるかによって、x_{00},x_{01},x_{10},x_{11}はばらつく。観測データでいえば、割付結果によって、 S_1=x_{01}+x_{10}や S_0=n_{10}-x_{10}+n_{11}-x_{11}が確率的に変動する。ランダム化に基づく推測では、これらの確率的変動が因果リスク差を推定するときの誤差の源となる。"
  },
  {
    "objectID": "posts/statistics/2025/潜在結果変数と統計的推測.html#一致性非干渉性効果の一様性",
    "href": "posts/statistics/2025/潜在結果変数と統計的推測.html#一致性非干渉性効果の一様性",
    "title": "潜在結果変数と統計的推測",
    "section": "3 一致性、非干渉性、効果の一様性",
    "text": "3 一致性、非干渉性、効果の一様性\n対象者iと対象者jの2人がいて、この2つの比較 Y_i-Y_jが因果効果としてなんらかの意味を持つための条件を紹介する。\n\nconsistency\n\nconsistencyとは、 Y_i=(1-A_i)Y_i^0+A_iY_i^1 で、観測されるデータと潜在結果変数の関係がリンクしていることをいう。\n\nno interference between unit\n\n他の解析単位の治療が、別の解析単位に影響品という仮定\n\n\nconsistencyとno interference between unitを合わせて、SUTVA(Stable Unit Treatment Value Assumption）呼ばれることがある。consistencyとno interference between unitが成立していたとしても、 Y_i-Y_jという比較は、個人の因果効果（\\tau_i = Y_i-Y_jまたは\\tau_j = Y_j-Y_i）とは一致しない。\n逆に、一致するための条件を考えると、 Y_i^0=Y_j^0と\\tau_j=\\tau_i が成り立っていればよいと考えられる。1つめは対象者iと対象者jの疾患の状態に差がないことを意味する。これは交絡と関連付けて、比較可能性(comparability)という概念に相当する。また、2つ目は、解析単位間で因果効果が等しいことであり、これを効果の一様性（between-unit homogeneity of effects）と呼ぶ。これは、全ての解析個体i（i=1,…,N)において、 Y_i^0とY_i^1の相関が1ということを意味するから強い仮定である。\n\ncomparability\nbetween-unit homogeneity of effects"
  },
  {
    "objectID": "posts/statistics/2025/潜在結果変数と統計的推測.html#条件付き因果リスク差",
    "href": "posts/statistics/2025/潜在結果変数と統計的推測.html#条件付き因果リスク差",
    "title": "潜在結果変数と統計的推測",
    "section": "4 条件付き因果リスク差",
    "text": "4 条件付き因果リスク差\n状況によっては、効果が均一とは考えにくく、しかも共変量によって効果の大きさや方向が変わるようなこともあり得る。このような共変量を効果修飾因子（effect modifier）という。潜在結果変数を用いて、 \\tau_i=Y_i^1-Y_i^0と相関する因子を効果修飾因子といってもよい。効果修飾の有無は、共変量 L_iにによって、条件づけた因果リスク差を用いることができる。\n\n\\tau(l) = E(Y_i^1-Y_i^0|L_i = l)\n\n\n\\bar \\tau(l) =  \\frac{\\sum_{i:L_i=l}Y_i^1}{N_l}-\\frac{\\sum_{i:L_i=l}Y_i^0}{N_l}\n\n後者の有限母集団リスク差は、リスク差の分母 N_lは共変量が L_i=lである人数であることに注意。\nそして、母集団因果リスク差と条件付き因果リスク差には、以下の関係がある。\n\n\\tau = E(Y_i^1-Y_i^0)= \\sum_l \\tau(l) Pr(L_i=l)\n\n\n\\bar \\tau= \\frac{1}{N}\\sum_{i=1}^N \\bar \\tau(L_i)\n\nという関係がある。つまり、有限集団因果リスク差は、条件付きリスク差を対象者N人について、平均をしたものである。"
  },
  {
    "objectID": "posts/statistics/2025/潜在結果変数と統計的推測.html#割付メカニズム",
    "href": "posts/statistics/2025/潜在結果変数と統計的推測.html#割付メカニズム",
    "title": "潜在結果変数と統計的推測",
    "section": "5 割付メカニズム",
    "text": "5 割付メカニズム\n対象者全体が、どの治療を受けるかを決める規則を、割付けメカニズム（assignment mechanism）という。個人ごとに一定の確率でランダムに割り付けるものである。このとき、割付確率は潜在結果変数 \\{Y_i^0,Y_i^1\\}と共変量L_iに依存しない。この単純ランダム化は以下のように表す。\n\nPr(A_1,A_2,...,A_N | Y_i^0,Y_i^1,L_1,Y_2^0,Y_2^1,L_2,...,Y_N^0,Y_N^1,L_N)=\\prod_{i=1}^N Pr(A_i)\n\nこのとき、期待値の上では、割付けたグループ間で共変量の偏りは生じない。\nImbens and Rubinでは、無視可能（ignorable)と非交絡（uncounfounded）という概念を用いて分類している。\n無視可能性とは、割付確率が観測データだけに依存することと定義され、以下のように表される。\n\n\\begin{aligned}\nPr(A_1,A_2,...,A_N | Y_i^0,Y_i^1,L_1,Y_2^0,Y_2^1,L_2,...,Y_N^0,Y_N^1,L_N)\\\\\n=Pr(A_1,A_2,...,A_N | Y_1,L_1,Y_2,L_2,...,Y_N,L_N)\\\\\n\\end{aligned}\n 非交絡とは、割付確率が共変量だけに依存することを指す。このよう簿は、強い無視可能性（strongly ignorable）とほぼ同じ意味で用いられている。強い無視可能な割付メカニズムは、割付が個人ごとに独立でなされるとき、以下で表すことをいう。\n\nPr(A_1,A_2,...,A_N | Y_i^0,Y_i^1,L_1,Y_2^0,Y_2^1,L_2,...,Y_N^0,Y_N^1,L_N)=\\prod_{i=1}^N Pr(A_i | L_i)\n\nなお、 e(l)=Pr(A_i=1 | L_i = l)をpropensity scoreという。"
  },
  {
    "objectID": "posts/statistics/2025/潜在結果変数と統計的推測.html#因果効果の識別に必要な2つの仮定",
    "href": "posts/statistics/2025/潜在結果変数と統計的推測.html#因果効果の識別に必要な2つの仮定",
    "title": "潜在結果変数と統計的推測",
    "section": "6 因果効果の識別に必要な2つの仮定",
    "text": "6 因果効果の識別に必要な2つの仮定\n因果効果を識別するためには、割付メカニズムについて、2つの仮定が必要となる。第一の仮定は、"
  },
  {
    "objectID": "posts/statistics/2025/実データ解析フォルダ構成.html#なぜフォルダ構成がそこまで重要なのか",
    "href": "posts/statistics/2025/実データ解析フォルダ構成.html#なぜフォルダ構成がそこまで重要なのか",
    "title": "利用しない：臨床研究のための実データ解析フォルダ構成",
    "section": "2.1 なぜフォルダ構成がそこまで重要なのか？",
    "text": "2.1 なぜフォルダ構成がそこまで重要なのか？\n単なるファイルの整理術と侮ってはいけません。適切なフォルダ構成は、研究の品質と直結します。\n\n再現性: いつ、誰が、どのデータとプログラムを使って、どんな結果を出したのかが明確になります。これは科学的根拠の基盤です。\n透明性: データの前処理から解析、結果生成までの全プロセスが明確になり、共同研究者や査読者、監査担当者が容易に追跡できるようになります。\n効率性: 目的のファイルに素早くアクセスでき、新しい解析の追加やデータ更新、論文執筆作業がスムーズに進みます。\n監査可能性: 規制当局への提出が求められる臨床研究では、全てのステップが厳格に管理されている証拠となります。\n\nそれでは、具体的なフォルダ構成を見ていきましょう！"
  },
  {
    "objectID": "posts/statistics/2025/実データ解析フォルダ構成.html#これが理想形臨床研究向けフォルダ構成",
    "href": "posts/statistics/2025/実データ解析フォルダ構成.html#これが理想形臨床研究向けフォルダ構成",
    "title": "利用しない：臨床研究のための実データ解析フォルダ構成",
    "section": "2.2 これが理想形！臨床研究向けフォルダ構成",
    "text": "2.2 これが理想形！臨床研究向けフォルダ構成\nこの構成は、研究のワークフロー全体を考慮し、データのライフサイクル、プログラム開発、ドキュメント管理、コミュニケーションまでを網羅しています。フォルダ名の先頭に数字を付けることで、常に論理的な順序で表示されるよう工夫しています。\nプロジェクトルート/\n├── 00_Resources/           # 勉強した教材、参考文献、参考資料など\n├── 01_Docs/                # ドキュメント関連（研究計画書、構造定義書など）\n│   ├── 01_Protocol/        # 研究計画書\n│   ├── 02_Specs/           # 各種仕様書（データ定義書、ADS構造定義書、TLFモックアップ）\n│   └── 03_SAP/             # 統計解析計画書（SAP）\n├── 02_Data/                # データ関連\n│   ├── 01_Raw/             # 生データ（変更不可）\n│   └── 02_External/        # 外部参照データ（辞書、マスタなど）\n├── 03_Programs/            # プログラム関連\n│   ├── 01_ADS/             # 解析用データセット（ADS）作成プログラム\n│   │   ├── Develop/        # 開発中のプログラム\n│   │   └── Final/          # 確定版プログラム\n│   ├── 02_TLF/             # 解析およびTLF作成プログラム\n│   │   ├── Develop/        # 開発中のプログラム\n│   │   └── Final/          # 確定版プログラム\n│   └── 03_Macros_Functions/# 共通マクロ、関数\n├── 04_Output/              # 出力物関連\n│   ├── 01_Logs/            # 全てのプログラム実行ログ\n│   ├── 02_ADS/             # 作成された解析用データセット\n│   ├── 03_TLF/             # Table, Listing, Figure（最終成果物）\n│   │   ├── YYYYMMDD/       # 解析実行日ごとの出力フォルダ\n│   │   │   ├── Tables/\n│   │   │   ├── Listings/\n│   │   │   └── Figures/\n│   └── 04_Interim_Output/  # その他中間出力、一時ファイル、探索的解析結果など\n├── 05_Paper/               # 論文執筆関連\n│   ├── Drafts/             # 論文ドラフト\n│   ├── Figures_Tables_Final/# 論文に掲載する最終版の図表\n│   └── References/         # 参考文献管理\n├── 06_Meeting/             # 会議関連（ミーティング議事録、進捗報告など）\n│   ├── YYYYMMDD_MeetingTitle/# 日付とミーティング名ごとのフォルダ\n│   └── Shared_Materials/   # 共有資料、テンプレートなど\n└── renv/                   # (Rの場合) Rパッケージ管理"
  },
  {
    "objectID": "posts/statistics/2025/実データ解析フォルダ構成.html#各フォルダの役割と運用のコツを徹底解説",
    "href": "posts/statistics/2025/実データ解析フォルダ構成.html#各フォルダの役割と運用のコツを徹底解説",
    "title": "利用しない：臨床研究のための実データ解析フォルダ構成",
    "section": "2.3 各フォルダの役割と運用のコツを徹底解説！",
    "text": "2.3 各フォルダの役割と運用のコツを徹底解説！\nそれぞれのフォルダには明確な役割があります。適切に使い分けることで、プロジェクト全体が整理され、将来の自分や共同研究者が困ることがなくなります。\n\n2.3.1 00_Resources/：研究の「知識ベース」\nこのフォルダは、シミュレーション研究と同様に、あなたの研究活動を支える背景知識や参考資料を格納します。新しい統計手法の論文、Rパッケージの使い方メモ、講義ノートなど、直接データやプログラムに関わらなくても、研究の理解を深めるために不可欠な情報を整理しておきましょう。\n\n\n2.3.2 01_Docs/：研究の「骨格」と「仕様書」\nプロジェクトの公式ドキュメントを集約する場所です。特に「構造定義書」がここに位置します。\n\n01_Protocol/: 研究計画書 (Protocol) を保管します。これらは、解析の根拠となる最重要ドキュメントであり、常に最新版を明確にしておく必要があります。\n02_Specs/: ここが「構造定義書」の中心です。生データ、前処理済みデータ、そして解析用データセット（ADS）の変数定義、ラベル、フォーマット、許容値、欠損値などを記述します。SASの場合は、この定義書からATTRIBやPROC FORMATで利用するSASコードを自動生成できるよう設計すると、効率が格段に上がります。作成するTable, Listing, Figure (TLF) のモックアップもここに置きます。\n03_SAP/: 統計解析計画書 (SAP) を保管します。以前の 03_Reports から独立させ、解析の根幹となるドキュメントであることを明確にしました。\n\n\n\n2.3.3 02_Data/：データの「聖域」と「加工履歴」\nデータのトレーサビリティを確保するための最も重要なフォルダです。\n\n01_Raw/: 外部から受け取ったオリジナルの生データを一切変更せずに格納します。データ受領時のハッシュ値なども記録しておくと、データの完全性を保証できます。\n03_External/: コーディング辞書（MedDRAなど）や外部基準値など、外部から参照するデータはここに置きます。これらのデータもバージョン管理を徹底しましょう。\n\n\n\n2.3.4 03_Programs/：プログラムの「開発」と「確定」\nプログラムのバージョン管理と変更履歴を明確にするための場所です。\n\n01_ADS/: 解析用データセット（ADS）を作成するプログラムです。\n\nDevelop/YYYYMMDD/: 日々の試行錯誤や修正を行う際の開発中のプログラムを日付ごとに整理します。\nFinal/vX.Y/: 品質管理（QC）をパスし、最終確定したADS作成プログラムをADSのバージョン（例: v1.0）に対応させて格納します。\n\n02_TLF/: 解析を実行し、Table, Listing, Figure (TLF) を作成するプログラムです。ADS作成プログラムと同様に、Develop/YYYYMMDD/ と Final/vX.Y/ で管理します。\n03_Macros_Functions/: プロジェクト全体で共通利用するマクロや関数をまとめます。04_Utilities/ は削除し、補助的なスクリプトは必要に応じてDevelopフォルダ内や03_Macros_Functionsに含める運用を想定します。\n\n\n\n2.3.5 04_Output/：解析結果の「証拠」と「成果物」\n解析結果と、それを生成した全ての証拠（ログ）を体系的に保存する場所です。\n\n01_Logs/: 全てのプログラム実行ログを、実行日時が分かるファイル名で日付フォルダ（YYYYMMDD/）内に保存します。ログにはプログラムの開始/終了時間、使用データ、エラーや警告の有無などを明確に記録し、何か問題が起きた際に迅速に原因を特定できるようにします。\n02_ADS/vX.Y/: 03_Programs/01_ADS/Final/vX.Y/ のプログラムで生成された最終確定版のADSを、バージョンごとに格納します。\n03_TLF/YYYYMMDD/: 03_Programs/02_TLF/Final/ のプログラムで生成された最終的なTable, Listing, Figureを、解析実行日ごとに保存します。最終的なTLFには、使用したADSやプログラムのバージョン、作成日時などの情報がフッターやコメントとして埋め込まれていると完璧です。\n04_Interim_Output/: 中間的な出力ファイル、一時ファイル、探索的データ解析の結果、品質管理のチェックリストやサマリー結果など、最終報告書には含まれないが保存しておきたい出力物を格納します。\n\n\n\n2.3.6 05_Paper/：論文執筆の「基地」\n論文執筆に特化したフォルダです。\n\nDrafts/: 論文のドラフト版を保存します。Word、Quarto (.qmd)、LaTeXファイルなど、使用ツールに応じて格納してください。\nFigures_Tables_Final/: 論文に実際に掲載する最終版の図表を高品質なフォーマット（PDF、TIFFなど）で保存します。これらは04_Output/03_TLF/ から厳選されたものになります。\nReferences/: 参考文献管理ファイル（BibTeXファイルなど）や、関連論文のPDFなどを格納します。\n\n\n\n2.3.7 06_Meeting/：議論の「記録」と「進捗」\n会議に関する全ての資料を体系的に管理します。特に、Quartoでの進捗報告を効率的に行うための工夫があります。\n\nYYYYMMDD_MeetingTitle/: 日付とミーティングタイトルを組み合わせたフォルダを、各ミーティングごとに作成します。このフォルダ直下に、議題、議事録（.docx, .pdfなど）、Quarto (.qmd) 形式の進捗報告書ソースファイル、そしてその.qmdファイルからレンダリングされた出力ファイル（.html, .pdfなど）など、そのミーティングに関連する全ての資料を格納します。\nShared_Materials/: プロジェクト全体で共有されるミーティングテンプレートや共通のガイドライン、過去の参考資料など、特定のミーティングに紐づかない汎用的な資料を置きます。"
  },
  {
    "objectID": "posts/statistics/2025/実データ解析フォルダ構成.html#rで一発作成フォルダ自動生成テンプレートと使用例",
    "href": "posts/statistics/2025/実データ解析フォルダ構成.html#rで一発作成フォルダ自動生成テンプレートと使用例",
    "title": "利用しない：臨床研究のための実データ解析フォルダ構成",
    "section": "2.4 Rで一発作成！フォルダ自動生成テンプレートと使用例",
    "text": "2.4 Rで一発作成！フォルダ自動生成テンプレートと使用例\nこの理想的なフォルダ構成を、Rのスクリプトでサクッと自動生成しちゃいましょう！以下のコードをRコンソールに貼り付けるか、新しい .R ファイルとして保存して実行するだけです。renv を使用しない設定 (use_renv = FALSE) での具体的な使用例も示しますね。\n#' 臨床研究における実データ解析プロジェクトのフォルダ構造を自動生成する関数\n#'\n#' @param project_name 作成するプロジェクトのルートフォルダ名\n#' @param use_renv renvパッケージを使用するかどうか (TRUE/FALSE)\n#'\n#' @return なし。指定されたパスにフォルダ構造を作成します。\n#' @export\ncreate_clinical_data_analysis_project_structure &lt;- function(project_name, use_renv = TRUE) {\n  # プロジェクトルートのパスを設定 (現在の作業ディレクトリに作成)\n  project_path &lt;- file.path(getwd(), project_name)\n\n  # ルートフォルダの作成\n  if (dir.exists(project_path)) {\n    stop(paste0(\"フォルダ '\", project_path, \"' は既に存在します。別の名前を指定するか、既存のフォルダを削除してください。\"))\n  }\n  dir.create(project_path, recursive = TRUE)\n  message(paste0(\"プロジェクトルートフォルダ '\", project_path, \"' を作成しました。\\n\"))\n\n  # サブフォルダの定義\n  # Develop/YYYYMMDDやFinal/vX.Yは実行時に手動または別スクリプトで作成することを推奨\n  sub_folders &lt;- c(\n    \"00_Resources\",\n    \"01_Docs/01_Protocol\",\n    \"01_Docs/02_Specs\",\n    \"01_Docs/03_SAP\", # 変更\n    \"02_Data/01_Raw\",\n    \"02_Data/02_External\", # 変更 (Processed削除)\n    \"03_Programs/01_ADS/Develop\", # 変更\n    \"03_Programs/01_ADS/Final\",   # 変更\n    \"03_Programs/02_TLF/Develop\", # 変更\n    \"03_Programs/02_TLF/Final\",   # 変更\n    \"03_Programs/03_Macros_Functions\", # 変更 (Utilities削除)\n    \"04_Output/01_Logs\",\n    \"04_Output/02_ADS\",\n    \"04_Output/03_TLF/Tables\",\n    \"04_Output/03_TLF/Listings\",\n    \"04_Output/03_TLF/Figures\",\n    \"04_Output/04_Interim_Output\",\n    \"05_Paper/Drafts\",\n    \"05_Paper/Figures_Tables_Final\",\n    \"05_Paper/References\",\n    \"06_Meeting/Shared_Materials\" # Meetingフォルダの初期作成\n  )\n\n  # サブフォルダの作成\n  for (folder in sub_folders) {\n    dir.create(file.path(project_path, folder), recursive = TRUE)\n    message(paste0(\"  - フォルダ '\", file.path(project_path, folder), \"' を作成しました。\\n\"))\n  }\n\n  # renvの初期化\n  if (use_renv) {\n    message(\"renvの初期化を開始します。これには少し時間がかかる場合があります...\\n\")\n    old_wd &lt;- getwd()\n    setwd(project_path)\n    tryCatch({\n      if (!requireNamespace(\"renv\", quietly = TRUE)) {\n        install.packages(\"renv\")\n      }\n      renv::init()\n      message(\"  - renvを初期化しました。\\n\")\n      message(\"    プロジェクトルートに '.Rprofile' と 'renv.lock' が生成されます。\\n\")\n    }, error = function(e) {\n      message(paste0(\"  - renvの初期化中にエラーが発生しました: \", e$message, \"\\n\"))\n      message(\"    手動で `renv::init()` を実行してください。\\n\")\n    }, finally = {\n      setwd(old_wd)\n    })\n  }\n\n  message(paste0(\"\\nプロジェクト '\", project_name, \"' のフォルダ構造が正常に作成されました。\"))\n  if (use_renv) {\n    message(paste0(\"RStudioで '\", project_name, \".Rproj' を開いた後、`renv::restore()` を実行して必要なパッケージをインストールしてください。\"))\n  } else {\n    message(paste0(\"RStudioで '\", project_name, \".Rproj' を開いて作業を開始してください。\"))\n  }\n  message(\"\\n**補足:**\")\n  message(\"  - `03_Programs/01_ADS/Develop/`, `03_Programs/02_TLF/Develop/` フォルダ内は、ご自身で `YYYYMMDD/` 形式のサブフォルダを作成し、ファイルを整理してください。\")\n  message(\"  - `03_Programs/.../Final/` や `04_Output/02_ADS/` フォルダ内は、ADSやTLFのバージョンに応じて `vX.Y/` 形式のサブフォルダを作成し、ファイルを整理してください。\")\n  message(\"  - 各プログラムの実行ログは `04_Output/01_Logs/YYYYMMDD/` に、日付とタイムスタンプを含むファイル名で保存することを推奨します。\")\n  message(\"  - `01_Docs/02_Specs/` にデータ定義書やADS構造定義書を必ず作成してください。\")\n  message(\"  - `06_Meeting/` フォルダ内は、ミーティングごとに `YYYYMMDD_MeetingTitle/` 形式のサブフォルダを作成し、その直下に関連する全てのファイルを整理してください。特に `.qmd` ファイルとその出力もここに含めます。\")\n}\n\n2.4.1 具体的な使用例（renv = FALSE の場合）\n\nRスクリプトの保存: 上記のRコードを、例えば create_project_structure.R という名前で保存します。\nプロジェクト作成場所への移動: RコンソールまたはRStudioのターミナルで、新しいプロジェクトを作成したいディレクトリに移動します\n\n# 例：ホームディレクトリの「研究プロジェクト」フォルダに移動する場合\nsetwd(\"C:/Users/kotas/OneDrive/デスクトップ/Project\")\n\n関数の実行: 作成したいプロジェクトの名前を指定して、create_clinical_data_analysis_project_structure() 関数を呼び出します。今回は renv を使わないので、use_renv = FALSE を設定します。\n# 「MyClinicalStudy_2025」という名前のプロジェクトフォルダを作成し、renvは使用しない\ncreate_clinical_data_analysis_project_structure(\"ClinicalStudy\", use_renv = FALSE)\n結果の確認: コマンドを実行すると、指定したディレクトリ内に MyClinicalStudy_2025/ という新しいフォルダが作成され、その中に上記のフォルダ構造が自動的に生成されているのが確認できるはずです。Rコンソールには、フォルダ作成の進行状況と完了メッセージが表示されます。\n\nこれで、すぐにプロジェクト作業に取りかかれる、クリーンで整理された環境が手に入ります。"
  },
  {
    "objectID": "posts/statistics/2025/実データ解析フォルダ構成.html#まとめあなたの研究を次のレベルへ",
    "href": "posts/statistics/2025/実データ解析フォルダ構成.html#まとめあなたの研究を次のレベルへ",
    "title": "利用しない：臨床研究のための実データ解析フォルダ構成",
    "section": "2.5 まとめ：あなたの研究を次のレベルへ",
    "text": "2.5 まとめ：あなたの研究を次のレベルへ\nこのフォルダ構成は、単なるファイルの置き場所ではありません。臨床研究におけるデータ解析の透明性、再現性、そして監査可能性を飛躍的に向上させるための、体系的なワークフローそのものです。\n\n00_Resources: 知識の蓄積を忘れずに。\n01_Docs: 構造定義書と**統計解析計画書（SAP）**が解析の羅針盤。\n02_Data: 生データは「聖域」、加工履歴を明確に。\n03_Programs: ADSとTLF作成プログラムを明確に分離し、開発と確定を分けて管理。\n04_Output: ログこそが解析結果の揺るぎない証拠。\n05_Paper: 論文執筆をスムーズに。\n06_Meeting: Quartoを活用し、ミーティングでの進捗報告も効率的かつ明確に。\n\nこのテンプレートを活用して、あなたの臨床研究プロジェクトをより強固で、信頼性の高いものにしていきましょう！もし具体的な解析作業で疑問が生じたら、いつでも聞いてくださいね。"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html",
    "href": "posts/statistics/2025/中間解析.html",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "",
    "text": "中間解析は、試験の進行中にデータを評価し、治療効果や安全性に関する情報を得るために行われます。これにより、試験の進行状況を把握し、必要に応じて試験デザインやプロトコルを調整することができます。 中間解析は、特に以下の目的で実施されます。\n\n治療効果の初期評価: 中間解析は、治療効果の初期評価を行うために使用されます。これにより、治療が有効であるかどうかを早期に判断することができます。\n安全性の評価: 中間解析は、安全性に関する情報を収集するためにも使用されます。これにより、治療が安全であるかどうかを早期に判断することができます。\n早期終了の判断: 中間解析の結果に基づいて、試験を早期に終了するかどうか（無効中止や有効中止）を判断することができます。これにより、無駄なリソースを節約することができます。\n倫理的な配慮: 中間解析は、倫理的な配慮からも重要です。治療が有効でない場合や安全性に問題がある場合、試験を早期に終了することで、被験者の安全を確保することができます。\nリソースの最適化: 中間解析は、試験の進行状況を把握し、リソースを最適化するためにも使用されます。これにより、試験の効率を向上させることができます。\n\n本記事では、以下の内容について説明します。\n\n抗がん剤第2相における2値アウトカムの中間解析\n\nSimonの最適法\nSimonのMinmax法\nFlemingデザイン\nBayes流の方法\n\n検証的試験における中間解析\n\nO’Brien-Flemingデザイン\nPocockデザイン\nLan-DeMetsデザイン（α spending function）\n\n\n\n\n\n抗がん剤第2相試験においては、治療効果を評価するために2値アウトカム（例: 完全奏効、部分奏効、無効など）が使用されます。中間解析は、これらのアウトカムを評価するために行われます。試験統計家として試験計画時に中間解析を実施する必要があるかを考えておく必要があります。\nこれらの資料が参考になります。\n\n山本先生：SASユーザー総会2010\n\nまた、基本的に本節での手法はSASのProc power等のプロシジャで簡単に実装されていません。SASで実行する場合は、ネットからマクロを活用するか、社内でSASマクロを開発しておく必要があります。若干ハードルが高いかもしれないですが、SASでサンプルサイズ設計マクロを開発しておき、RやWebサイトの計算結果との一致をもってQCを行うことができるので、開発しておくことをお勧めします。\n\n\n\nこの節での記法について導入します。検証的試験における中間解析では別途記法を定義します。\n\np : 奏効確率（主要評価項目）\np_0 : 閾値奏効確率（p が p_0 以下の場合，薬剤は無効と判断）\np_1 : 期待奏効確率（p が p_1 以上の場合，薬剤は有効かもしれないと判断）\n\\alpha : 第 I 種の過誤確率（一般的に0.05と規定）\n\\beta : 第 II 種の過誤確率（一般的に0.20と規定）\n\n\n\n\n仮説検定は片側検定として以下のように定義する。\n\nH_0 : p \\leq p_0\nH_1 : p &gt; p_1"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#中間解析の目的",
    "href": "posts/statistics/2025/中間解析.html#中間解析の目的",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "",
    "text": "中間解析は、試験の進行中にデータを評価し、治療効果や安全性に関する情報を得るために行われます。これにより、試験の進行状況を把握し、必要に応じて試験デザインやプロトコルを調整することができます。 中間解析は、特に以下の目的で実施されます。\n\n治療効果の初期評価: 中間解析は、治療効果の初期評価を行うために使用されます。これにより、治療が有効であるかどうかを早期に判断することができます。\n安全性の評価: 中間解析は、安全性に関する情報を収集するためにも使用されます。これにより、治療が安全であるかどうかを早期に判断することができます。\n早期終了の判断: 中間解析の結果に基づいて、試験を早期に終了するかどうか（無効中止や有効中止）を判断することができます。これにより、無駄なリソースを節約することができます。\n倫理的な配慮: 中間解析は、倫理的な配慮からも重要です。治療が有効でない場合や安全性に問題がある場合、試験を早期に終了することで、被験者の安全を確保することができます。\nリソースの最適化: 中間解析は、試験の進行状況を把握し、リソースを最適化するためにも使用されます。これにより、試験の効率を向上させることができます。\n\n本記事では、以下の内容について説明します。\n\n抗がん剤第2相における2値アウトカムの中間解析\n\nSimonの最適法\nSimonのMinmax法\nFlemingデザイン\nBayes流の方法\n\n検証的試験における中間解析\n\nO’Brien-Flemingデザイン\nPocockデザイン\nLan-DeMetsデザイン（α spending function）"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#抗がん剤第2相における2値アウトカムの中間解析",
    "href": "posts/statistics/2025/中間解析.html#抗がん剤第2相における2値アウトカムの中間解析",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "",
    "text": "抗がん剤第2相試験においては、治療効果を評価するために2値アウトカム（例: 完全奏効、部分奏効、無効など）が使用されます。中間解析は、これらのアウトカムを評価するために行われます。試験統計家として試験計画時に中間解析を実施する必要があるかを考えておく必要があります。\nこれらの資料が参考になります。\n\n山本先生：SASユーザー総会2010\n\nまた、基本的に本節での手法はSASのProc power等のプロシジャで簡単に実装されていません。SASで実行する場合は、ネットからマクロを活用するか、社内でSASマクロを開発しておく必要があります。若干ハードルが高いかもしれないですが、SASでサンプルサイズ設計マクロを開発しておき、RやWebサイトの計算結果との一致をもってQCを行うことができるので、開発しておくことをお勧めします。"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#パラメータの定義",
    "href": "posts/statistics/2025/中間解析.html#パラメータの定義",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "",
    "text": "この節での記法について導入します。検証的試験における中間解析では別途記法を定義します。\n\np : 奏効確率（主要評価項目）\np_0 : 閾値奏効確率（p が p_0 以下の場合，薬剤は無効と判断）\np_1 : 期待奏効確率（p が p_1 以上の場合，薬剤は有効かもしれないと判断）\n\\alpha : 第 I 種の過誤確率（一般的に0.05と規定）\n\\beta : 第 II 種の過誤確率（一般的に0.20と規定）"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#仮説検定",
    "href": "posts/statistics/2025/中間解析.html#仮説検定",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "",
    "text": "仮説検定は片側検定として以下のように定義する。\n\nH_0 : p \\leq p_0\nH_1 : p &gt; p_1"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#arguments",
    "href": "posts/statistics/2025/中間解析.html#arguments",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "2.1 Arguments",
    "text": "2.1 Arguments\n\n\n\n\n\n\n\nパラメータ\n説明\n\n\n\n\npu\nunacceptable response rate; baseline response rate that needs to be exceeded for treatment to be deemed promising\n\n\npa\nresponse rate that is desirable; should be larger than pu\n\n\nep1\nthreshold for the probability of declaring drug desirable under pu (target type 1 error rate); between 0 and 1\n\n\nep2\nthreshold for the probability of rejecting the drug under pa (target type 2 error rate); between 0 and 1\n\n\nnmax\nmaximum total sample size (default 100; can be at most 1000)\n\n\nx\nobject returned by ph2simon\n\n\n…\narguments to be passed onto plot and print commands called within"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#value",
    "href": "posts/statistics/2025/中間解析.html#value",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "2.2 Value",
    "text": "2.2 Value\nph2simon returns a list with pu, pa, alpha, beta and nmax as above and:\n\n\n\n\n\n\n\n出力\n説明\n\n\n\n\nout\nmatrix of best 2 stage designs for each value of total sample size n. The 6 columns in the matrix are:\n\n\n\n\n\n\nカラム\n説明\n\n\n\n\nr1\nnumber of responses needed to exceeded in first stage\n\n\nn1\nnumber of subjects treated in first stage\n\n\nr\nnumber of responses needed to exceeded at the end of trial\n\n\nn\ntotal number of subjects to be treated in the trial\n\n\nEN(pu)\nexpected number of patients in the trial under pu\n\n\nPET(pu)\nprobability of stopping after the first stage under pu\n\n\n\nTrial is stopped early if &lt;= r1 responses are seen in the first stage and treatment is considered desirable only when &gt;r responses seen."
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#methods",
    "href": "posts/statistics/2025/中間解析.html#methods",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "2.3 Methods",
    "text": "2.3 Methods\n\nprint(ph2simon): formats and returns the minimax, optimal and any admissible designs.\nplot(ph2simon): plots the expected sample size against the maximum sample size as in Jung et al., 2001"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#実務に応用する際において",
    "href": "posts/statistics/2025/中間解析.html#実務に応用する際において",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "2.4 実務に応用する際において",
    "text": "2.4 実務に応用する際において\nSimonのMinmax、SimonのOptimanデザインのいずれの方法においても、「薬剤が無効な場合に早期中止を判断するための2段階デザイン無効な場合の期待患者数/を最小にしたい」という無効中止のみを考えた試験デザインである。 すなわち、Rの出力結果から、第1段階目のn1人において、r1人未満の奏効例数であれば、試験を無効中止とするようなデザインである。Rのパッケージで症例数設計をする場合、この数値が正しいことを試験統計家としてValidationをしておく必要はある。"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#数学的背景について",
    "href": "posts/statistics/2025/中間解析.html#数学的背景について",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "2.5 数学的背景について",
    "text": "2.5 数学的背景について\n別記事で解説します。"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#試験概要",
    "href": "posts/statistics/2025/中間解析.html#試験概要",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "6.1 試験概要",
    "text": "6.1 試験概要\n今回設計した臨床試験の基本パラメータと解析計画をまとめました。この試験では2段階の逐次デザインを採用し、中間解析で早期中止の可能性を検討します。"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#試験設計パラメータ",
    "href": "posts/statistics/2025/中間解析.html#試験設計パラメータ",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "6.2 試験設計パラメータ",
    "text": "6.2 試験設計パラメータ\n\n\n\n項目\n設定値\n\n\n\n\n登録期間\n2年\n\n\n追跡期間\n5年\n\n\n目標症例数\n126.7例（2群計128例）63.25例/年\n\n\n期待イベント数\n96.5例（対照群: 55.4例、治療群: 41.1例）"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#解析スケジュール",
    "href": "posts/statistics/2025/中間解析.html#解析スケジュール",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "6.3 解析スケジュール",
    "text": "6.3 解析スケジュール\n\n6.3.1 中間解析（第1段階）\n\n実施時期: 試験開始から3年後（2.92年）\n期待イベント数: 48.3例（対照群: 30.4例、治療群: 17.9例）\n\n\n\n6.3.2 最終解析（第2段階）\n\n実施時期: 全症例の追跡完了後\n期待イベント数: 96.5例"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#統計的判定基準",
    "href": "posts/statistics/2025/中間解析.html#統計的判定基準",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "6.4 統計的判定基準",
    "text": "6.4 統計的判定基準\n\n\n\n\n\n\n\n\n\n解析段階\n統計量\n判定基準\n結論\n\n\n\n\n中間解析\nZ₁ &gt; 2.96259\n有効性境界を超過\n有効中止\n\n\n\nZ₁ &lt; 0.86994\n無効性境界を下回る\n無効中止\n\n\n\n0.86994 ≤ Z₁ ≤ 2.96259\n境界値の間\n試験継続\n\n\n最終解析\nZ₂ &gt; 1.89189\n有効性境界を超過\n有効"
  },
  {
    "objectID": "posts/statistics/2025/中間解析.html#まとめ",
    "href": "posts/statistics/2025/中間解析.html#まとめ",
    "title": "第2相抗がん剤開発及び検証的試験の中間解析",
    "section": "6.5 まとめ",
    "text": "6.5 まとめ\nこの逐次デザインにより、試験の途中で明確な結果が得られた場合には早期中止が可能となり、患者さんの負担軽減と試験の効率化が期待できます。特に、中間解析時点で強い有効性が示された場合や、逆に無効性が明らかになった場合には、倫理的観点からも適切な判断ができる設計となっている。\nこの後は、Proc lifetest Procedure等で実際に解析を行い推定値に基づくZ統計量を算出した上で、Proc Seqtest Procedureで中間解析の結果を評価することとなる。"
  },
  {
    "objectID": "posts/statistics/2025/TTE総説.html",
    "href": "posts/statistics/2025/TTE総説.html",
    "title": "TTE総説論文",
    "section": "",
    "text": "本記事では、薬剤疫学会で公開された下記2つの論文を勉強する。"
  },
  {
    "objectID": "posts/statistics/2025/TTE総説.html#要約",
    "href": "posts/statistics/2025/TTE総説.html#要約",
    "title": "TTE総説論文",
    "section": "1 要約",
    "text": "1 要約\n\n観察研究におけるバイアス\n\nランダム化の欠如による交絡\n不適切な研究デザインがもたらす選択バイアス\nImmortal Time bias"
  },
  {
    "objectID": "posts/statistics/2025/SQL入門1.html",
    "href": "posts/statistics/2025/SQL入門1.html",
    "title": "SQL入門1",
    "section": "",
    "text": "本記事では、SASによるProc SQL Procedureについて解説する。参考文献は以下の通りである。\n\n\n\n2021年度SASユーザー総会：臨床試験のデータハンドリングとSQLプロシジャ\n2007年度SASユーザー総会：臨床試験データの加工におけるSAS/Proc SQL の活用事例：データセット併合と図表作成\n2007年度SASユーザー総会：SQL プロシジャの利用－安全性の集計を題材に－\n2016年度：データハンドリングにおけるSQLプロシジャの利活用 -PROC SQL入門ー"
  },
  {
    "objectID": "posts/statistics/2025/SQL入門1.html#参考文献",
    "href": "posts/statistics/2025/SQL入門1.html#参考文献",
    "title": "SQL入門1",
    "section": "",
    "text": "2021年度SASユーザー総会：臨床試験のデータハンドリングとSQLプロシジャ\n2007年度SASユーザー総会：臨床試験データの加工におけるSAS/Proc SQL の活用事例：データセット併合と図表作成\n2007年度SASユーザー総会：SQL プロシジャの利用－安全性の集計を題材に－\n2016年度：データハンドリングにおけるSQLプロシジャの利活用 -PROC SQL入門ー"
  },
  {
    "objectID": "posts/statistics/2025/SQL入門1.html#利点1事前ソートが不要",
    "href": "posts/statistics/2025/SQL入門1.html#利点1事前ソートが不要",
    "title": "SQL入門1",
    "section": "2.1 利点1：事前ソートが不要",
    "text": "2.1 利点1：事前ソートが不要\nData Stepの場合：\n/* 各データセットを事前にソートする必要がある */\nproc sort data=dataset1; by id; run;\nproc sort data=dataset2; by id; run;\ndata merged;\n    merge dataset1 dataset2;\n    by id;\nrun;\nProc SQLの場合：\n/* ソート不要で直接結合可能 */\nproc sql;\n    create table merged as\n    select * from dataset1 a\n    left join dataset2 b\n    on a.id = b.id;\nquit;"
  },
  {
    "objectID": "posts/statistics/2025/SQL入門1.html#利点2併合と同時にソートが可能",
    "href": "posts/statistics/2025/SQL入門1.html#利点2併合と同時にソートが可能",
    "title": "SQL入門1",
    "section": "2.2 利点2：併合と同時にソートが可能",
    "text": "2.2 利点2：併合と同時にソートが可能\nProc SQLでは、order byを用いることにより、データセットの併合と同時にデータをソートすることが可能である。そのため、データセットの併合後にProc sortで改めてソートすることはせずに、Proc SQLのみで目的に応じた並び順にすることが可能である。\nproc sql;\n    create table result as\n    select * from dataset1 a\n    left join dataset2 b\n    on a.id = b.id\n    order by id, visit_date;  /* 結合と同時にソート */\nquit;"
  },
  {
    "objectID": "posts/statistics/2025/SQL入門1.html#利点3同名変数の上書き回避",
    "href": "posts/statistics/2025/SQL入門1.html#利点3同名変数の上書き回避",
    "title": "SQL入門1",
    "section": "2.3 利点3：同名変数の上書き回避",
    "text": "2.3 利点3：同名変数の上書き回避\nData Stepの問題： 併合前の2つのデータセットに同じ変数名が存在する場合、データステップでマージするとその変数名のデータは上書きされてしまう\nProc SQLの解決策：\n/* 表4.1.5のような状況での外部結合 */\nproc sql;\n    create table result as\n    select a.subject_id, a.sex as patient_sex, a.test_code, a.value,\n           b.sex as reference_sex, b.lower_limit, b.upper_limit\n    from measurement_data a\n    left join reference_data b\n    on a.test_code = b.test_code\n    and (b.sex = '.' or a.sex = b.sex);\nquit;\n基準値データセットに「性別」のデータが存在し、かつ測定値データセットの「性別」と他の異なるオブザベーションは結合データセットから削除される"
  },
  {
    "objectID": "posts/statistics/2025/SQL入門1.html#利点4集計関数による個別値と平均値の比較",
    "href": "posts/statistics/2025/SQL入門1.html#利点4集計関数による個別値と平均値の比較",
    "title": "SQL入門1",
    "section": "2.4 利点4：集計関数による個別値と平均値の比較",
    "text": "2.4 利点4：集計関数による個別値と平均値の比較\n従来の方法の問題： 従来方向のデータの平均値を求めることは、SAS関数のmeanを用いることでデータステップでも可能である。しかし、オブザベーション方向のデータの平均値を求めるためには、Proc MEANSなどの別のプロシジャを用いる必要がある。また、retainステートメントを用いたオブザベーション方向の累計の計算により、データステップでもオブザベーション方向の平均値を求めることは可能であるが、個々の測定値と求めた平均値を比較するためには別工程の処理が必要である。\nProc SQLの解決策：\nproc sql;\n    create table comparison as\n    select subjid, paramcd, value,\n           mean(value) as group_mean,\n           value - mean(value) as deviation_from_mean\n    from lab_data\n    group by paramcd;\nquit;\nそれに対して、Proc SQLでは、集計するための関数を用いることで、個々の測定値とオブザベーション方向の平均値を比較することが可能となる。"
  },
  {
    "objectID": "posts/statistics/2025/SQL入門1.html#基本構文",
    "href": "posts/statistics/2025/SQL入門1.html#基本構文",
    "title": "SQL入門1",
    "section": "3.1 基本構文",
    "text": "3.1 基本構文\n1つのデータセットを加工して、1つのデータセットを作成する場合の基本的なProc SQLの構文：他にも諸々の指定ができるが、基礎的な事項は以下の通りである。\nProc SQL ;\n    create table 作成データセット名    as\n    select      元のデータセット名.変数名1,\n                元のデータセット名.変数名2    as  変更後の変数名2\n    from        元のデータセット名\n    where       データ抽出の条件1\n    group by    グループ分け\n    having      データ抽出の条件2\n    order by    ソート変数\n;\n複数SQLステートメントのまとめ書き\nproc sql &lt;オプション&gt; ;\n    sqlステートメント1 ;\n    sqlステートメント2 ;\n    sqlステートメント3 ;\n;\nquit;"
  },
  {
    "objectID": "posts/statistics/2025/SQL入門1.html#各構文要素の説明",
    "href": "posts/statistics/2025/SQL入門1.html#各構文要素の説明",
    "title": "SQL入門1",
    "section": "3.2 各構文要素の説明",
    "text": "3.2 各構文要素の説明\n\n3.2.1 create table 作成データセット名 as\n作成するデータセットの名称を定義する。\n\n\n3.2.2 select\nselect  元のデータセット名.変数名1,\n        元のデータセット名.変数名2 as 変更後の変数名2\n\n作成するデータセットに保存する変数を定義する（データステップではkeepステートメントに相当）\n元のデータセットにおける変数名を指定した後に「as」を加え、変更後の変数名を記載することで、作成するデータセットに保存する変数の変数名を変更することが可能（データステップではrenameステートメントに相当）\n基本的に作成するデータセットに保存する全ての変数を定義する必要がある\n元のデータセットに存在する全ての変数をそのまま用いる場合には「*」で代用することも可能\n定義された変数名の順でデータセットが作成されるため、変数の並び順を変更することが簡単\n\n\n\n3.2.3 from 元のデータセット名\n元のデータセットを指定する（データステップではsetステートメントに相当）。\n\n\n3.2.4 where データ抽出の条件1\n元のデータセットからデータを抽出する条件を指定する。\n\n\n3.2.5 group by グループ分け\n集計する場合などの状況においてグループ分けの条件を設定する。\n\n\n3.2.6 having データ抽出の条件2\n元のデータセットからデータを抽出する条件を指定する。集計するための関数を利用して条件を指定する場合には、「where」ではなく「having」で指定する。\n\n\n3.2.7 order by ソート変数\nデータのソートに用いる変数を指定する。Proc SQLでは、データセット作成と同時にデータをソートすることが可能である。"
  },
  {
    "objectID": "posts/statistics/2025/SQL入門1.html#data-stepとの比較",
    "href": "posts/statistics/2025/SQL入門1.html#data-stepとの比較",
    "title": "SQL入門1",
    "section": "3.3 Data Stepとの比較",
    "text": "3.3 Data Stepとの比較\n上記のProc SQLで作成したプログラムをデータステップおよびProc SQL以外のプロシジャで作成する場合：\nData 作成データセット名 ;\n    set 元のデータセット名 ;\n    where       データ抽出の条件1 ;\n    rename      変数名2 = 変更後の変数名2 ;\n    keep        変数名1  変更後の変数名2 ;\n    if          データ抽出の条件2 ;\nrun ;\n\nProc sort data=作成データセット名 ;\n    by ソート変数 ;\nrun ;"
  },
  {
    "objectID": "posts/statistics/2025/SQL入門1.html#proc-sqlの利点",
    "href": "posts/statistics/2025/SQL入門1.html#proc-sqlの利点",
    "title": "SQL入門1",
    "section": "3.4 Proc SQLの利点",
    "text": "3.4 Proc SQLの利点\n\n複数の工程を1工程にまとめられる：プログラム作成においてエラーを少なくするという利点がある\n変更箇所の特定が容易：変更箇所を見つけやすいため、変数名の変更や条件の変更を行なう際に変更漏れを少なくするという利点がある\n統一された記法：Proc SQLでは「Proc SQL;」～「quit」、データステップでは「Data」～「run;」、Proc SQL以外のプロシジャでは「Proc」～「run;」を1工程と表現する"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "",
    "text": "大阪SASユーザー総会の下記資料が大変勉強になるので、自分の備忘録用にコピペさせていただきた。基本的に下記資料をみればよい。\n\nThink Framework"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#はじめに",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#はじめに",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "2.1 はじめに",
    "text": "2.1 はじめに\nSASプログラミングにおいて、毎回同じような処理を一から書くのは非効率的です。本記事では、Yugo Mikiさんの「Think Framework」の知見を参考に、SASプログラムをフレームワーク化することで、開発効率を大幅に向上させる手法について解説します。"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#library-vs-framework根本的な発想の違い",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#library-vs-framework根本的な発想の違い",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "2.2 Library vs Framework：根本的な発想の違い",
    "text": "2.2 Library vs Framework：根本的な発想の違い\n\n2.2.1 従来のLibraryアプローチ\n\nマクロライブラリなどが有名\nライブラリからマクロやプログラムを参照してプログラム中に利用する\nプログラムの再利用に特化した使い方\nプログラミングコードを部品化し、組み合わせる事でプログラムを作成する\n\n\n\n2.2.2 新しいFrameworkアプローチ\n\nSAS LSAFなどが有名。一方、「それって一体何なの？」ってくらい実体が見えない\n一般的な機能を持つ共通コードを持つ\n標準的なコードはフレームワークが持ち、要求されるコードの実現にプログラマーのリソースを集中させる事で効率化を達成する"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#だからどういうことなのさ",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#だからどういうことなのさ",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "2.3 「だからどういうことなのさ？」",
    "text": "2.3 「だからどういうことなのさ？」\nつまり：\n\nいつも書くコードはフレームワークに持たせましょう！\n\n- いつも使う機能はフレームワークに持たせましょう！\n- よく使うマクロとかもフレームワークに持たせましょう！\n- ついでに社内ルールとかもフレームワークに持たせておきましょう！\nってこと。\nこれをSASで良い感じに実現して、快適な開発環境を作りましょう！"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#sasプログラムの要素分析とフレームワーク化判断",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#sasプログラムの要素分析とフレームワーク化判断",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "2.4 SASプログラムの要素分析とフレームワーク化判断",
    "text": "2.4 SASプログラムの要素分析とフレームワーク化判断\n\n2.4.1 SDTM to ADaM変換の場合\n\n\n\n\n\n\n\n\n処理項目\n考察\nフレームワーク化判断\n\n\n\n\nClear log and data\n毎回使う。対象とする。\n✅ 完全自動化\n\n\nSetting macros\n毎回使うところだけ対象とする。\n⚡ 部分自動化\n\n\nSetting library\n毎回違う。出来るだけ一箇所に集約したい。\n🔧 設定管理\n\n\nSetting format\n仕様書のコードリストにあるものは対象とする。\n✅ 完全自動化\n\n\nImport data and files\nファイル名と仕様が毎回違う。やや無理。\n⚡ テンプレート化\n\n\nMerge\n毎回違う。対象外。\n❌ プログラマーの腕の見せ所\n\n\nMapping to new variables\n毎回違う。対象外。\n❌ プログラマーの腕の見せ所\n\n\nSort\nマクロ化してライブラリへ。\n✅ 共通化\n\n\nDrop extra variables\n仕様書から自動取得。\n✅ メタデータ連携\n\n\nUpload\nマクロ化してライブラリへ。\n✅ 共通化\n\n\nHeader, sections, comments\nSOPや手順との兼ね合い。対象とする。\n✅ テンプレート自動生成\n\n\n\n\n\n2.4.2 ADaM to TFLs変換の場合\n\nClear log and data\nSetting macros\nSetting library\nSetting format\nImport data and files\nMerge\nCall procedures（プロシージャの呼び出し）\nSort\nMake up to requests from mockup（モックアップからのリクエスト作成）\nUpload\nHeader, section, and comments\n\n🎯 重要な気づき\nプログラマーに頑張って欲しい所はここ！（MergeとMapping）\nということはそれ以外の所はフレームワークに持たせた方がいい！"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#sasによるフレームワーク設計",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#sasによるフレームワーク設計",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "2.5 SASによるフレームワーク設計",
    "text": "2.5 SASによるフレームワーク設計\n\n2.5.1 基本コンセプト\n\nSASプログラム開発時を想定\nSASによるプログラムテンプレート作成の一歩先\n\n\n\n2.5.2 URS（User Requirement Specification）\n\n使用するのはBase SASと一般的な外部ファイル（EXCEL）\nプログラムからプログラムをcallしても良い\n本番実行時のみログを外部に保管\nマクロは今回はmacro.sasを毎回includeする\nフレームワーク化検討のほぼ全てを実装する\n\n\n\n2.5.3 フレームワーク仕様\n\n2.5.3.1 全実行プログラム\n\n全実行時ログを保管\n\n\n\n2.5.3.2 Macro.sas\n\nSort\nインポート（proc sort）\nエクスポート（proc sort）\n仕様書から変数のattribute作成\n仕様書からcodelistのformat作成\n仕様書からkeepする変数リスト作成\n\n\n\n2.5.3.3 Each_dsn.sas\n\nStand aloneで動く\n実施時にログ、データセット等をクリアする\n社内規則に従ったheader, section, commentを作成する\n\nプログラムにおける面倒な”いつもの”を持たせておくのがフレームワークです。"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#実装例とテクニック",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#実装例とテクニック",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "2.6 実装例とテクニック",
    "text": "2.6 実装例とテクニック\n\n2.6.1 全実行プログラム：環境分離の巧妙な仕組み\n/* 基本的にはマクロで実装 */\n%macro derive(dsn, title);\n   proc printto log = \"&path.¥&project._&dsn._log.txt\" new;\n   title1 \"&project.:&title.\";\n   run;\n   \n   %include \"&pgpath.¥&dsn..sas\";\n   \n   proc printto; \n   run;\n%mend;\n\n%derive(adsl)\n%derive(adae)\nPrinttoでincludeを挟むだけで開発環境と本番実行環境を分離できる！ 開発時：それぞれのプログラムで実行。ログはログ画面へ出力される 本番実行時：ログはprinttoで指定したフォルダへ出力される\n%macro derive(lib, dsn, key, where);\n    proc sort data = &lib..&dsn out = work.&dsn;\n        by &key.;\n        &where.;\n    run;\n%mend;\nMacroはlibraryにして保管・管理しておくと楽です。今回のmacro.sasはそれ自体がマクロライブラリとして機能します。 フレームワークからライブラリが読み込まれるような仕様にしています。 個別プログラムの構造化 ログクリアとヘッダーの順序問題 常識的に考えるとheaderが先である。だって”head”erだから。でも少し考えてみてもいい。\n/* パターン1 */\ndm \"log; clear;\";\n/* program name : test */\n/* author : anonymous */\n\n/* パターン2 */\n/* program name : test */\n/* author : anonymous */\ndm \"log; clear;\";\nどっちも同じ？ でも少し考えてみてもいい。もし、printtoではなくdmステートメントでログを保存していたら？後者の場合、ログからheaderが消えてしまう。どうやってログを残すかによって使えないパターンがあるので注意しよう。\nPhUSE GPPの参考から持ってきました。立派なヘッダーです。 これを作るの時間かかるんじゃない？ → こういうものはフレームワークに持たせましょう。 ちょっと工夫すると結構作れちゃったりするものです。 よく見たら進捗管理ファイル、帳票一覧、ADaM仕様書にほとんどの情報が入っている。\n/***********************************************************************\n* Project         : Sample Drug, Sample Indication,Study1\n* Program name    : sample.sas\n* Author          : smithb\n* Date created    : 20100816\n* Purpose         : Summarize demographics data for the study.\n* Revision History :\n* Date        Author      Ref     Revision (Date in YYYYMMDD format)\n* 20100818    smithb      1       Removed subjects with who have not been dosed per spec.\n***********************************************************************/"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#マクロの使用パターン",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#マクロの使用パターン",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "2.7 マクロの使用パターン",
    "text": "2.7 マクロの使用パターン\nマクロはいつもどうやって使う？\n/* 1. %include statement */\n%include \"macro.sas\";\n\n/* 2. Set auto compiled macro */\nFILENAME fileref 'the path to the AUTOCALL library';\nOPTIONS MAUTOSOURCE SASAUTOS=(SASAUTOS fileref);\n\n/* 3. Set stored macro */\nLIBNAME mylib 'C:¥temp';\nOPTIONS MSTORED SASMSTORE=mylib;"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#実践的な考察",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#実践的な考察",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "6.1 実践的な考察",
    "text": "6.1 実践的な考察\n\n6.1.1 良かった点\n\nHeaderの記入箇所が少なくて楽\n毎回やってた設定が不要になり、ミスもなくなった\n余計な設定や処理に悩まされないので、データを見る時間などに作業時間を使えている気がする\n\n\n\n6.1.2 良くなかった点（正直な評価）\n\n簡単なプログラムのはずなんだけどやたらとコードが長い\n少しプログラムの構造が変わると、適用できない\n開発計画が大きく変わると対応できない"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#組織レベルでの戦略的価値",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#組織レベルでの戦略的価値",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "6.2 組織レベルでの戦略的価値",
    "text": "6.2 組織レベルでの戦略的価値\n\n6.2.1 会社としての評価\n\nやってみてもいい。\n申請や海外に提供するなど、プログラムへの要求が高くなればなるほど威力が出る\n会社として基本的なプログラムの書き方やセクションわけなどを決めてしまっても、この方法ならコスト高にならない。むしろ、いつもここにこういう事が書かれていると分かった方が開発、またはそのための教育においても有利。"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#フレームワーク化の本質的な価値",
    "href": "posts/statistics/2025/SASプログラミング業務のフレームワーク.html#フレームワーク化の本質的な価値",
    "title": "SASプログラミング業務のフレームワーク",
    "section": "6.3 フレームワーク化の本質的な価値",
    "text": "6.3 フレームワーク化の本質的な価値\n\n6.3.1 抽象化レベルの設定\n\n6.3.1.1 高レベル抽象化（完全自動化）\n\n環境設定、ログ管理、基本的なハウスキーピング\n一度設定すれば、プログラマーは意識する必要がない\n\n\n\n6.3.1.2 中レベル抽象化（テンプレート化）\n\nプログラム構造、ヘッダー、セクション分け\n骨格は提供するが、中身はプログラマーが記述\n\n\n\n6.3.1.3 低レベル抽象化（ユーティリティ提供）\n\nよく使う処理の関数化\n呼び出すかどうかはプログラマーの判断\n\n\n\n\n6.3.2 組織的導入戦略\n\n6.3.2.1 段階的導入アプローチ\n\nPhase 1: 基盤整備：標準的なフォルダ構造とユーティリティマクロ\nPhase 2: テンプレート化：プログラムテンプレートとヘッダー標準化\nPhase 3: 自動化拡張：設定管理の自動化とバリデーション機能\nPhase 4: 最適化：継続的な改善とパターンの蓄積\n\n\n\n\n6.3.3 フレームワーク化の真の価値は：\n🎯 焦点の明確化 本当に重要な作業（データ変換ロジック、分析手法の選択）に集中できる環境の構築\n🚀 スケーラビリティの確保 個人の経験や知識に依存しない、組織として継続可能な開発体制\n🔄 継続的改善の文化 一度作って終わりではなく、常に最適化し続ける仕組み\n🤝 協働の促進 標準化されたアプローチにより、チームメンバー間の連携と知識共有の促進"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html",
    "href": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html",
    "title": "SASプログラミングのPitfalls and Bad Habits",
    "section": "",
    "text": "How Not to SAS: Avoiding Common Pitfalls and Bad Habits\nSAS is a powerful tool for data analysis, but its flexibility can sometimes lead you into developing bad programming habits. Although these shortcuts might not break your programs immediately, they can lead to inefficient, error-prone, and hard-to-maintain code. This paper identifies common pitfalls and provides straightforward, practical solutions to avoid them.\nEffective code organization is foundational to successful SAS programming. Yet, it’s easy to overlook best practices and fall into poor habits. Here are three common pitfalls you should avoid:"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#本記事では以下の文献を参考にsasのpitfallsとbad-hatibsをまとめます",
    "href": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#本記事では以下の文献を参考にsasのpitfallsとbad-hatibsをまとめます",
    "title": "SASプログラミングのPitfalls and Bad Habits",
    "section": "",
    "text": "How Not to SAS: Avoiding Common Pitfalls and Bad Habits\nSAS is a powerful tool for data analysis, but its flexibility can sometimes lead you into developing bad programming habits. Although these shortcuts might not break your programs immediately, they can lead to inefficient, error-prone, and hard-to-maintain code. This paper identifies common pitfalls and provides straightforward, practical solutions to avoid them.\nEffective code organization is foundational to successful SAS programming. Yet, it’s easy to overlook best practices and fall into poor habits. Here are three common pitfalls you should avoid:"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#code-organization-readability",
    "href": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#code-organization-readability",
    "title": "SASプログラミングのPitfalls and Bad Habits",
    "section": "2 CODE ORGANIZATION ＆ READABILITY",
    "text": "2 CODE ORGANIZATION ＆ READABILITY\nGood Practice\n\nUse Clear and concise SAS Comments in SAS Code\n\nblock comments like /* my comment */\nsingle-line comments like * my comment;\n\n\n/* my program comment */\ndata one; set sashelp.cars;\n*subset to certain car types - SUV;\nwhere type = \"SUV\";\nrun;\nBad Tips\n\nWRITE ALL YOUR CODE IN ONE LONG, CONTINUOUS BLOCK\n\n「コードを延々とスクロールして見ることは、忍耐力と視力を試すエキサイティングな方法だから」。コードを1つの巨大なブロックとして書くのは、よくある有害な習慣です。このような方法はデバッグやコード修正を煩雑にし、エラーが見逃される可能性を高めます。\n推奨される方法：モジュラーにコードを書く\nコードをより小さく、論理的なステップに分割し、明確なヘッダーやモジュラーセクションを使って各ブロックの目的を定義しましょう。モジュラーコードは読みやすさを向上させるだけでなく、他のプロジェクトや分析でセクションを簡単に再利用でき、貴重な時間を節約できます。\n/* 悪い例：すべてが1つのブロック */\ndata work.analysis;set mylib.rawdata;if age &gt;= 18 and status='Active';length category $20;if score &gt;= 90 then category='Excellent';else if score &gt;= 80 then category='Good';else category='Needs Improvement';run;proc sort data=work.analysis;by category descending score;run;proc means data=work.analysis mean std;class category;var score;output out=work.summary mean=avg_score std=std_score;run;\n\n/* 良い例：モジュラー構造 */\n/*************************************/\n/* Step 1: Data Filtering & Cleanup */\n/*************************************/\ndata work.filtered_data;\n    set mylib.rawdata;\n    where age &gt;= 18 and status = 'Active';\nrun;\n\n/********************************/\n/* Step 2: Category Assignment */\n/********************************/\ndata work.categorized_data;\n    set work.filtered_data;\n    length category $20;\n    \n    if score &gt;= 90 then category = 'Excellent';\n    else if score &gt;= 80 then category = 'Good';\n    else category = 'Needs Improvement';\nrun;\n\n/************************/\n/* Step 3: Data Sorting */\n/************************/\nproc sort data=work.categorized_data;\n    by category descending score;\nrun;\n\n/****************************/\n/* Step 4: Summary Analysis */\n/****************************/\nproc means data=work.categorized_data mean std;\n    class category;\n    var score;\n    output out=work.summary \n           mean=avg_score \n           std=std_score;\nrun;\nGood Practice：\n\nUse modular, structured code\n\nclearly separate different steps with comments\nさっきと同じことですね。\n\n\nBad Tips：\n\nUSE UNCLEAR OR ARBITRARY VARIABLE NAMES （不明確または恣意的な変数名を使用する）\n\nVAR1やXのような暗号的な変数名を使うことは、あなたのコードを、あなた自身や後にそのコードを引き継ぐ人にとって解けないパズルゲームに変えてしまいます。VAR1、X、TEMPのような貧弱な変数名を選択することは、可読性を低下させ、コードに混乱をもたらします（Program 2）。また、汎用的な名前は、エラーや結果の誤解釈の可能性を高めます。\n推奨：意味のある説明的な変数名を使用する\ndata new;\n    set old;\n    x = a * b;\n    y = x + c;\n    if z &gt; 10 then flag = 1;\n    temp = var1 / var2;\nrun;\nGood Practice：Use meaningful and descriptive variable names\n意味のある説明的な変数名（Xの代わりにCustomerAgeなど）を使用することで、あなたや同僚が各変数の目的と内容を素早く把握できるようになります（Program 3）。アンダースコア（customer_age）やキャメルケース（CustomerAge）などの一貫した命名規則を採用することで、さらに明確性が向上します。明確に命名された変数は、デバッグを簡素化し、分析中のエラーの可能性を大幅に削減します。\ndata salaryinfo2021;\n    set salaryinfo2020;\n    newsalary = oldsalary + increase;\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#debugging-error-handling",
    "href": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#debugging-error-handling",
    "title": "SASプログラミングのPitfalls and Bad Habits",
    "section": "3 DEBUGGING ＆ ERROR HANDLING",
    "text": "3 DEBUGGING ＆ ERROR HANDLING\nProper debugging and error handling are critical for creating reliable SAS programs. Here are common pitfalls and the best practices you should follow:\nBad Tips:\n\nIGNORING THE SAS LOG WINDOW\n\nRed text is just a suggestion. Who needs to debug when you can keep running the code? Ignoring the log window means missing critical information about errors and warnings, causing unnoticed mistakes and incorrect results.\nGood Practices:\n\nCheck all the messages in your log\n\nAlways check for ERROR, WARNING, and NOTE messages in your log. Each of these messages can indicate fatal failures in your code . Understanding SAS Log Messages:\n\nERROR: Critical issues that prevent SAS from executing your code. Your results are incomplete or incorrect until these are resolved.\nWARNING: Potential issues that SAS identifies but doesn’t stop execution. These should be reviewed and addressed to ensure accuracy.\nNOTE: Informational messages about code execution. These offer insights into dataset creation, memory usage, and other operational details.\n\nBad Tips:\n\nNEVER USE DEBUGGING OPTIONS\n\nSkipping debugging options is a great way to keep your coding life exciting—who doesn’t love spending hours chasing hidden bugs? Avoiding the use of debugging options can significantly hinder your ability to troubleshoot and resolve standard SAS code and macro-related issues efficiently.\nGood Practices:\n\nUse system options to help your debug SAS code\n\n便利なGlobal option\n\nMSGLEVEL=I: Provides additional informational messages in the log, especially useful when merging datasets to identify issues such as mismatches or data alignment problems.（ndefined）\nSOURCE: Displays the original SAS statements in the log.\nSOURCE2: Shows included SAS code from %INCLUDE statements.\nFMTERR: Issues an error if a specified format cannot be found.\nDSNFERR: Issues an error when a referenced dataset does not exist.\nOBS=0: Compiles the program without executing it, useful for syntax checking.\nNOREPLACE: Prevents accidental overwriting of existing datasets.\n\nプログラム開発時（デバック環境）と本番環境実行時でglobal optionを使い分けることができると上級者になれるかもしれないですね。\noptions MSGLEVEL=I \n        SOURCE \n        SOURCE2 \n        FMTERR \n        DSNFERR \n        OBS=0 \n        NOREPLACE;\nマクロ実行時は以下のglobal optionが役に立つ。\nUse debugging options specifically designed for macros: OPTIONS MPRINT SYMBOLGEN MLOGIC;\n\nMPRINT: Displays the actual SAS statements generated by macro execution, helping you identify issues within macros.\nSYMBOLGEN: Shows the resolution of macro variables, assisting you in confirming that macro variables resolve correctly.\nMLOGIC: Provides detailed information about macro execution, including macro parameter values and logical branching, useful for troubleshooting complex macro logic.\n\n/* Turn on options */\noptions mprint symbolgen mlogic mautosource mcompilenote=ALL;\nBad Tips:\n\nRUNNING CODE WITHOUT VERIFYING INPUT DATA\n\nJust assume your dataset is perfect—because real-world data is always flawless, right? Trusting imported data without verification can lead to incorrect analyses, wasted time, and unreliable results.\nGood Practices:\nAssume all data is “guilty until proven innocent”\n\nInspect dataset properties using PROC CONTENTS, PROC MEANS, and PROC FREQ before analysis.\nValidate key uniqueness, check for missing values, and confirm data quality before merging datasets.\nCheck for numeric-to-character conversions and unexpected results to avoid unintended data type changes and associated analytical errors."
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#data-management-mistakes",
    "href": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#data-management-mistakes",
    "title": "SASプログラミングのPitfalls and Bad Habits",
    "section": "4 DATA MANAGEMENT MISTAKES",
    "text": "4 DATA MANAGEMENT MISTAKES\nEfficient data management is crucial in SAS programming to avoid data loss, facilitate easy retrieval, and ensure accurate analyses. Here are some common mistakes to avoid and best practices to adopt:\nBad Tips:\n\nSTORE ALL YOUR DATA IN WORK\n\nBecause who doesn’t enjoy the adrenaline rush of potentially losing hours of work? Storing all data in the temporary WORK library is risky because data stored there is deleted once your SAS session ends. This practice can lead to significant data loss, especially if you encounter unexpected session closures or interruptions.\nGood Practices:\n\nStore important data in permanent libraries\n\nStore important datasets in permanent libraries to ensure data persistence beyond the current session. Permanent libraries help secure your data, enabling long-term storage, sharing across sessions, and preventing accidental data loss.\nBAD TIP:\n\nAVOID USING LIBRARIES Why make things easy when you can spend extra hours hunting for files? Avoiding the use of libraries can lead to disorganized file management, making it challenging to locate datasets and maintain clean project structures.\n\nGood Practice:\n\nCreate and use SAS libraries Use SAS libraries to streamline data management by logically grouping related datasets. Clearly named and structured libraries improve data accessibility, simplify data sharing, and enhance project collaboration.\n\nBAD TIP:\n\nRUNNING CODE ON PRODUCTION DATABASE WITHOUT TESTING IT FIRST Nothing spices up the workday quite like taking unnecessary risks with live data! Running untested code directly on a production database risks data integrity, can cause significant disruptions, and might lead to costly errors or downtime.\n\nGood Practice:\n\nUse a development or test environment for creating code\nAlways test your code thoroughly in a safe, isolated environment before deploying it to production.Comprehensive testing helps identify potential issues early, ensuring that your code operates reliably and safeguards the production environment."
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#working-with-dates-times-in-sas",
    "href": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#working-with-dates-times-in-sas",
    "title": "SASプログラミングのPitfalls and Bad Habits",
    "section": "5 WORKING WITH DATES ＆ TIMES IN SAS",
    "text": "5 WORKING WITH DATES ＆ TIMES IN SAS\nAccurate handling of dates and times is critical for reliable analyses in SAS. Mistakes in this area can lead to serious analytical errors and confusion. Here are common pitfalls and best practices to adopt:\nUNDERSTANDING SAS DATES\nSAS dates are numeric values representing the number of days since January 1, 1960. This numeric representation simplifies calculations involving dates, such as finding differences between two dates or shifting dates by specific intervals.\nExample:\n\nJanuary 1, 1960, is represented as 0.\nJanuary 2, 1960, is represented as 1.\nDecember 31, 1959, is represented as -1. When printed or displayed, SAS applies date formats to convert these numeric values into readable dates.\n\ndata _null_;\n today_date = today();\n put today_date= date9.;\nrun;\n\ntoday_date=17MAR2025\nGood Practice:\n\nEfficiently handle date values\n\nSAS dates, times, and datetime values are stored as numbers, making them ideal for calculations and comparisons.\nUse the DATEPART(datetime_variable) function to easily extract date values from datetime variables.\nUtilize the INTNX function for precise date shifting, such as adjusting to the first day of the next month.\n\ndata one;\ndtvalue=2064365417;\nStartDate=put(datepart(dtvalue), date9.);\nEndDate=put(intnx('DAY', datepart(dtvalue), 3), date9.);\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#automationreusability",
    "href": "posts/statistics/2025/SASプログラミングにおけるPitfalls.html#automationreusability",
    "title": "SASプログラミングのPitfalls and Bad Habits",
    "section": "6 AUTOMATION＆REUSABILITY",
    "text": "6 AUTOMATION＆REUSABILITY\nAutomation and reuse of code are essential for improving efficiency, accuracy, and maintainability in your SAS workflows. Here are two common pitfalls to avoid and best practices to adopt:\nここは、基本的にマクロを使おうという趣旨。原文を参照。\n## Bad\n\ndata report1;\n set sales;\n where year = 2025;\nrun;\ndata report2;\n set expenses;\n where year = 2025;\nrun;\n## Good\n%let report_year = 2025;\ndata report1;\n set sales;\n where year = &report_year;\nrun;\ndata report2;\n set expenses;\n where year = &report_year;\nrun;\n## Bad\n\nproc means data=dataset1;\n var sales;\nrun;\nproc means data=dataset2;\n var sales;\nrun;\n## Good\n%macro summarize_sales(dataset);\nproc means data=&dataset;\n var sales;\nrun;\n%mend;\n%summarize_sales(dataset1);\n%summarize_sales(dataset2);"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html",
    "href": "posts/statistics/2025/SASによる便利関数1.html",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "CATS関数\nCATX関数\nWHICHN・WHICHC関数\nCHOOSEN・CHOOSEC関数\nCOALESCE・COALESCEC関数\nVVALUE・VVALUEX関数\nCMISS関数\nIFN・IFC関数\nCALL MISSING\nCALL SYMPUTX\n\nSASプログラミングで頻繁に使用される便利な関数を、実用的なプログラム例とともに解説します。\n\n\n機能： 複数の文字列を連結し、各引数の前後の空白を自動削除\ndata example1;     name = \"田中\";     id = \"001\";     dept = \"営業部\";          /* 従来の方法 */     result1 = trim(name) || trim(id) || trim(dept);          /* CATS関数を使用 */     result2 = cats(name, id, dept);          put result1= result2=; run;\n出力： result1=田中001営業部 result2=田中001営業部\nCATS関数は自動的に前後の空白を削除するため、TRIMやLEFT関数が不要になり、コードがシンプルになります。\n\n\n\n機能： 指定した区切り文字で複数の文字列を連結\ndata example2;     year = 2025;     month = 6;     day = 16;          /* 日付文字列の作成 */     date_slash = catx(\"/\", year, month, day);     date_hyphen = catx(\"-\", year, month, day);          /* CSVフォーマットの作成 */     csv_line = catx(\",\", \"田中太郎\", 30, \"東京都\");          put date_slash= date_hyphen= csv_line=; run;\n出力： date_slash=2025/6/16 date_hyphen=2025-6-16 csv_line=田中太郎,30,東京都\n\n\n\n機能： 指定した値がリストの何番目にあるかを返す\ndata ae_severity;     input pt $ severity $;          /* 重篤度レベルをコード化 */     severity_code = whichc(severity, \"軽度\", \"中等度\", \"重度\", \"重篤\");          /* グレード分類への変換 */     ctcae_grade = whichc(severity, \"Grade1\", \"Grade2\", \"Grade3\", \"Grade4\", \"Grade5\");      datalines; 頭痛 軽度 発熱 中等度 呼吸困難 重度 ; run;\npt=頭痛 severity=軽度 severity_code=1 ctcae_grade=0\npt=発熱 severity=中等度 severity_code=2 ctcae_grade=0\npt=呼吸困難 severity=重度 severity_code=3 ctcae_grade=0\n\nseverity_code：指定したリスト内での位置を返す\n\n“軽度” → 1番目 → 1\n“中等度” → 2番目 → 2\n“重度” → 3番目 → 3\n\n\n\n\nctcae_grade：CTCAEグレード用のリストにマッチしないため全て0\n\nデータの”軽度”、“中等度”、“重度”は”Grade1”、“Grade2”等とマッチしない\nマッチしない場合はWHICHC関数は0を返す\n\n\n\n\n\n機能： インデックス番号に基づいてリストから値を選択\ndata example4;     do i = 1 to 4;         /* 数値版：CHOOSEN */         threshold = choosen(i, 60, 70, 80, 90);                  /* 文字版：CHOOSEC */         grade = choosec(i, \"D\", \"C\", \"B\", \"A\");                  put i= threshold= grade=;     end; run;\n出力： i=1 threshold=60 grade=D i=2 threshold=70 grade=C i=3 threshold=80 grade=B i=4 threshold=90 grade=A\n・Y番目のXの値を返す。\n・第2引数以降に数値型の変数または値を指定する場合はCHOOSEN関数を用いる。\n・第2引数以降に文字型の変数または値を指定する場合はCHOOSEC関数を用いる。\n\n\n\n機能： 最初の非欠損値を返す\n/* サンプルデータの作成 */ data sample_data;     input ID X1 $ X2 $ X3 $;     datalines; 1 AA    BB 2    CC DD 3       EE 4 FF     5           6 GG HH II ; run;  /* 方法1: IF-ELSE文を使用 */ data result1;     set sample_data;     length Y $2.;     if X1^=\"\" then Y=X1;     else if X2^=\"\" then Y=X2;     else if X3^=\"\" then Y=X3; run;  /* 方法2: COALESCEC関数を使用 */ data result2;     set sample_data;     length Y $2.;     Y = coalescec(X1, X2, X3); run;\nポイント：\n\nこの関数は「引数のうち最初に欠損値以外で登場する値を返す」という機能を持っています。\nCOALESCE： 数値の場合、欠損値（.）をスキップして最初の有効な値（85）を返す\nCOALESCEC： 文字の場合、空白をスキップして最初の有効な文字列を返す\n\n\n\n\n機能： フォーマットが適用された値を文字列として取得\ndata DT1;   format X yymmdd10.;   X = '13jun2017'd; run;   data DT2;   set DT1;   length Y $20.;   Y = put( X, yymmdd10.); run;   data DT2;   set DT1;   length Y1 Y2 $20.;    /* vvalue関数を使った例 */   Y1 = vvalue( X );    /* vvaluex関数を使った例 */   Y2 = vvaluex( \"X\" );  run;\n\nY1 = vvalue( X )：「 vvalue( X ) 」で変数Xに割り当てられているFORMAT「YYMMDD10.」を使って文字変換した値「2017-06-13」を返しています。\nY2 = vvaluex( “X” );vvaluex も vvalue と同じ機能を持っているのですが、違いは以下の通り。\nvvalue( X )      … 変数名を指定\nvvaluex( “X” )  … 変数名を表す文字値を指定\nつまり、「 vvaluex( “X” ) 」で変数Xに割り当てられているFORMAT「YYMMDD10.」を使って文字変換した値「2017-06-13」を返しています。\n\n\n\n\n機能： 欠損値の個数をカウント\ndata example7;     input name $ age height weight;          missing_count = cmiss(age, height, weight);     complete_data = (missing_count = 0);          put name= missing_count= complete_data=;      datalines; 田中 25 170 65 佐藤 . 165 . 山田 30 . 70 ; run;\n出力： name=田中 missing_count=0 complete_data=1 name=佐藤 missing_count=2 complete_data=0 name=山田 missing_count=1 complete_data=0\n\n\n\n機能： 条件に基づいて値を返す三項演算子\ndata DT1;    length X1 $10.;    X1=\"YES\"; output;    X1=\"NO\"; output; run;  #Before data DT2;    set DT1;    if X1 = \"YES\" then X2=1;    else  X2=0; run;  #After data DT2;    set DT1;    X2 = ifn(X1=\"YES\",1,0); run;\nRのifelse関数みたいな気持ち。\n\n\n\n機能： 複数の変数を一度に欠損値に設定\ndata example9;     name = \"田中\";     age = 25;     score = 85;          /* 条件に応じて全データを欠損値に */     if age &lt; 20 then call missing(of name age score);          put name= age= score=; run;\nこのSASコードはCALL MISSINGルーチンを使って、条件に応じて複数の変数を一度に欠損値に設定する例です。\n\n\n\n機能： データステップ内でマクロ変数を作成・更新\ndata example10;     input dept $ sales;          /* 部門別に動的にマクロ変数を作成 */     call symputx(cats(\"sales_\", dept), sales);          /* 最大売上をマクロ変数に格納 */     retain max_sales;     if _n_ = 1 then max_sales = sales;     else max_sales = max(max_sales, sales);     if  _EOF then call symputx(\"OBS\", _N_);      datalines; 営業 1200 技術 800 総務 300 ; run;  /* データステップ終了後に最大値を取得 */ data _null_;     set example10 end=last;     retain max_sales;     if _n_ = 1 then max_sales = sales;     else max_sales = max(max_sales, sales);     if last then call symputx(\"max_sales_total\", max_sales); run;  %put &sales_営業 &sales_技術 &sales_総務; %put &max_sales_total;\n出力： 1200 800 300 1200\nこれらの関数を使いこなすことで、SASプログラミングの効率と可読性が大幅に向上します。特にデータクリーニングや条件分岐処理において威力を発揮する関数群です。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html#cats関数",
    "href": "posts/statistics/2025/SASによる便利関数1.html#cats関数",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "機能： 複数の文字列を連結し、各引数の前後の空白を自動削除\ndata example1;     name = \"田中\";     id = \"001\";     dept = \"営業部\";          /* 従来の方法 */     result1 = trim(name) || trim(id) || trim(dept);          /* CATS関数を使用 */     result2 = cats(name, id, dept);          put result1= result2=; run;\n出力： result1=田中001営業部 result2=田中001営業部\nCATS関数は自動的に前後の空白を削除するため、TRIMやLEFT関数が不要になり、コードがシンプルになります。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html#catx関数",
    "href": "posts/statistics/2025/SASによる便利関数1.html#catx関数",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "機能： 指定した区切り文字で複数の文字列を連結\ndata example2;     year = 2025;     month = 6;     day = 16;          /* 日付文字列の作成 */     date_slash = catx(\"/\", year, month, day);     date_hyphen = catx(\"-\", year, month, day);          /* CSVフォーマットの作成 */     csv_line = catx(\",\", \"田中太郎\", 30, \"東京都\");          put date_slash= date_hyphen= csv_line=; run;\n出力： date_slash=2025/6/16 date_hyphen=2025-6-16 csv_line=田中太郎,30,東京都"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html#whichnwhichc関数",
    "href": "posts/statistics/2025/SASによる便利関数1.html#whichnwhichc関数",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "機能： 指定した値がリストの何番目にあるかを返す\ndata ae_severity;     input pt $ severity $;          /* 重篤度レベルをコード化 */     severity_code = whichc(severity, \"軽度\", \"中等度\", \"重度\", \"重篤\");          /* グレード分類への変換 */     ctcae_grade = whichc(severity, \"Grade1\", \"Grade2\", \"Grade3\", \"Grade4\", \"Grade5\");      datalines; 頭痛 軽度 発熱 中等度 呼吸困難 重度 ; run;\npt=頭痛 severity=軽度 severity_code=1 ctcae_grade=0\npt=発熱 severity=中等度 severity_code=2 ctcae_grade=0\npt=呼吸困難 severity=重度 severity_code=3 ctcae_grade=0\n\nseverity_code：指定したリスト内での位置を返す\n\n“軽度” → 1番目 → 1\n“中等度” → 2番目 → 2\n“重度” → 3番目 → 3\n\n\n\n\nctcae_grade：CTCAEグレード用のリストにマッチしないため全て0\n\nデータの”軽度”、“中等度”、“重度”は”Grade1”、“Grade2”等とマッチしない\nマッチしない場合はWHICHC関数は0を返す"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html#choosenchoosec関数",
    "href": "posts/statistics/2025/SASによる便利関数1.html#choosenchoosec関数",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "機能： インデックス番号に基づいてリストから値を選択\ndata example4;     do i = 1 to 4;         /* 数値版：CHOOSEN */         threshold = choosen(i, 60, 70, 80, 90);                  /* 文字版：CHOOSEC */         grade = choosec(i, \"D\", \"C\", \"B\", \"A\");                  put i= threshold= grade=;     end; run;\n出力： i=1 threshold=60 grade=D i=2 threshold=70 grade=C i=3 threshold=80 grade=B i=4 threshold=90 grade=A\n・Y番目のXの値を返す。\n・第2引数以降に数値型の変数または値を指定する場合はCHOOSEN関数を用いる。\n・第2引数以降に文字型の変数または値を指定する場合はCHOOSEC関数を用いる。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html#coalescecoalescec関数",
    "href": "posts/statistics/2025/SASによる便利関数1.html#coalescecoalescec関数",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "機能： 最初の非欠損値を返す\n/* サンプルデータの作成 */ data sample_data;     input ID X1 $ X2 $ X3 $;     datalines; 1 AA    BB 2    CC DD 3       EE 4 FF     5           6 GG HH II ; run;  /* 方法1: IF-ELSE文を使用 */ data result1;     set sample_data;     length Y $2.;     if X1^=\"\" then Y=X1;     else if X2^=\"\" then Y=X2;     else if X3^=\"\" then Y=X3; run;  /* 方法2: COALESCEC関数を使用 */ data result2;     set sample_data;     length Y $2.;     Y = coalescec(X1, X2, X3); run;\nポイント：\n\nこの関数は「引数のうち最初に欠損値以外で登場する値を返す」という機能を持っています。\nCOALESCE： 数値の場合、欠損値（.）をスキップして最初の有効な値（85）を返す\nCOALESCEC： 文字の場合、空白をスキップして最初の有効な文字列を返す"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html#vvaluevvaluex関数",
    "href": "posts/statistics/2025/SASによる便利関数1.html#vvaluevvaluex関数",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "機能： フォーマットが適用された値を文字列として取得\ndata DT1;   format X yymmdd10.;   X = '13jun2017'd; run;   data DT2;   set DT1;   length Y $20.;   Y = put( X, yymmdd10.); run;   data DT2;   set DT1;   length Y1 Y2 $20.;    /* vvalue関数を使った例 */   Y1 = vvalue( X );    /* vvaluex関数を使った例 */   Y2 = vvaluex( \"X\" );  run;\n\nY1 = vvalue( X )：「 vvalue( X ) 」で変数Xに割り当てられているFORMAT「YYMMDD10.」を使って文字変換した値「2017-06-13」を返しています。\nY2 = vvaluex( “X” );vvaluex も vvalue と同じ機能を持っているのですが、違いは以下の通り。\nvvalue( X )      … 変数名を指定\nvvaluex( “X” )  … 変数名を表す文字値を指定\nつまり、「 vvaluex( “X” ) 」で変数Xに割り当てられているFORMAT「YYMMDD10.」を使って文字変換した値「2017-06-13」を返しています。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html#cmiss関数",
    "href": "posts/statistics/2025/SASによる便利関数1.html#cmiss関数",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "機能： 欠損値の個数をカウント\ndata example7;     input name $ age height weight;          missing_count = cmiss(age, height, weight);     complete_data = (missing_count = 0);          put name= missing_count= complete_data=;      datalines; 田中 25 170 65 佐藤 . 165 . 山田 30 . 70 ; run;\n出力： name=田中 missing_count=0 complete_data=1 name=佐藤 missing_count=2 complete_data=0 name=山田 missing_count=1 complete_data=0"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html#ifnifc関数",
    "href": "posts/statistics/2025/SASによる便利関数1.html#ifnifc関数",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "機能： 条件に基づいて値を返す三項演算子\ndata DT1;    length X1 $10.;    X1=\"YES\"; output;    X1=\"NO\"; output; run;  #Before data DT2;    set DT1;    if X1 = \"YES\" then X2=1;    else  X2=0; run;  #After data DT2;    set DT1;    X2 = ifn(X1=\"YES\",1,0); run;\nRのifelse関数みたいな気持ち。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html#call-missing",
    "href": "posts/statistics/2025/SASによる便利関数1.html#call-missing",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "機能： 複数の変数を一度に欠損値に設定\ndata example9;     name = \"田中\";     age = 25;     score = 85;          /* 条件に応じて全データを欠損値に */     if age &lt; 20 then call missing(of name age score);          put name= age= score=; run;\nこのSASコードはCALL MISSINGルーチンを使って、条件に応じて複数の変数を一度に欠損値に設定する例です。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる便利関数1.html#call-symputx",
    "href": "posts/statistics/2025/SASによる便利関数1.html#call-symputx",
    "title": "SASによる便利関数1",
    "section": "",
    "text": "機能： データステップ内でマクロ変数を作成・更新\ndata example10;     input dept $ sales;          /* 部門別に動的にマクロ変数を作成 */     call symputx(cats(\"sales_\", dept), sales);          /* 最大売上をマクロ変数に格納 */     retain max_sales;     if _n_ = 1 then max_sales = sales;     else max_sales = max(max_sales, sales);     if  _EOF then call symputx(\"OBS\", _N_);      datalines; 営業 1200 技術 800 総務 300 ; run;  /* データステップ終了後に最大値を取得 */ data _null_;     set example10 end=last;     retain max_sales;     if _n_ = 1 then max_sales = sales;     else max_sales = max(max_sales, sales);     if last then call symputx(\"max_sales_total\", max_sales); run;  %put &sales_営業 &sales_技術 &sales_総務; %put &max_sales_total;\n出力： 1200 800 300 1200\nこれらの関数を使いこなすことで、SASプログラミングの効率と可読性が大幅に向上します。特にデータクリーニングや条件分岐処理において威力を発揮する関数群です。"
  },
  {
    "objectID": "posts/statistics/2025/SAS_Proc Transpose_arrayによるデータ転置.html",
    "href": "posts/statistics/2025/SAS_Proc Transpose_arrayによるデータ転置.html",
    "title": "Proc Transpose/ARRAYによる転置",
    "section": "",
    "text": "0.1 SAS初心者必見！横持ちデータを縦持ちに変換する3つの方法【ADaM対応・完全コード付き】\nこんにちは！SASプログラミングを学び始めたばかりの皆さん、データ形式の「横持ち」「縦持ち」で困ったことはありませんか？\n臨床試験データの標準モデルであるADaMでは、基本的に「1行に1つの分析結果」を格納する縦持ち形式が求められます。しかし、元のデータは被験者IDごとに検査結果が横に並んだ横持ち形式であることがよくあります。\n今回は、この横持ちデータを縦持ちに変換するための代表的な3つの方法を、ADaM変数（PARAM, AVAL, AVISITNなど）を使いながら、初心者向けにじっくり解説していきます。すべてのコードはコピー＆ペーストで実行可能です。\n\n0.1.1 準備：今回のサンプルデータとゴール\nまず、変換元となる横持ちデータ（source_data）を見てみましょう。被験者ごと、訪問ごと（VISIT 1, 2, 3）の検査値（TC: 総コレステロール, HDL: HDLコレステロール）が横に並んでいます。\n【変換元】横持ちデータ\nUSUBJID | TC_1 | HDL_1 | TC_2 | HDL_2 | TC_3 | HDL_3\n-------------------------------------------------------\nP01     | 212  | 50    | 224  | 64    | 204  | 73\nP02     | 206  | 58    | 208  | 63    | 212  | 58\nP03     | 221  | 47    | 236  | 70    | 242  | 38\nそして、このデータを以下の縦持ち形式に変換するのが今回のゴールです。\n【ゴール】縦持ちデータ (ADaM風)\nUSUBJID | PARAMN | PARAM     | AVISITN | AVAL\n-------------------------------------------------\nP01     | 1      | TC        | 1       | 212\nP01     | 1      | TC        | 2       | 224\nP01     | 1      | TC        | 3       | 204\nP01     | 2      | HDL       | 1       | 50\n...     | ...    | ...       | ...     | ...\nそれでは、具体的な変換方法を見ていきましょう！\n\n\n\n0.2 方法1：PROC TRANSPOSE - SASの変形マジック\nPROC TRANSPOSEは、その名の通りデータセットの行と列を入れ替える（転置する）ための強力なプロシジャです。\n\n0.2.1 ステップ1：まず転置してみる\nproc transpose data=source_data out=transposed_data(rename=(COL1=AVAL _NAME_=VARNAME));\n  by USUBJID;\n  var TC_1--TC_3 HDL_1--HDL_3;\nrun;\n\nproc transpose ...; run;: PROC TRANSPOSEの開始と終了を示します。\ndata=source_data: 変換の対象データセットを指定します。\nout=transposed_data(...): 変換後のデータセット名と、オプションを指定します。\nrename=(...): 自動生成される変数名を、より分かりやすい名前に変更します。\nby USUBJID;: 非常に重要なステートメントです。「どの変数を基準にグループ化するか」を指定します。今回はUSUBJIDごとに行をまとめたいので、USUBJIDを指定します。\nvar ...;: どの変数を縦持ちに変換（転置）するかを指定します。\n\n\n\n0.2.2 ステップ2：データステップでADaM変数に整える\n次に、この中間データをデータステップで加工し、VARNAMEからPARAMとAVISITNを作り出します。\ndata final_data_transpose;\n  set transposed_data;\n  \n  PARAM   = scan(VARNAME, 1, '_');\n  AVISITN = input(scan(VARNAME, 2, '_'), 8.);\n  \n  if PARAM = 'TC' then PARAMN = 1;\n  else if PARAM = 'HDL' then PARAMN = 2;\n  \n  drop VARNAME;\nrun;\nscan関数は、指定した区切り文字（今回は_）で文字列を分割してくれる便利な関数です。これでPROC TRANSPOSEを使った変換が完成です！\n\n\n\n0.3 方法2：ARRAYステートメント - 最もスマートな方法\nARRAYは**「複数の変数を一時的にグループ化して、番号で扱えるようにする」**機能です。 これを使いこなすと、非常に効率的なプログラミングが可能になります。\ndata final_data_array;\n  set source_data;\n  \n  /*【重要ポイント】\n   * '--'記法は変数の物理的順序に依存しエラーの原因になりうるため、\n   * ここでは変数をすべて明示的にリストアップします。\n   */\n  array aval_group{2, 3} TC_1 TC_2 TC_3 HDL_1 HDL_2 HDL_3; \n  \n  array params{2} $ _temporary_ ('TC', 'HDL');\n  array paramns{2} _temporary_ (1, 2);\n  \n  do j = 1 to 2; [cite: 434]\n    do i = 1 to 3; [cite: 435]\n      PARAMN  = paramns{j};\n      PARAM   = params{j};\n      AVISITN = i;\n      AVAL    = aval_group{j, i}; [cite: 436]\n      output; [cite: 437]\n    end;\n  end;\n  \n  keep USUBJID PARAMN PARAM AVISITN AVAL;\nrun;\n\narray aval_group{2, 3} ...;: 2行3列の二次元配列を定義します。 SASはリストされた変数を行優先で配列に格納します。\n_temporary_: このオプションで定義した配列は、データセットに出力されない一時的な作業領域として使えます。\ndo j = 1 to 2; do i = 1 to 3;: 外側のループで検査項目を、内側のループで訪問を回しています。\n\nこの方法は、一つのデータステップで完結するため、非常に効率的でコードもスッキリします。\n\n\n0.4 方法3：マクロ - 同じ作業はSASに任せよう\n最後は、地道なDATAステップの繰り返しを自動化するマクロを使った方法です。\n%macro reshape(param_nm, param_num);\n  %do visit = 1 %to 3;\n    data temp_&param_nm._&visit.;\n      set source_data;\n      PARAMN  = &param_num.;\n      PARAM   = \"&param_nm.\";\n      AVISITN = &visit.;\n      AVAL    = &param_nm._&visit;\n      keep USUBJID PARAMN PARAM AVISITN AVAL;\n    run;\n  %end;\n%mend reshape;\n\n%reshape(TC, 1);\n%reshape(HDL, 2);\n\ndata final_data_macro;\n  /*【重要ポイント】\n   * '--'記法はデータセットの物理的順序に依存しエラーの原因になるため、\n   * ここでは結合したいデータセット名をすべて明示的にリストアップします。\n   */\n  set temp_TC_1 temp_TC_2 temp_TC_3\n      temp_HDL_1 temp_HDL_2 temp_HDL_3;\nrun;\n\n%macro ... %mend;: このブロックで一連の処理をマクロとして定義します。\n%do ... %end;: マクロ内で繰り返し処理を行います。\n&param_nm.や&visit.はマクロ変数と呼ばれ、マクロ呼び出し時に指定された値に置き換わります。\n\nこの方法は、一つ一つの処理は単純ですが、中間データセットが複数作られるのが特徴です。\n\n\n0.5 まとめとAppendix\n3つの方法を紹介しましたが、いかがでしたか？\n\n\n\n\n\n\n\n\n\nアプローチ\nメリット\nデメリット\nこんな人におすすめ\n\n\nPROC TRANSPOSE\n大量の変数を一度に転置できる\n複数ステップが必要になることがある\nまずはSASのプロシジャに慣れたい人\n\n\nARRAY\nコードが簡潔で最も効率的\n配列の概念（特に二次元）に慣れが必要\nSASでのデータ加工を極めたい人\n\n\nマクロ\n繰り返し処理を自動化できる\n中間データセットが多くなりがち\n特定の定型処理を何度も再利用したい人\n\n\n\n初学者の皆さんは、まずはPROC TRANSPOSEから試してみて、慣れてきたらぜひARRAYステートメントに挑戦してみてください。\n\n0.5.1 Appendix: 全手法の完全なサンプルプログラム\n以下に、本稿で紹介した3つのアプローチについて、横持ちの疑似データ作成から縦持ちデータへ変換するまでの一連のSASプログラムを掲載します。\n/* ============================================== */\n/* 1. 変換元となる横持ちデータを作成              */\n/* ============================================== */\ndata source_data;\n  input USUBJID $ TC_1 HDL_1 TC_2 HDL_2 TC_3 HDL_3;\n  cards;\nP01 212 50 224 64 204 73\nP02 206 58 208 63 212 58\nP03 221 47 236 70 242 38\n;\nrun;\n\ntitle \"変換前の横持ちデータ (source_data)\";\nproc print data=source_data;\nrun;\ntitle;\n\n/* ============================================== */\n/* 手法1: PROC TRANSPOSE を使用したプログラム       */\n/* ============================================== */\nproc transpose data=source_data out=transposed_data(rename=(COL1=AVAL _NAME_=VARNAME));\n  by USUBJID;\n  var TC_1--TC_3 HDL_1--HDL_3;\nrun;\n\ndata final_data_transpose;\n  set transposed_data;\n  PARAM   = scan(VARNAME, 1, '_');\n  AVISITN = input(scan(VARNAME, 2, '_'), 8.);\n  if PARAM = 'TC' then PARAMN = 1;\n  else if PARAM = 'HDL' then PARAMN = 2;\n  drop VARNAME;\nrun;\n\nproc sort data=final_data_transpose;\n  by USUBJID PARAMN AVISITN;\nrun;\n\ntitle \"手法1 (PROC TRANSPOSE) による変換結果\";\nproc print data=final_data_transpose;\nrun;\ntitle;\n\n/* ============================================== */\n/* 手法2: ARRAY を使用したプログラム              */\n/* ============================================== */\ndata final_data_array;\n  set source_data;\n  \n  array aval_group{2, 3} TC_1 TC_2 TC_3 HDL_1 HDL_2 HDL_3;\n  \n  array params{2} $ _temporary_ ('TC', 'HDL');\n  array paramns{2} _temporary_ (1, 2);\n  \n  do j = 1 to 2;\n    do i = 1 to 3;\n      PARAMN  = paramns{j};\n      PARAM   = params{j};\n      AVISITN = i;\n      AVAL    = aval_group{j, i};\n      output;\n    end;\n  end;\n  \n  keep USUBJID PARAMN PARAM AVISITN AVAL;\nrun;\n\nproc sort data=final_data_array;\n  by USUBJID PARAMN AVISITN;\nrun;\n\ntitle \"手法2 (ARRAY) による変換結果\";\nproc print data=final_data_array;\nrun;\ntitle;\n\n/* ============================================== */\n/* 手法3: マクロを使用したプログラム              */\n/* ============================================== */\n%macro reshape(param_nm, param_num);\n  %do visit = 1 %to 3;\n    data temp_&param_nm._&visit.;\n      set source_data;\n      PARAMN  = &param_num.;\n      PARAM   = \"&param_nm.\";\n      AVISITN = &visit.;\n      AVAL    = &param_nm._&visit;\n      \n      keep USUBJID PARAMN PARAM AVISITN AVAL;\n    run;\n  %end;\n%mend reshape;\n\n%reshape(TC, 1);\n%reshape(HDL, 2);\n\ndata final_data_macro;\n  set temp_TC_1 temp_TC_2 temp_TC_3\n      temp_HDL_1 temp_HDL_2 temp_HDL_3;\nrun;\n\nproc sort data=final_data_macro;\n  by USUBJID PARAMN AVISITN;\nrun;\n\ntitle \"手法3 (マクロ) による変換結果\";\nproc print data=final_data_macro;\nrun;\ntitle;\n\nproc datasets lib=work nolist;\n  delete temp_:;\nquit;"
  },
  {
    "objectID": "posts/statistics/2025/RAWデータ加工_Rename.html",
    "href": "posts/statistics/2025/RAWデータ加工_Rename.html",
    "title": "データセット作成のTips",
    "section": "",
    "text": "SASでデータ処理をしていると、時には「あれ？これってどういう意図があるんだろう？」と首をかしげるコードに出会うことがありますよね。今回は、そんな疑問を解消する、ちょっと高度だけど非常に便利なデータ変換テクニックをご紹介します！\n例として、次のようなSASコードを見てみましょう。\n*-----------------------------------------------------------------------------*;\n* DATA PROCESS ;\n*-----------------------------------------------------------------------------*;\n/* オリジナルデータセットから新しいデータセットを作成 */\ndata work.ProcessedData ;\n  /* 変数の長さと型を定義 */\n  length PatientID $15 VisitDate $10 Gender $1 GenderNum 8 AgeYears 8 UnitAge $5 ;\n  /* 元データセットを読み込み、特定変数をリネームして取り込む */\n  set work.RawData ( rename = (Gender = _Gender Age = _Age ) ) ;\n\n  /* 新しい変数の値を設定 */\n  PatientID = cats(\"PAT-\", _N_); /* 患者IDを自動生成 */\n  VisitDate = \"2024-06-15\" ; /* 仮の訪問日を設定 */\n\n  /* 年齢データの変換 */\n  AgeYears = input( _Age, best. ) ; /* _Age（元の年齢データ）を数値に変換 */\n  UnitAge = \"YEARS\" ; /* 年齢の単位を設定 */\n\n  /* 性別データの変換と数値化 */\n  if _Gender = \"1\" then Gender = \"M\" ; /* _Genderが\"1\"なら\"M\"（男性）に */\n  else if _Gender = \"2\" then Gender = \"F\" ; /* _Genderが\"2\"なら\"F\"（女性）に */\n  GenderNum = input( _Gender, best.) ; /* _Genderを数値としてGenderNumに格納 */\n\n  /* 新しいデータセットに残す変数を指定 */\n  keep PatientID VisitDate Gender GenderNum AgeYears UnitAge ;\n\nproc sort data=work.ProcessedData ;\n  by PatientID ; /* 患者IDでソート */\nrun ;\n\n---\n\ndata work.DAT9 ;\n  merge work.DAT9_1 work.DAT9_2 work.DAT9_3 work.DAT9_4 ;\n  by SUBJID ;\n\n  /* 日付・時刻データの変換例 */\n  /* 元データ: \"2023-01-01 10\" のような形式 */\n  FPAYDT  = input( substr(FPAYDTC, 1, 10), yymmdd10. ); /* 日付部分のみを抽出してSAS日付値に変換 */\n  PAYDT1  = input( substr(PAYDTC1, 1, 10), yymmdd10. ); /* 同上 */\n  PAYDT2  = input( substr(PAYDTC2, 1, 10), yymmdd10. ); /* 同上 */\n  \n  /* 完全な日時文字列を作成（例: \"2023-01-01 10:00\"）*/\n  _FPAYDTC = strip( FPAYDTC ) || \":00\" ; \n  _PAYDTC1 = strip( PAYDTC1 ) || \":00\" ;\n  _PAYDTC2 = strip( PAYDTC2 ) || \":00\" ;\n\n  /* 支払い期間に応じた時間差の計算 */\n  if PAYPD = '第1期' then do ;\n    ELATIME = round( ( input( _FPAYDTC, e8601dt19. ) - input( _PAYDTC1, e8601dt19. ) ) / ( 60*60 ) , 1e-10 ); /* 時間単位の差分を計算 */\n  end ;\n  if PAYPD = '第2期' then do ;  \n    ELATIME = round( ( input( _FPAYDTC, e8601dt19. ) - input( _PAYDTC2, e8601dt19. ) ) / ( 60*60 ), 1e-10 ) ; /* 時間単位の差分を計算 */\n  end ;\n\nrun ;\n\n---\n\ndata work.SCRCM ;\n  length SUBJID $12 SCRCMFL $1 ;\n  merge work.CM work.SCR;\n  by SBJID ;\n  SUBJID = SBJID ; /* 変数名を合わせる */\n\n  /* 日付形式の変換例 */\n  /* 元データ: \"YYYY/MM/DD\" -&gt; SASが扱える \"YYYY-MM-DD\" に変換 */\n  if CMSTDTC ^= \"\" then _CMSTDTC = tranwrd ( CMSTDTC , \"/\", \"-\" ) ;\n  if CMENDTC ^= \"\" then _CMENDTC = tranwrd ( CMENDTC , \"/\", \"-\" ) ;\n\n  /* 期間内に該当するかフラグを立てる */\n  if ( CMSTDTC ^= \"\" and CMENDTC ^= \"\" ) and input ( _CMSTDTC, yymmdd10. ) &lt;= SCRDT &lt;= input ( _CMENDTC, yymmdd10. ) then SCRCMFL = \"Y\" ;\n  /* 開始日または終了日と完全に一致する場合も対象 */\n  if SCRDT ^= . and ( input ( _CMSTDTC, yymmdd10. ) = SCRDT or SCRDT = input ( _CMENDTC, yymmdd10. )) then SCRCMFL = \"Y\" ;\n  /* 上記条件に当てはまらない場合は\"N\" */\n  if SCRCMFL = \"\" then SCRCMFL = \"N\" ;\n  /* 開始日または終了日が欠損している場合はフラグをクリア */\n  if ( CMSTDTC = \"\" or CMENDTC = \"\" ) and SCRCMFL ^= \"Y\" then SCRCMFL = \"\" ;\n  /* 開始日がSCRDTより未来の場合は\"N\" */\n  if CMSTDTC ^= \"\" and input ( _CMSTDTC, yymmdd10.) &gt; SCRDT then SCRCMFL = \"N\" ;\n\n  keep SUBJID SCRCMFL ;\nproc sort ;\n  by SUBJID decending SCRCMFL ;\nrun ;\nこのコード、特に注目してほしいのは、setステートメントの**renameオプションと、その後に続くGender = _Gender;のような処理、そして日付・時刻を扱う際に登場する様々な関数**です。「せっかくrenameで名前を変えたのに、なんでまた元の名前の変数に代入し直すの？」と感じるかもしれませんね。実はここに、データ処理のベストプラクティスが隠されているんです！\n\n\nまず、set work.RawData ( rename = (Gender = _Gender Age = _Age ) )の部分。 これは、元のデータセットwork.RawDataから変数を読み込む際に、Genderという変数名を一時的に_Genderに、Ageを_Ageにそれぞれ変更して、新しいデータセットwork.ProcessedDataに持ち込む、という指示です。\nなぜこんなことをするのでしょうか？\n\n\nこれが最も大きな理由です。多くの場合、元データ（ここではwork.RawData）のGender変数は「1（男性）」や「2（女性）」のような数値コードで格納されていることがあります。しかし、最終的に使いたいのは「M（Male）」や「F（Female）」のような、より直感的で標準化された文字データだったりします。\nこのコードでは、まさにその変換を行っています。\nif _Gender = \"1\" then Gender = \"M\" ;\n  else if _Gender = \"2\" then Gender = \"F\" ;\nここで、一時的に_Genderという名前で保持しておいた元の数値コードを参照し、新しいGender変数に変換後の「M」や「F」を代入しています。_Ageについても同様に、文字列として読み込まれた年齢データをinput関数を使って数値型のAgeYearsに変換しています。もしrenameで直接GenderやAgeとして読み込んでしまうと、このような元の値を使った条件分岐やデータ型変換が難しくなってしまいますよね。\n\n\n\n\nここで、renameオプションの働きをもう少し具体的に見てみましょう。\n元のデータセット work.RawData のイメージ:\n\n\n\nPatientID\nGender\nAge\nDiagnosis\n\n\nP001\n1\n35\n一般健診\n\n\nP002\n2\n48\n頭痛\n\n\nP003\n1\n22\n発熱\n\n\nP004\n2\n60\n高血圧\n\n\n\nこのwork.RawDataデータセットを、setステートメントで読み込む際に、次のように指定しています。\nset work.RawData ( rename = (Gender = _Gender Age = _Age ) ) ;\nこの一行が実行されると、work.ProcessedDataデータステップの内部では、Genderは_Genderとして、Ageは_Ageとして扱われます。つまり、データステップのその瞬間、変数は以下のように見えている、とイメージしてください。\ndata work.ProcessedData ステップ内部での変数の見え方（一時的）:\n\n\n\nPatientID\n_Gender\n_Age\nDiagnosis\n\n\nP001\n1\n35\n一般健診\n\n\nP002\n2\n48\n頭痛\n\n\nP003\n1\n22\n発熱\n\n\nP004\n2\n60\n高血圧\n\n\n\nこのように元の変数名を一時的に変更しておくことで、その後の処理で新しいGenderやAgeYearsといった変数を心置きなく作成できるわけです。\n\n\n\n提供されたコードには、日付や時刻の文字列をSASが認識できる数値（SAS日付値やSAS日時値）に変換するための様々な関数が使われています。これらは非常に頻繁に利用されるので、ぜひ使い方を覚えておきましょう。\n\n\nINPUT関数は、文字列をSASが扱える数値に変換するための万能選手です。特に日付や時刻の文字列を扱う際には、その文字列がどのような形式（フォーマット）であるかを指定する必要があります。\n\nYYYYMMDD10.: YYYY-MM-DD形式の文字列（例: “2023-01-01”）をSAS日付値に変換します。\n\n FPAYDT = input( substr(FPAYDTC, 1, 10), yymmdd10. );\n /* FPAYDTCが \"2023-01-01 10\" の場合、\"2023-01-01\" の部分だけを抽出して変換 */\n ```\n\n-   **`E8601DT19.`**: `YYYY-MM-DDTHH:MM:SS`または`YYYY-MM-DD HH:MM:SS`形式のISO 8601日時文字列（例: \"2023-01-01 10:00:00\"）をSAS日時値に変換します。SAS日時値は、1960年1月1日0時0分0秒からの秒数を表す数値です。\n\n``` {.sas eval=\"FALSE,\" code-line-numbers=\"true,\" code-overflow=\"wrap\"}     \n ELATIME = round( ( input( _FPAYDTC, e8601dt19. ) - input( _PAYDTC1, e8601dt19. ) ) / ( 60*60 ) , 1e-10 );\n /* _FPAYDTCと_PAYDTC1（いずれも\"YYYY-MM-DD HH:MM\"形式）を日時値に変換し、その差を時間単位で計算 */ \n ```\n\n#### 2. `SUBSTR`関数\n\n`SUBSTR`関数は、\\*\\*文字列の一部を抜き出す（部分文字列を抽出する）\\*\\*ために使われます。\n\n``` {.sas eval=\"FALSE,\" code-line-numbers=\"true,\" code-overflow=\"wrap\"}        \nFPAYDT = input( substr(FPAYDTC, 1, 10), yymmdd10. );\n/* FPAYDTC（例: \"2023-01-01 10\"）の1文字目から10文字目（つまり\"2023-01-01\"）を抽出 */\nこの例では、日付と時間の情報が混ざった文字列から、日付の部分だけを取り出してINPUT関数に渡しています。\n\n\n\nSTRIP関数は、文字列の先頭や末尾にある余分な半角スペースを取り除くために使われます。特にファイルから読み込んだデータは、固定長形式などで不要なスペースが含まれていることがあるため、後の処理で問題を起こさないようにSTRIPで整形することが推奨されます。\n_FPAYDTC = strip( FPAYDTC ) || \":00\" ;\n/* FPAYDTCの末尾のスペースを取り除き、\"2023-01-01 10\" を \"2023-01-01 10:00\" のように変換 */\nこの例では、STRIPを使ってFPAYDTCから不要なスペースを削除し、それに:00を結合して、E8601DT19.フォーマットで読み取れる「時:分」の形式に整えています。\n\n\n\nTRANWRD関数は、文字列中の特定の文字列を別の文字列に置き換えるために使われます。\n_CMSTDTC = tranwrd ( CMSTDTC , \"/\", \"-\" ) ;\n/* CMSTDTC（例: \"2023/01/01\"）の \"/\" を \"-\" に置き換え、\"2023-01-01\" に変換 */\nこれは、元のデータの日付区切りがスラッシュ（/）なのに、SASの標準的な日付フォーマット（YYYYMMDD10.など）がハイフン（-）を要求する場合によく使われます。INPUT関数で正しく変換できるように、事前にTRANWRDで文字列を整形しているわけですね。\n\n\n\n\nこのように、setステートメントでのrenameオプション、そしてINPUT, SUBSTR, STRIP, TRANWRDといった関数を組み合わせることで、SASはどんな形式のデータでも柔軟に処理し、分析に適した形にクリーンアップできます。\n\n元の値を一時的に保持しつつ、\nデータ型や値を変換・整形し、\n新しい標準化された変数を作成する\n\nこの一連の流れを理解することで、より柔軟で堅牢なSASプログラムを書けるようになります。ぜひ皆さんのデータ処理にもこれらのテクニックを取り入れてみてくださいね！\nデータ処理は奥が深いですが、一つ一つのテクニックを理解していくと、どんどん楽しくなりますよ！何か他にSASに関する疑問があれば、いつでも聞いてくださいね！"
  },
  {
    "objectID": "posts/statistics/2025/RAWデータ加工_Rename.html#sasデータ処理の舞台裏renameと変数変換の賢いテクニック",
    "href": "posts/statistics/2025/RAWデータ加工_Rename.html#sasデータ処理の舞台裏renameと変数変換の賢いテクニック",
    "title": "データセット作成のTips",
    "section": "",
    "text": "SASでデータ処理をしていると、時には「あれ？これってどういう意図があるんだろう？」と首をかしげるコードに出会うことがありますよね。今回は、そんな疑問を解消する、ちょっと高度だけど非常に便利なデータ変換テクニックをご紹介します！\n例として、次のようなSASコードを見てみましょう。\n*-----------------------------------------------------------------------------*;\n* DATA PROCESS ;\n*-----------------------------------------------------------------------------*;\n/* オリジナルデータセットから新しいデータセットを作成 */\ndata work.ProcessedData ;\n  /* 変数の長さと型を定義 */\n  length PatientID $15 VisitDate $10 Gender $1 GenderNum 8 AgeYears 8 UnitAge $5 ;\n  /* 元データセットを読み込み、特定変数をリネームして取り込む */\n  set work.RawData ( rename = (Gender = _Gender Age = _Age ) ) ;\n\n  /* 新しい変数の値を設定 */\n  PatientID = cats(\"PAT-\", _N_); /* 患者IDを自動生成 */\n  VisitDate = \"2024-06-15\" ; /* 仮の訪問日を設定 */\n\n  /* 年齢データの変換 */\n  AgeYears = input( _Age, best. ) ; /* _Age（元の年齢データ）を数値に変換 */\n  UnitAge = \"YEARS\" ; /* 年齢の単位を設定 */\n\n  /* 性別データの変換と数値化 */\n  if _Gender = \"1\" then Gender = \"M\" ; /* _Genderが\"1\"なら\"M\"（男性）に */\n  else if _Gender = \"2\" then Gender = \"F\" ; /* _Genderが\"2\"なら\"F\"（女性）に */\n  GenderNum = input( _Gender, best.) ; /* _Genderを数値としてGenderNumに格納 */\n\n  /* 新しいデータセットに残す変数を指定 */\n  keep PatientID VisitDate Gender GenderNum AgeYears UnitAge ;\n\nproc sort data=work.ProcessedData ;\n  by PatientID ; /* 患者IDでソート */\nrun ;\n\n---\n\ndata work.DAT9 ;\n  merge work.DAT9_1 work.DAT9_2 work.DAT9_3 work.DAT9_4 ;\n  by SUBJID ;\n\n  /* 日付・時刻データの変換例 */\n  /* 元データ: \"2023-01-01 10\" のような形式 */\n  FPAYDT  = input( substr(FPAYDTC, 1, 10), yymmdd10. ); /* 日付部分のみを抽出してSAS日付値に変換 */\n  PAYDT1  = input( substr(PAYDTC1, 1, 10), yymmdd10. ); /* 同上 */\n  PAYDT2  = input( substr(PAYDTC2, 1, 10), yymmdd10. ); /* 同上 */\n  \n  /* 完全な日時文字列を作成（例: \"2023-01-01 10:00\"）*/\n  _FPAYDTC = strip( FPAYDTC ) || \":00\" ; \n  _PAYDTC1 = strip( PAYDTC1 ) || \":00\" ;\n  _PAYDTC2 = strip( PAYDTC2 ) || \":00\" ;\n\n  /* 支払い期間に応じた時間差の計算 */\n  if PAYPD = '第1期' then do ;\n    ELATIME = round( ( input( _FPAYDTC, e8601dt19. ) - input( _PAYDTC1, e8601dt19. ) ) / ( 60*60 ) , 1e-10 ); /* 時間単位の差分を計算 */\n  end ;\n  if PAYPD = '第2期' then do ;  \n    ELATIME = round( ( input( _FPAYDTC, e8601dt19. ) - input( _PAYDTC2, e8601dt19. ) ) / ( 60*60 ), 1e-10 ) ; /* 時間単位の差分を計算 */\n  end ;\n\nrun ;\n\n---\n\ndata work.SCRCM ;\n  length SUBJID $12 SCRCMFL $1 ;\n  merge work.CM work.SCR;\n  by SBJID ;\n  SUBJID = SBJID ; /* 変数名を合わせる */\n\n  /* 日付形式の変換例 */\n  /* 元データ: \"YYYY/MM/DD\" -&gt; SASが扱える \"YYYY-MM-DD\" に変換 */\n  if CMSTDTC ^= \"\" then _CMSTDTC = tranwrd ( CMSTDTC , \"/\", \"-\" ) ;\n  if CMENDTC ^= \"\" then _CMENDTC = tranwrd ( CMENDTC , \"/\", \"-\" ) ;\n\n  /* 期間内に該当するかフラグを立てる */\n  if ( CMSTDTC ^= \"\" and CMENDTC ^= \"\" ) and input ( _CMSTDTC, yymmdd10. ) &lt;= SCRDT &lt;= input ( _CMENDTC, yymmdd10. ) then SCRCMFL = \"Y\" ;\n  /* 開始日または終了日と完全に一致する場合も対象 */\n  if SCRDT ^= . and ( input ( _CMSTDTC, yymmdd10. ) = SCRDT or SCRDT = input ( _CMENDTC, yymmdd10. )) then SCRCMFL = \"Y\" ;\n  /* 上記条件に当てはまらない場合は\"N\" */\n  if SCRCMFL = \"\" then SCRCMFL = \"N\" ;\n  /* 開始日または終了日が欠損している場合はフラグをクリア */\n  if ( CMSTDTC = \"\" or CMENDTC = \"\" ) and SCRCMFL ^= \"Y\" then SCRCMFL = \"\" ;\n  /* 開始日がSCRDTより未来の場合は\"N\" */\n  if CMSTDTC ^= \"\" and input ( _CMSTDTC, yymmdd10.) &gt; SCRDT then SCRCMFL = \"N\" ;\n\n  keep SUBJID SCRCMFL ;\nproc sort ;\n  by SUBJID decending SCRCMFL ;\nrun ;\nこのコード、特に注目してほしいのは、setステートメントの**renameオプションと、その後に続くGender = _Gender;のような処理、そして日付・時刻を扱う際に登場する様々な関数**です。「せっかくrenameで名前を変えたのに、なんでまた元の名前の変数に代入し直すの？」と感じるかもしれませんね。実はここに、データ処理のベストプラクティスが隠されているんです！\n\n\nまず、set work.RawData ( rename = (Gender = _Gender Age = _Age ) )の部分。 これは、元のデータセットwork.RawDataから変数を読み込む際に、Genderという変数名を一時的に_Genderに、Ageを_Ageにそれぞれ変更して、新しいデータセットwork.ProcessedDataに持ち込む、という指示です。\nなぜこんなことをするのでしょうか？\n\n\nこれが最も大きな理由です。多くの場合、元データ（ここではwork.RawData）のGender変数は「1（男性）」や「2（女性）」のような数値コードで格納されていることがあります。しかし、最終的に使いたいのは「M（Male）」や「F（Female）」のような、より直感的で標準化された文字データだったりします。\nこのコードでは、まさにその変換を行っています。\nif _Gender = \"1\" then Gender = \"M\" ;\n  else if _Gender = \"2\" then Gender = \"F\" ;\nここで、一時的に_Genderという名前で保持しておいた元の数値コードを参照し、新しいGender変数に変換後の「M」や「F」を代入しています。_Ageについても同様に、文字列として読み込まれた年齢データをinput関数を使って数値型のAgeYearsに変換しています。もしrenameで直接GenderやAgeとして読み込んでしまうと、このような元の値を使った条件分岐やデータ型変換が難しくなってしまいますよね。\n\n\n\n\nここで、renameオプションの働きをもう少し具体的に見てみましょう。\n元のデータセット work.RawData のイメージ:\n\n\n\nPatientID\nGender\nAge\nDiagnosis\n\n\nP001\n1\n35\n一般健診\n\n\nP002\n2\n48\n頭痛\n\n\nP003\n1\n22\n発熱\n\n\nP004\n2\n60\n高血圧\n\n\n\nこのwork.RawDataデータセットを、setステートメントで読み込む際に、次のように指定しています。\nset work.RawData ( rename = (Gender = _Gender Age = _Age ) ) ;\nこの一行が実行されると、work.ProcessedDataデータステップの内部では、Genderは_Genderとして、Ageは_Ageとして扱われます。つまり、データステップのその瞬間、変数は以下のように見えている、とイメージしてください。\ndata work.ProcessedData ステップ内部での変数の見え方（一時的）:\n\n\n\nPatientID\n_Gender\n_Age\nDiagnosis\n\n\nP001\n1\n35\n一般健診\n\n\nP002\n2\n48\n頭痛\n\n\nP003\n1\n22\n発熱\n\n\nP004\n2\n60\n高血圧\n\n\n\nこのように元の変数名を一時的に変更しておくことで、その後の処理で新しいGenderやAgeYearsといった変数を心置きなく作成できるわけです。\n\n\n\n提供されたコードには、日付や時刻の文字列をSASが認識できる数値（SAS日付値やSAS日時値）に変換するための様々な関数が使われています。これらは非常に頻繁に利用されるので、ぜひ使い方を覚えておきましょう。\n\n\nINPUT関数は、文字列をSASが扱える数値に変換するための万能選手です。特に日付や時刻の文字列を扱う際には、その文字列がどのような形式（フォーマット）であるかを指定する必要があります。\n\nYYYYMMDD10.: YYYY-MM-DD形式の文字列（例: “2023-01-01”）をSAS日付値に変換します。\n\n FPAYDT = input( substr(FPAYDTC, 1, 10), yymmdd10. );\n /* FPAYDTCが \"2023-01-01 10\" の場合、\"2023-01-01\" の部分だけを抽出して変換 */\n ```\n\n-   **`E8601DT19.`**: `YYYY-MM-DDTHH:MM:SS`または`YYYY-MM-DD HH:MM:SS`形式のISO 8601日時文字列（例: \"2023-01-01 10:00:00\"）をSAS日時値に変換します。SAS日時値は、1960年1月1日0時0分0秒からの秒数を表す数値です。\n\n``` {.sas eval=\"FALSE,\" code-line-numbers=\"true,\" code-overflow=\"wrap\"}     \n ELATIME = round( ( input( _FPAYDTC, e8601dt19. ) - input( _PAYDTC1, e8601dt19. ) ) / ( 60*60 ) , 1e-10 );\n /* _FPAYDTCと_PAYDTC1（いずれも\"YYYY-MM-DD HH:MM\"形式）を日時値に変換し、その差を時間単位で計算 */ \n ```\n\n#### 2. `SUBSTR`関数\n\n`SUBSTR`関数は、\\*\\*文字列の一部を抜き出す（部分文字列を抽出する）\\*\\*ために使われます。\n\n``` {.sas eval=\"FALSE,\" code-line-numbers=\"true,\" code-overflow=\"wrap\"}        \nFPAYDT = input( substr(FPAYDTC, 1, 10), yymmdd10. );\n/* FPAYDTC（例: \"2023-01-01 10\"）の1文字目から10文字目（つまり\"2023-01-01\"）を抽出 */\nこの例では、日付と時間の情報が混ざった文字列から、日付の部分だけを取り出してINPUT関数に渡しています。\n\n\n\nSTRIP関数は、文字列の先頭や末尾にある余分な半角スペースを取り除くために使われます。特にファイルから読み込んだデータは、固定長形式などで不要なスペースが含まれていることがあるため、後の処理で問題を起こさないようにSTRIPで整形することが推奨されます。\n_FPAYDTC = strip( FPAYDTC ) || \":00\" ;\n/* FPAYDTCの末尾のスペースを取り除き、\"2023-01-01 10\" を \"2023-01-01 10:00\" のように変換 */\nこの例では、STRIPを使ってFPAYDTCから不要なスペースを削除し、それに:00を結合して、E8601DT19.フォーマットで読み取れる「時:分」の形式に整えています。\n\n\n\nTRANWRD関数は、文字列中の特定の文字列を別の文字列に置き換えるために使われます。\n_CMSTDTC = tranwrd ( CMSTDTC , \"/\", \"-\" ) ;\n/* CMSTDTC（例: \"2023/01/01\"）の \"/\" を \"-\" に置き換え、\"2023-01-01\" に変換 */\nこれは、元のデータの日付区切りがスラッシュ（/）なのに、SASの標準的な日付フォーマット（YYYYMMDD10.など）がハイフン（-）を要求する場合によく使われます。INPUT関数で正しく変換できるように、事前にTRANWRDで文字列を整形しているわけですね。\n\n\n\n\nこのように、setステートメントでのrenameオプション、そしてINPUT, SUBSTR, STRIP, TRANWRDといった関数を組み合わせることで、SASはどんな形式のデータでも柔軟に処理し、分析に適した形にクリーンアップできます。\n\n元の値を一時的に保持しつつ、\nデータ型や値を変換・整形し、\n新しい標準化された変数を作成する\n\nこの一連の流れを理解することで、より柔軟で堅牢なSASプログラムを書けるようになります。ぜひ皆さんのデータ処理にもこれらのテクニックを取り入れてみてくださいね！\nデータ処理は奥が深いですが、一つ一つのテクニックを理解していくと、どんどん楽しくなりますよ！何か他にSASに関する疑問があれば、いつでも聞いてくださいね！"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Lifereg.html",
    "href": "posts/statistics/2025/Proc_Lifereg.html",
    "title": "Proc LIFEREG",
    "section": "",
    "text": "SASで生存時間分析を行う際、PROC LIFEREGのコードを見て、ふと疑問に思ったことはありませんか？\nproc lifereg data=mydata;\n    model log(time)*status(0) = treatment age;\nrun;\nなぜ、生存時間 time をそのまま使わず、一手間かけて log(time) と対数をとるのでしょうか？ 単純に time = treatment age; と書くのとでは、何が違うのでしょうか？\nこの記事では、そんな疑問に答えるべく、PROC LIFEREGがlog(T)をモデル化する理由を、統計的な背景から分かりやすく解説していきます。\n\n\nなぜ対数をとるのか？その最大の理由は、共変量と生存時間の関係を、シンプルで扱いやすい「線形モデル」として表現するためです。\nPROC LIFEREGで使われるモデルの基本形は、加速故障時間（AFT）モデルと呼ばれ、以下のように表現されます。\n\nlog(T)=\\beta_0+\\beta_1X_1+…+\\sigma\\epsilon\n\nの式こそが、すべての謎を解く鍵となります。\n\nlog(T): 生存時間の対数。これがモデルの「目的変数(Y)」になります。\nβX: β_0 + β_1X_1 + ... の部分。共変量（説明変数）から計算される予測部分です。\nσε: モデルの「誤差」部分。σはばらつきを調整するスケールパラメータ、εは基準となる標準化された誤差です。\n\nこの「線形」という形にすることで、私たち分析者には大きなメリットがもたらされます。\n\n\n\n\n\nもし生存時間Tを直接モデル化しようとすると、共変量Xとの関係は非常に複雑な曲線を描くことがほとんどです。しかし、log(T)に変換することで、その関係は美しい**一次方程式（直線）**になります。\nこれにより、「Xが1単位増えると、生存時間の対数がβだけ変化する」という、非常に直感的で分かりやすい解釈が可能になるのです。\n\n\n\nここがAFTモデルの最もエレガントな部分です。モデルの基本形 log(T) = βX + σε はそのままに、誤差項εが従う確率分布を変えるだけで、様々な生存時間分布を表現できます。\n\n\n\n\n\n\n\n\nεが従う分布（標準化済）\nTが従う生存時間分布\nPROC LIFEREGでの指定\n\n\n標準極値分布\nワイブル分布 (Weibull)\nDIST=WEIBULL (デフォルト)\n\n\n標準正規分布\n対数正規分布 (Lognormal)\nDIST=LNORMAL\n\n\n標準ロジスティック分布\n対数ロジスティック分布 (Log-logistic)\nDIST=LOGISTIC\n\n\n\nlog変換は、多種多様な分布を同じ土俵で議論するための「共通言語」のような役割を果たしているのです。\n\n\n\n\nLIFEREG procedure:\ndata Headache;\n   input Minutes Group Censor @@;\n   datalines;\n11  1  0   12  1  0   19  1  0   19  1  0\n19  1  0   19  1  0   21  1  0   20  1  0\n21  1  0   21  1  0   20  1  0   21  1  0\n20  1  0   21  1  0   25  1  0   27  1  0\n30  1  0   21  1  1   24  1  1   14  2  0\n16  2  0   16  2  0   21  2  0   21  2  0\n23  2  0   23  2  0   23  2  0   23  2  0\n25  2  1   23  2  0   24  2  0   24  2  0\n26  2  1   32  2  1   30  2  1   30  2  0\n32  2  1   20  2  1\n;\n\nproc lifereg data=Headache;\n   class Group;\n   model Minutes*Censor(1)=Group /dist = exponential;\n   output out=New cdf=Prob;\nrun;\n\n\n\nこんにちは！今回は、SASを使った生存時間分析の定番PROC LIFEREGの出力結果を、初心者の方にも分かりやすく解説していきます。\n頭痛薬の効果持続時間を2つのグループで比較する、というシナリオで見ていきましょう。\n\n\nまず、使用したSASコードはこちらです。\nSAS\ndata Headache;\n   input Minutes Group Censor @@;\n   datalines;\n11  1  0   12  1  0   19  1  0   19  1  0\n19  1  0   19  1  0   21  1  0   20  1  0\n21  1  0   21  1  0   20  1  0   21  1  0\n20  1  0   21  1  0   25  1  0   27  1  0\n30  1  0   21  1  1   24  1  1   14  2  0\n16  2  0   16  2  0   21  2  0   21  2  0\n23  2  0   23  2  0   23  2  0   23  2  0\n25  2  1   23  2  0   24  2  0   24  2  0\n26  2  1   32  2  1   30  2  1   30  2  0\n32  2  1   20  2  1\n;\n\nproc lifereg data=Headache;\n   class Group;\n   model Minutes*Censor(1)=Group /dist = exponential;\n   output out=New cdf=Prob;\nrun;\nmodelステートメントのdist = exponentialで、今回は指数分布を仮定して分析しています。Censor(1)は、Censor変数の値が1の場合にそのデータが「打ち切り」であることを示します。\nそれでは、このコードが生成したアウトプットを上から順に見ていきましょう！\n\n\n\n\n分離変数の水準の情報 (Level Information for Class Variables) CLASSステートメントで指定したGroup変数が、値として1と2を持っていることを示しています。\n適合度統計量 (Goodness-of-Fit Statistics) AICやBICといった指標が並んでいますね。これらの値は小さいほど、モデルの当てはまりが良いことを意味します。この結果だけでは評価できませんが、例えばdist = weibull（ワイブル分布）で分析した結果と比較して、どちらのモデルがよりデータにフィットしているかを見るために使います。\nアルゴリズムは収束しました。(The algorithm has converged.) モデルの計算が問題なく完了したことを示す、重要なメッセージです。これが表示されていれば一安心です。\n\n\n\n\n\n効果に対する Type III 分析 (Type III Analysis of Effects) ここが最初の重要な結果です。この表は、モデルに含まれる変数Groupが、生存時間（頭痛薬の効果持続時間）に統計的に有意な影響を与えているかを「全体として」検定します。\n注目するのはPr &gt; ChiSqの値です。これが、いわゆるp値です。\n\n結果: Pr &gt; ChiSq = 0.2778\n\n一般的にp値が0.05より小さい場合に「有意な差がある」と判断しますが、今回は0.2778と0.05より大きいです。 したがって、「2つのグループ間で、効果持続時間に統計的に有意な差があるとは言えない」という結論になります。\n\n\n\n\n\n最大尤度パラメータ推定値の分析 (Analysis of Maximum Likelihood Parameter Estimates) モデルの「中身」を詳しく見ていく部分です。\n\nIntercept (切片): これは基準となるグループ（今回はGroup 2）の対数スケールでの平均生存時間を示しています。推定値は3.5354です。\nGroup 1: これがこの分析の主役です。この行は、基準のGroup 2と比較して、Group 1がどれだけ違うかを示しています。\n\n推定値: -0.3999\nPr &gt; ChiSq (p値): 0.2778 p値はType III 分析の結果と一致していますね。やはり有意ではありません。\n\n尺度 (Scale) & Weibull 形状 (Weibull shape): dist = exponential（指数分布）を指定したため、尺度 (σ) は自動的に1に固定されます。Weibull 形状は尺度の逆数（1/σ）なので、こちらも1になります。これは、正しく指数分布モデルが適用されていることを示しています。\n\n\n\n\nGroup 1の推定値-0.3999はどう解釈すれば良いのでしょうか？ PROC LIFEREGは対数（log）で時間をモデル化しているので、元の時間スケールに戻すにはexp()を使います。\nexp(−0.3999)≈0.670\nこれは加速因子 (Acceleration Factor) と呼ばれ、「Group 1の時間は、Group 2の時間の0.67倍になる」と解釈できます。つまり、Group 1の方が効果持続時間（イベント発生までの時間）が短い傾向にある、という結果です。（ただし、この差は統計的に有意ではありませんでした。）\n\n\n\n\n今回の分析から分かったことをまとめると、以下のようになります。\n\n頭痛薬の効果持続時間について、指数分布モデルを適用して分析した。\nグループ（Group 1 vs Group 2）による持続時間の差は、統計的に有意ではなかった (p = 0.2778)。\nモデル上は、Group 1の効果持続時間はGroup 2の約0.67倍と、短い傾向が示唆されたが、これは偶然の範囲内と言える。\n\nPROC LIFEREGの出力は一見複雑に見えますが、見るべきポイントを押さえれば、分析結果を深く理解することができます。次はdist=weibullを試して、AICを比較してみるのも面白いかもしれませんね！\n\n\n\nこれは、モデルがlog(時間)を扱っていることに直結します。\nモデルの基本形は以下の通りです。\n\nGroup 2 (基準): log(Time_G2) = Intercept\nGroup 1: log(Time_G1) = Intercept + β_G1\n\nここで、2つの式の差をとってみます。\nlog(Time_G1) - log(Time_G2) = β_G1\n対数の性質上、「引き算」は「割り算」に変換できます（log(a) - log(b) = log(a/b)）。\nlog(Time_G1 / Time_G2) = β_G1\n最後に、両辺をexp()してlogを外すと、\nTime_G1 / Time_G2 = exp(β_G1)\nこの式が最終的な答えです。Group 1の時間とGroup 2の時間の関係は、差（Time_G1 - Time_G2）ではなく**比（Time_G1 / Time_G2）**で表されていますね。\nこのexp(β_G1)が、前回解説した加速因子 (0.67) です。グループの効果は、基準となる時間を何倍に「加速」または「減速」させるか、という乗法的な（掛け算の）効果として解釈されるのです。"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Lifereg.html#結論ファーストすべては美しい線形モデルのため",
    "href": "posts/statistics/2025/Proc_Lifereg.html#結論ファーストすべては美しい線形モデルのため",
    "title": "Proc LIFEREG",
    "section": "",
    "text": "なぜ対数をとるのか？その最大の理由は、共変量と生存時間の関係を、シンプルで扱いやすい「線形モデル」として表現するためです。\nPROC LIFEREGで使われるモデルの基本形は、加速故障時間（AFT）モデルと呼ばれ、以下のように表現されます。\n\nlog(T)=\\beta_0+\\beta_1X_1+…+\\sigma\\epsilon\n\nの式こそが、すべての謎を解く鍵となります。\n\nlog(T): 生存時間の対数。これがモデルの「目的変数(Y)」になります。\nβX: β_0 + β_1X_1 + ... の部分。共変量（説明変数）から計算される予測部分です。\nσε: モデルの「誤差」部分。σはばらつきを調整するスケールパラメータ、εは基準となる標準化された誤差です。\n\nこの「線形」という形にすることで、私たち分析者には大きなメリットがもたらされます。"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Lifereg.html#なぜ一手間かけて-logt-にするのか3つの大きなメリット",
    "href": "posts/statistics/2025/Proc_Lifereg.html#なぜ一手間かけて-logt-にするのか3つの大きなメリット",
    "title": "Proc LIFEREG",
    "section": "",
    "text": "もし生存時間Tを直接モデル化しようとすると、共変量Xとの関係は非常に複雑な曲線を描くことがほとんどです。しかし、log(T)に変換することで、その関係は美しい**一次方程式（直線）**になります。\nこれにより、「Xが1単位増えると、生存時間の対数がβだけ変化する」という、非常に直感的で分かりやすい解釈が可能になるのです。\n\n\n\nここがAFTモデルの最もエレガントな部分です。モデルの基本形 log(T) = βX + σε はそのままに、誤差項εが従う確率分布を変えるだけで、様々な生存時間分布を表現できます。\n\n\n\n\n\n\n\n\nεが従う分布（標準化済）\nTが従う生存時間分布\nPROC LIFEREGでの指定\n\n\n標準極値分布\nワイブル分布 (Weibull)\nDIST=WEIBULL (デフォルト)\n\n\n標準正規分布\n対数正規分布 (Lognormal)\nDIST=LNORMAL\n\n\n標準ロジスティック分布\n対数ロジスティック分布 (Log-logistic)\nDIST=LOGISTIC\n\n\n\nlog変換は、多種多様な分布を同じ土俵で議論するための「共通言語」のような役割を果たしているのです。"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Lifereg.html#参考sas-helpのプログラム",
    "href": "posts/statistics/2025/Proc_Lifereg.html#参考sas-helpのプログラム",
    "title": "Proc LIFEREG",
    "section": "",
    "text": "LIFEREG procedure:\ndata Headache;\n   input Minutes Group Censor @@;\n   datalines;\n11  1  0   12  1  0   19  1  0   19  1  0\n19  1  0   19  1  0   21  1  0   20  1  0\n21  1  0   21  1  0   20  1  0   21  1  0\n20  1  0   21  1  0   25  1  0   27  1  0\n30  1  0   21  1  1   24  1  1   14  2  0\n16  2  0   16  2  0   21  2  0   21  2  0\n23  2  0   23  2  0   23  2  0   23  2  0\n25  2  1   23  2  0   24  2  0   24  2  0\n26  2  1   32  2  1   30  2  1   30  2  0\n32  2  1   20  2  1\n;\n\nproc lifereg data=Headache;\n   class Group;\n   model Minutes*Censor(1)=Group /dist = exponential;\n   output out=New cdf=Prob;\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Lifereg.html#sasのproc-lifereg徹底解説指数分布モデルの結果を読み解く",
    "href": "posts/statistics/2025/Proc_Lifereg.html#sasのproc-lifereg徹底解説指数分布モデルの結果を読み解く",
    "title": "Proc LIFEREG",
    "section": "",
    "text": "こんにちは！今回は、SASを使った生存時間分析の定番PROC LIFEREGの出力結果を、初心者の方にも分かりやすく解説していきます。\n頭痛薬の効果持続時間を2つのグループで比較する、というシナリオで見ていきましょう。\n\n\nまず、使用したSASコードはこちらです。\nSAS\ndata Headache;\n   input Minutes Group Censor @@;\n   datalines;\n11  1  0   12  1  0   19  1  0   19  1  0\n19  1  0   19  1  0   21  1  0   20  1  0\n21  1  0   21  1  0   20  1  0   21  1  0\n20  1  0   21  1  0   25  1  0   27  1  0\n30  1  0   21  1  1   24  1  1   14  2  0\n16  2  0   16  2  0   21  2  0   21  2  0\n23  2  0   23  2  0   23  2  0   23  2  0\n25  2  1   23  2  0   24  2  0   24  2  0\n26  2  1   32  2  1   30  2  1   30  2  0\n32  2  1   20  2  1\n;\n\nproc lifereg data=Headache;\n   class Group;\n   model Minutes*Censor(1)=Group /dist = exponential;\n   output out=New cdf=Prob;\nrun;\nmodelステートメントのdist = exponentialで、今回は指数分布を仮定して分析しています。Censor(1)は、Censor変数の値が1の場合にそのデータが「打ち切り」であることを示します。\nそれでは、このコードが生成したアウトプットを上から順に見ていきましょう！\n\n\n\n\n分離変数の水準の情報 (Level Information for Class Variables) CLASSステートメントで指定したGroup変数が、値として1と2を持っていることを示しています。\n適合度統計量 (Goodness-of-Fit Statistics) AICやBICといった指標が並んでいますね。これらの値は小さいほど、モデルの当てはまりが良いことを意味します。この結果だけでは評価できませんが、例えばdist = weibull（ワイブル分布）で分析した結果と比較して、どちらのモデルがよりデータにフィットしているかを見るために使います。\nアルゴリズムは収束しました。(The algorithm has converged.) モデルの計算が問題なく完了したことを示す、重要なメッセージです。これが表示されていれば一安心です。\n\n\n\n\n\n効果に対する Type III 分析 (Type III Analysis of Effects) ここが最初の重要な結果です。この表は、モデルに含まれる変数Groupが、生存時間（頭痛薬の効果持続時間）に統計的に有意な影響を与えているかを「全体として」検定します。\n注目するのはPr &gt; ChiSqの値です。これが、いわゆるp値です。\n\n結果: Pr &gt; ChiSq = 0.2778\n\n一般的にp値が0.05より小さい場合に「有意な差がある」と判断しますが、今回は0.2778と0.05より大きいです。 したがって、「2つのグループ間で、効果持続時間に統計的に有意な差があるとは言えない」という結論になります。\n\n\n\n\n\n最大尤度パラメータ推定値の分析 (Analysis of Maximum Likelihood Parameter Estimates) モデルの「中身」を詳しく見ていく部分です。\n\nIntercept (切片): これは基準となるグループ（今回はGroup 2）の対数スケールでの平均生存時間を示しています。推定値は3.5354です。\nGroup 1: これがこの分析の主役です。この行は、基準のGroup 2と比較して、Group 1がどれだけ違うかを示しています。\n\n推定値: -0.3999\nPr &gt; ChiSq (p値): 0.2778 p値はType III 分析の結果と一致していますね。やはり有意ではありません。\n\n尺度 (Scale) & Weibull 形状 (Weibull shape): dist = exponential（指数分布）を指定したため、尺度 (σ) は自動的に1に固定されます。Weibull 形状は尺度の逆数（1/σ）なので、こちらも1になります。これは、正しく指数分布モデルが適用されていることを示しています。\n\n\n\n\nGroup 1の推定値-0.3999はどう解釈すれば良いのでしょうか？ PROC LIFEREGは対数（log）で時間をモデル化しているので、元の時間スケールに戻すにはexp()を使います。\nexp(−0.3999)≈0.670\nこれは加速因子 (Acceleration Factor) と呼ばれ、「Group 1の時間は、Group 2の時間の0.67倍になる」と解釈できます。つまり、Group 1の方が効果持続時間（イベント発生までの時間）が短い傾向にある、という結果です。（ただし、この差は統計的に有意ではありませんでした。）\n\n\n\n\n今回の分析から分かったことをまとめると、以下のようになります。\n\n頭痛薬の効果持続時間について、指数分布モデルを適用して分析した。\nグループ（Group 1 vs Group 2）による持続時間の差は、統計的に有意ではなかった (p = 0.2778)。\nモデル上は、Group 1の効果持続時間はGroup 2の約0.67倍と、短い傾向が示唆されたが、これは偶然の範囲内と言える。\n\nPROC LIFEREGの出力は一見複雑に見えますが、見るべきポイントを押さえれば、分析結果を深く理解することができます。次はdist=weibullを試して、AICを比較してみるのも面白いかもしれませんね！\n\n\n\nこれは、モデルがlog(時間)を扱っていることに直結します。\nモデルの基本形は以下の通りです。\n\nGroup 2 (基準): log(Time_G2) = Intercept\nGroup 1: log(Time_G1) = Intercept + β_G1\n\nここで、2つの式の差をとってみます。\nlog(Time_G1) - log(Time_G2) = β_G1\n対数の性質上、「引き算」は「割り算」に変換できます（log(a) - log(b) = log(a/b)）。\nlog(Time_G1 / Time_G2) = β_G1\n最後に、両辺をexp()してlogを外すと、\nTime_G1 / Time_G2 = exp(β_G1)\nこの式が最終的な答えです。Group 1の時間とGroup 2の時間の関係は、差（Time_G1 - Time_G2）ではなく**比（Time_G1 / Time_G2）**で表されていますね。\nこのexp(β_G1)が、前回解説した加速因子 (0.67) です。グループの効果は、基準となる時間を何倍に「加速」または「減速」させるか、という乗法的な（掛け算の）効果として解釈されるのです。"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html",
    "href": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html",
    "title": "Proc Contents ProcedureとProc Dataset Procedure",
    "section": "",
    "text": "本記事では、Proc Contents ProcedureとProc Dtaset Procecdureについて解説する。"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#sas",
    "href": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#sas",
    "title": "Proc Contents ProcedureとProc Dataset Procedure",
    "section": "",
    "text": "本記事では、Proc Contents ProcedureとProc Dtaset Procecdureについて解説する。"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#proc-contents-procedure",
    "href": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#proc-contents-procedure",
    "title": "Proc Contents ProcedureとProc Dataset Procedure",
    "section": "0.2 Proc Contents Procedure",
    "text": "0.2 Proc Contents Procedure\nCONTENTS プロシージャは、SAS データセットの内容を表示し、SAS ライブラリのディレクトリを印刷します。一般的に、CONTENTS プロシージャは DATASETS プロシージャの CONTENTS ステートメントと同じ機能を持ちます。CONTENTS プロシージャと PROC DATASETS の CONTENTS ステートメントの違いは以下の通りです：\n\nPROC CONTENTS の DATA= オプションにおける libref のデフォルトは Work です。CONTENTS ステートメントでは、デフォルトはプロシージャ入力ライブラリの libref です。\nPROC CONTENTS は順次ファイルを読み取ることができます。CONTENTS ステートメントはできません。"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#proc-contents-procedureの特徴",
    "href": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#proc-contents-procedureの特徴",
    "title": "Proc Contents ProcedureとProc Dataset Procedure",
    "section": "0.3 Proc Contents Procedureの特徴",
    "text": "0.3 Proc Contents Procedureの特徴\n\nPROC CONTENTS reports metadata about the table and the metadata about the variables.\nデータセットにおけるVariable、Type(Char、Num） 、Fromat、Labelをデータセット化できる！"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#構文",
    "href": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#構文",
    "title": "Proc Contents ProcedureとProc Dataset Procedure",
    "section": "0.4 構文",
    "text": "0.4 構文\nPROC CONTENTS &lt;options&gt;;\n\n0.4.1 DATA=SAS-file-specification\nspecifies an entire library or a specific SAS data set within a library. SAS-file-specification can take one of the following forms:\n\n\n0.4.2 &lt;libref.&gt;SAS-data-set\nnames one SAS data set to process. The default for libref is the libref of the procedure input library. For example, to obtain the contents of the SAS data set HtWt from the procedure input library, use the following CONTENTS statement:\ncontents data=HtWt;\nTo obtain the contents of a specific version from a generation group, use the GENNUM= data set option as shown in the following CONTENTS statement:\ncontents data=HtWt(gennum=3);\n\n\n0.4.3 &lt;libref.&gt;_ALL_\ngives you information about all SAS data sets that have the type or types specified by the MEMTYPE= option. libref refers to the SAS library. The default for libref is the libref of the procedure input library.\n\nIf you are using the _ALL_ keyword, you need Read access to all read-protected SAS data sets in the SAS library.\nDATA=_ALL_ automatically prints a listing of the SAS files that are contained in the SAS library. Note that for SAS views, all librefs that are associated with the views must be assigned in the current session in order for them to be processed for the listing.\n\n\n\n\n\n\n\n\nDefault\nmost recently created data set in your job or session, from any SAS library.\n\n\n\n\nTip\nIf you specify a read-protected data set in the DATA= option but do not give the Read password, by default the procedure looks in the PROC DATASETS statement for the Read password. However, if you do not specify the DATA= option and the default data set (last one created in the session) is Read protected, the procedure does not look in the PROC DATASETS statement for the Read password.\n\n\n\nすなわち指定したlibraryに含まれるSASデータセット全てを指定することもできるし、指定したlibraryの特定のデータセットを指定することもできる。\n\n\n0.4.4 MEMTYPE=(member-type(s))\nrestricts processing to one or more member types. The CONTENTS statement produces output only for member types DATA, VIEW, and ALL, which includes DATA and VIEW.\nMEMTYPE= in the CONTENTS statement differs from MEMTYPE= in most of the other statements in the DATASETS procedure in the following ways:\n\nA slash does not precede the option.\nYou cannot enclose the MEMTYPE= option in parentheses to limit its effect to only the SAS file immediately preceding it.\n\nMEMTYPE= results in a directory of the library in which the DATA= member is located. However, MEMTYPE= does not limit the types of members whose contents are displayed unless the _ALL_ keyword is used in the DATA= option. For example, the following statements produce the contents of only the SAS data sets with the member type DATA:\nproc datasets memtype=data;   \nmentypeはデータセットのみが欲しいときは指定したらよいと思うが、基本的にする必要はないだろう。\n\n\n0.4.5 NOPRINT\nsuppresses printing the output of the CONTENTS statement.\n\n\n0.4.6 ORDER=COLLATE | CASECOLLATE | IGNORECASE | VARNUM\n基本的にorder = varnumとしておけばよい。\n\n\n\n\n\n\n\nCOLLATE\nprints a list of variables in alphabetical order beginning with uppercase and then lowercase names.\n\n\nCASECOLLATE\nprints a list of variables in alphabetical order even if they include mixed-case names and numerics.\n\n\nIGNORECASE\nprints a list of variables in alphabetical order ignoring the case of the letters.\n\n\nVARNUM\nis the same as the VARNUM option.\n\n\n\n\n\n\nNote\nThe ORDER= option does not affect the order of the OUT= and OUT2= data sets.\n\n\n\n\nSee\nVARNUM\n\n\nExample\nSee Using the ORDER= Option to compare the default and the four options for ORDER=.\n\n\n\n\n\n0.4.7 OUT=SAS-data-set\nnames an output SAS data set.\n\n\n\nTip\nOUT= does not suppress the printed output from the statement. If you want to suppress the printed output, you must use the NOPRINT option.\n\n\n\n\nSee\nThe OUT= Data Set for a description of the variables in the OUT= data set.\n\n\n\n\n\n0.4.8 OUT2=SAS-data-set\nnames the output data set to contain information about indexes and integrity constraints.\n\n\n\nNote\nWhen you use the OUT2=PermanentLibrary_ALL_ option within PROC CONTENTS or PROC DATASETS with the CONTENTS statement, you must also set the REPLACE=YES data set option or the REPLACE system option.\n\n\n\n\nTips\nIf UPDATECENTILES was not specified in the index definition, then the default value of 5 is used in the re-create variable of the OUT2 data set.\n\n\nOUT2= does not suppress the printed output from the statement. To suppress the printed output, use the NOPRINT option.\n\n\n\nSee\nThe OUT2= Data Set for a description of the variables in the OUT2= data set.\n\n\n\n以下のようにしておけばよい\nproc contents data=sashelp.class out=out1 varnum ; run;"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#short-option",
    "href": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#short-option",
    "title": "Proc Contents ProcedureとProc Dataset Procedure",
    "section": "0.5 Short Option",
    "text": "0.5 Short Option\nShort Optionを使えば、データセットに格納されている順番で、全変数を1つのマクロ変数に格納できる！たまに便利では？\n\n参考記事：PROC CONTENTSのSHORTオプションはアイディア次第で役に立ちそう。\n\n\n*** 全変数名を（データセットに格納されてる順で）1つのマクロ変数に格納する ;\nods output PositionShort = OUT1;\n    proc contents data=sashelp.class  short  varnum;\n    run;\nods output close;\n\ndata _NULL_ ;\n    set OUT1 ;\n    call symputx(\"VARS\", VARIABLES);\nrun;\n\n%put &VARS;\nOUT1にVARIABLESという1変数が格納され、そこにデータセットに含まれる全変数が格納されている。"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#copy",
    "href": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#copy",
    "title": "Proc Contents ProcedureとProc Dataset Procedure",
    "section": "1.1 Copy",
    "text": "1.1 Copy\nUsed to copy or move a SAS member from one library to another.To limit copying to specific members, use either the SELECT or EXCLUDE options. To specify a different library to copy from use either the DATASETS LIBRARY option to specify a default library or use the IN= option. To move a member from one library to another and then delete the original member, use the MOVE optionThe following example moves two members from lib1 to lib2:\nLIBNAME lib1 ‘SAS-data-library’;\nLIBNAME lib2 ‘SAS-data-library’;\nPROC DATASETS;\nCOPY IN=lib1 OUT=lib2 MOVE;\nSELECT member1 member2;\nRUN;\nデータセットのCopyについてはProc Copyも役に立つ\n\n【PROC COPY】データセットを他のライブラリに一括コピー\n\n  PROC COPY\n       IN                =   コピー対象のライブラリ\n       OUT            =   出力先のライブラリ\n       MEMTYPE  =   (コピー対象のデータのタイプ) ;\n       SELECT     コピー対象のデータ ;\n       EXCLUDE  コピー対象外のデータ ;\n   RUN;"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#kill",
    "href": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#kill",
    "title": "Proc Contents ProcedureとProc Dataset Procedure",
    "section": "1.2 Kill",
    "text": "1.2 Kill\nThe following example shows how to delete all the members within a permanent SAS library using the KILL option:\nLIBNAME input ‘SAS-data-library’;\nPROC DATASETS LIBRARY=input KILL;\nRUN;"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#データセットのattribを全部消す",
    "href": "posts/statistics/2025/Proc_Contents_Proc_Dataset.html#データセットのattribを全部消す",
    "title": "Proc Contents ProcedureとProc Dataset Procedure",
    "section": "1.3 データセットのattribを全部消す",
    "text": "1.3 データセットのattribを全部消す\nlibname mylib 'c:\\mylib';\nproc contents data=mylib.class;\nrun;\nproc datasets lib=mylib memtype=data;\n   modify class;\n     attrib _all_ label=' ';\n     attrib _all_ format=;\ncontents data=mylib.class;\nrun;\nquit;"
  },
  {
    "objectID": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html",
    "href": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html",
    "title": "Joint Interventionにおけるモデル構築",
    "section": "",
    "text": "疫学研究において、2つの二値曝露（A₁, A₂）がアウトカムに与える共同効果（Joint Effect）を評価する場面は頻繁にあります。その際のモデリング戦略として、一般的に以下の2つのアプローチが考えられます。\n\n4カテゴリーのダミー変数モデル: 参照群（例: A₁=0, A₂=0）を基準に、3つのダミー変数を作成する\n交互作用モデル: 主効果（A₁, A₂）と交互作用項（A₁ × A₂）をモデルに投入する\n\n本稿では、まず線形回帰モデルにおける両者の数学的な等価性を詳細に示し、Rを用いた数値シミュレーションでその結果を実証します。さらに、この関係がロジスティック回帰モデルにも拡張できることを確認します。\n\n\n\n線形回帰の枠組みでは、2つのモデルが数学的に全く同じものであることを明確に示せます。\n\n\nアウトカムをY、曝露グループごとのサンプル平均を Ȳₐ₁ₐ₂ とします。\n\n\n参照群を (A₁=0, A₂=0) とします。\nE[Y] = β₀ + β₁D₁₀ + β₂D₀₁ + β₃D₁₁\n最小二乗法（OLS）による係数βの推定量は、サンプル平均との差で直感的に表現できます。\n\nβ̂₀ = Ȳ₀₀\nβ̂₁ = Ȳ₁₀ − Ȳ₀₀\nβ̂₂ = Ȳ₀₁ − Ȳ₀₀\nβ̂₃ = Ȳ₁₁ − Ȳ₀₀\n\nここで、β̂₃が参照群に対する共同効果の直接的な推定量となります。\n\n\n\nE[Y] = γ₀ + γ₁A₁ + γ₂A₂ + γ₃(A₁ × A₂)\nこのモデルの係数γも同様にサンプル平均から導出できます。\n\nγ̂₀ = Ȳ₀₀\nγ̂₁ = Ȳ₁₀ − Ȳ₀₀ （A₁の主効果）\nγ̂₂ = Ȳ₀₁ − Ȳ₀₀ （A₂の主効果）\nγ̂₃ = Ȳ₁₁ − Ȳ₁₀ − Ȳ₀₁ + Ȳ₀₀ （交互作用）\n\nModel Iにおける共同効果は、係数の線形和 γ̂₁ + γ̂₂ + γ̂₃ で表されます。これを展開すると、\n(Ȳ₁₀ − Ȳ₀₀) + (Ȳ₀₁ − Ȳ₀₀) + (Ȳ₁₁ − Ȳ₁₀ − Ȳ₀₁ + Ȳ₀₀) = Ȳ₁₁ − Ȳ₀₀\nとなり、これはModel Dのβ̂₃と代数的に完全に一致します。\n\n\n\n\n点推定量の正体が同じ（どちらも Ȳ₁₁ − Ȳ₀₀）であるため、標準誤差も必然的に同じになります。\nVar(β̂₃) = Var(γ̂₁ + γ̂₂ + γ̂₃) = Var(Ȳ₁₁ − Ȳ₀₀) = σ²/n₁₁ + σ²/n₀₀\nしたがって、両モデルは同じ情報を異なるパラメータで表現した**再パラメータ化（re-parameterization）**の関係にあり、導き出される結論は同一です。\n\n\n\n\n理論的な等価性を、シミュレーションデータを用いてRで確認します。\n# パッケージの読み込み（必要に応じてインストール）\n# install.packages(\"emmeans\")\nlibrary(emmeans)\n\n# 再現性のためのシード設定\nset.seed(123)\n\n# 1. データ生成\nn &lt;- 400\ndf &lt;- data.frame(\n  A1 = rbinom(n, 1, 0.5),\n  A2 = rbinom(n, 1, 0.4)\n)\n\n# Yを生成 (A1=1, A2=1のときに強い効果を持たせる)\ndf$Y_linear &lt;- 10 + 2*df$A1 + 3*df$A2 + 5*df$A1*df$A2 + rnorm(n, 0, 4)\n\n# 2. 4カテゴリーのダミー変数を作成\n# 参照群を A1=0, A2=0 に設定\ndf$group &lt;- factor(paste0(\"A1=\", df$A1, \", A2=\", df$A2))\ndf$group &lt;- relevel(df$group, ref = \"A1=0, A2=0\")\n\n# 3. モデルのフィッティング\n# Model D: ダミー変数モデル\nmodel_D_lm &lt;- lm(Y_linear ~ group, data = df)\ncat(\"--- ダミー変数モデル (Model D) の結果 ---\\n\")\nprint(summary(model_D_lm))\n\n# Model I: 交互作用モデル\nmodel_I_lm &lt;- lm(Y_linear ~ A1 * A2, data = df)\ncat(\"\\n--- 交互作用モデル (Model I) の結果 ---\\n\")\nprint(summary(model_I_lm))\n\n# 4. 共同効果の比較\ncat(\"\\n--- 共同効果の点推定値と標準誤差の比較 ---\\n\")\n\n# Model Dから共同効果を直接取得\nbeta3 &lt;- summary(model_D_lm)$coefficients[\"groupA1=1, A2=1\",]\ncat(sprintf(\"Model D (ダミー) の共同効果: Estimate = %.4f, SE = %.4f\\n\", beta3[1], beta3[2]))\n\n# Model Iから共同効果を計算\n# 方法1: emmeansパッケージを使用\njoint_effect &lt;- emmeans(model_I_lm, ~ A1*A2)\ncontrast_result &lt;- contrast(joint_effect, method = list(\"Joint Effect (1,1 vs 0,0)\" = c(-1, 0, 0, 1)))\ncontrast_summary &lt;- summary(contrast_result)\ncat(sprintf(\"Model I (交互作用) の共同効果: Estimate = %.4f, SE = %.4f (emmeansを使用)\\n\",\n            contrast_summary$estimate[1], contrast_summary$SE[1]))\n\n# 方法2: 手動計算による確認\n# γ1 + γ2 + γ3 = 共同効果\ncoef_I &lt;- summary(model_I_lm)$coefficients\njoint_manual &lt;- coef_I[\"A1\", \"Estimate\"] + coef_I[\"A2\", \"Estimate\"] + coef_I[\"A1:A2\", \"Estimate\"]\ncat(sprintf(\"Model I (交互作用) の共同効果: Estimate = %.4f (手動計算)\\n\", joint_manual))\n\n# 標準誤差の手動計算（共分散行列を使用）\nvcov_matrix &lt;- vcov(model_I_lm)\n# γ1 + γ2 + γ3 の分散を計算\ncontrast_vector &lt;- c(0, 1, 1, 1)  # (Intercept, A1, A2, A1:A2)\njoint_var &lt;- t(contrast_vector) %*% vcov_matrix %*% contrast_vector\njoint_se_manual &lt;- sqrt(joint_var)\ncat(sprintf(\"Model I (交互作用) の共同効果: SE = %.4f (手動計算)\\n\", joint_se_manual))\n\n\n上記のRコードを実行すると、以下の結果が得られます。\nModel D の groupA1=1, A2=1 の係数：\n\nEstimate: 10.1044\nStd. Error: 0.5856\n\nModel I から計算したJoint Effect：\n\nemmeansを使用: Estimate = 10.1044, SE = 0.5856\n手動計算: Estimate = 10.1044, SE = 0.5856\n\nこの結果は、共同効果の点推定値と標準誤差が、小数点以下4桁まで完全に一致することを示しており、理論的な等価性を実証しています。\n\n\n\n\nこの等価性の原則は、ロジスティック回帰にもそのまま適用される。再パラメータ化の関係は、この線形予測子の部分で成立するため、共同効果の対数オッズ比とその標準誤差も、2つのモデルで完全に一致します。\n\n\n# 再現性のためのシード設定\nset.seed(456)\n\n# 1. ロジスティック回帰用のデータ生成\nlogit_p &lt;- -2 + 0.5*df$A1 + 0.8*df$A2 + 1.2*df$A1*df$A2\nprob &lt;- exp(logit_p) / (1 + exp(logit_p))\ndf$Y_logistic &lt;- rbinom(n, 1, prob)\n\n# 2. モデルのフィッティング\n# Model D: ダミー変数モデル\nmodel_D_glm &lt;- glm(Y_logistic ~ group, data = df, family = \"binomial\")\n# Model I: 交互作用モデル\nmodel_I_glm &lt;- glm(Y_logistic ~ A1 * A2, data = df, family = \"binomial\")\n\n# 3. 共同効果（対数オッズ比）の比較\ncat(\"\\n--- ロジスティック回帰における共同効果の比較 ---\\n\")\n\n# Model Dから直接取得\nbeta3_glm &lt;- summary(model_D_glm)$coefficients[\"groupA1=1, A2=1\",]\ncat(sprintf(\"Model D (ダミー) の共同効果(LOR): Estimate = %.4f, SE = %.4f\\n\", beta3_glm[1], beta3_glm[2]))\n\n# Model Iから計算\njoint_effect_glm &lt;- emmeans(model_I_glm, ~ A1*A2)\ncontrast_glm &lt;- contrast(joint_effect_glm, method = list(\"Joint Effect (1,1 vs 0,0)\" = c(-1, 0, 0, 1)))\ncontrast_glm_summary &lt;- summary(contrast_glm)\ncat(sprintf(\"Model I (交互作用) の共同効果(LOR): Estimate = %.4f, SE = %.4f (emmeansを使用)\\n\",\n            contrast_glm_summary$estimate[1], contrast_glm_summary$SE[1]))\n\n# 手動計算による確認\ncoef_glm &lt;- summary(model_I_glm)$coefficients\njoint_manual_glm &lt;- coef_glm[\"A1\", \"Estimate\"] + coef_glm[\"A2\", \"Estimate\"] + coef_glm[\"A1:A2\", \"Estimate\"]\ncat(sprintf(\"Model I (交互作用) の共同効果(LOR): Estimate = %.4f (手動計算)\\n\", joint_manual_glm))\n\n# 標準誤差の手動計算\nvcov_matrix_glm &lt;- vcov(model_I_glm)\ncontrast_vector &lt;- c(0, 1, 1, 1)  # (Intercept, A1, A2, A1:A2)\njoint_var_glm &lt;- t(contrast_vector) %*% vcov_matrix_glm %*% contrast_vector\njoint_se_manual_glm &lt;- sqrt(joint_var_glm)\ncat(sprintf(\"Model I (交互作用) の共同効果(LOR): SE = %.4f (手動計算)\\n\", joint_se_manual_glm))\n\n\n\nModel D (logistic) の groupA1=1, A2=1 の係数：\n\nEstimate: 1.6801\nStd. Error: 0.3567\n\nModel I (logistic) から計算したJoint Effect：\n\nemmeansを使用: Estimate = 1.6801, SE = 0.3567\n手動計算: Estimate = 1.6801, SE = 0.3567\n\nロジスティック回帰においても、共同効果の対数オッズ比とその標準誤差が完全に一致することが確認できました。\n\n\n\n\n2つの二値曝露を扱う際、4カテゴリーのダミー変数モデルと交互作用モデルは、単なる再パラメータ化の関係にあります。そのため、線形回帰・ロジスティック回帰を問わず、適切に計算すれば共同効果の点推定値と標準誤差は完全に一致します。\n実証結果のポイント：\n\n線形回帰、ロジスティック回帰ともに、ダミー変数モデルを使おうが交互作用モデルを使おうが同じ結果\n点推定値だけでなく標準誤差も小数点以下4桁まで完全に一致\n\nモデルの選択は、研究の目的に応じて行うべきです。\n\nダミー変数モデル: 各曝露パターンの効果を、共通の参照群と比較して直接的に解釈したい場合に直感的です\n交互作用モデル: 主効果からの逸脱、つまり交互作用の大きさとその有意性を直接評価したい場合に適しています\n\nJoint Intervention Exposureにおいてはどちらのモデルを使おうが結果は同じとなる。あくまで解析には興味がなく、"
  },
  {
    "objectID": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html#はじめに",
    "href": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html#はじめに",
    "title": "Joint Interventionにおけるモデル構築",
    "section": "",
    "text": "疫学研究において、2つの二値曝露（A₁, A₂）がアウトカムに与える共同効果（Joint Effect）を評価する場面は頻繁にあります。その際のモデリング戦略として、一般的に以下の2つのアプローチが考えられます。\n\n4カテゴリーのダミー変数モデル: 参照群（例: A₁=0, A₂=0）を基準に、3つのダミー変数を作成する\n交互作用モデル: 主効果（A₁, A₂）と交互作用項（A₁ × A₂）をモデルに投入する\n\n本稿では、まず線形回帰モデルにおける両者の数学的な等価性を詳細に示し、Rを用いた数値シミュレーションでその結果を実証します。さらに、この関係がロジスティック回帰モデルにも拡張できることを確認します。"
  },
  {
    "objectID": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html#線形回帰モデルにおける等価性の証明",
    "href": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html#線形回帰モデルにおける等価性の証明",
    "title": "Joint Interventionにおけるモデル構築",
    "section": "",
    "text": "線形回帰の枠組みでは、2つのモデルが数学的に全く同じものであることを明確に示せます。\n\n\nアウトカムをY、曝露グループごとのサンプル平均を Ȳₐ₁ₐ₂ とします。\n\n\n参照群を (A₁=0, A₂=0) とします。\nE[Y] = β₀ + β₁D₁₀ + β₂D₀₁ + β₃D₁₁\n最小二乗法（OLS）による係数βの推定量は、サンプル平均との差で直感的に表現できます。\n\nβ̂₀ = Ȳ₀₀\nβ̂₁ = Ȳ₁₀ − Ȳ₀₀\nβ̂₂ = Ȳ₀₁ − Ȳ₀₀\nβ̂₃ = Ȳ₁₁ − Ȳ₀₀\n\nここで、β̂₃が参照群に対する共同効果の直接的な推定量となります。\n\n\n\nE[Y] = γ₀ + γ₁A₁ + γ₂A₂ + γ₃(A₁ × A₂)\nこのモデルの係数γも同様にサンプル平均から導出できます。\n\nγ̂₀ = Ȳ₀₀\nγ̂₁ = Ȳ₁₀ − Ȳ₀₀ （A₁の主効果）\nγ̂₂ = Ȳ₀₁ − Ȳ₀₀ （A₂の主効果）\nγ̂₃ = Ȳ₁₁ − Ȳ₁₀ − Ȳ₀₁ + Ȳ₀₀ （交互作用）\n\nModel Iにおける共同効果は、係数の線形和 γ̂₁ + γ̂₂ + γ̂₃ で表されます。これを展開すると、\n(Ȳ₁₀ − Ȳ₀₀) + (Ȳ₀₁ − Ȳ₀₀) + (Ȳ₁₁ − Ȳ₁₀ − Ȳ₀₁ + Ȳ₀₀) = Ȳ₁₁ − Ȳ₀₀\nとなり、これはModel Dのβ̂₃と代数的に完全に一致します。\n\n\n\n\n点推定量の正体が同じ（どちらも Ȳ₁₁ − Ȳ₀₀）であるため、標準誤差も必然的に同じになります。\nVar(β̂₃) = Var(γ̂₁ + γ̂₂ + γ̂₃) = Var(Ȳ₁₁ − Ȳ₀₀) = σ²/n₁₁ + σ²/n₀₀\nしたがって、両モデルは同じ情報を異なるパラメータで表現した**再パラメータ化（re-parameterization）**の関係にあり、導き出される結論は同一です。"
  },
  {
    "objectID": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html#rによる実証線形回帰",
    "href": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html#rによる実証線形回帰",
    "title": "Joint Interventionにおけるモデル構築",
    "section": "",
    "text": "理論的な等価性を、シミュレーションデータを用いてRで確認します。\n# パッケージの読み込み（必要に応じてインストール）\n# install.packages(\"emmeans\")\nlibrary(emmeans)\n\n# 再現性のためのシード設定\nset.seed(123)\n\n# 1. データ生成\nn &lt;- 400\ndf &lt;- data.frame(\n  A1 = rbinom(n, 1, 0.5),\n  A2 = rbinom(n, 1, 0.4)\n)\n\n# Yを生成 (A1=1, A2=1のときに強い効果を持たせる)\ndf$Y_linear &lt;- 10 + 2*df$A1 + 3*df$A2 + 5*df$A1*df$A2 + rnorm(n, 0, 4)\n\n# 2. 4カテゴリーのダミー変数を作成\n# 参照群を A1=0, A2=0 に設定\ndf$group &lt;- factor(paste0(\"A1=\", df$A1, \", A2=\", df$A2))\ndf$group &lt;- relevel(df$group, ref = \"A1=0, A2=0\")\n\n# 3. モデルのフィッティング\n# Model D: ダミー変数モデル\nmodel_D_lm &lt;- lm(Y_linear ~ group, data = df)\ncat(\"--- ダミー変数モデル (Model D) の結果 ---\\n\")\nprint(summary(model_D_lm))\n\n# Model I: 交互作用モデル\nmodel_I_lm &lt;- lm(Y_linear ~ A1 * A2, data = df)\ncat(\"\\n--- 交互作用モデル (Model I) の結果 ---\\n\")\nprint(summary(model_I_lm))\n\n# 4. 共同効果の比較\ncat(\"\\n--- 共同効果の点推定値と標準誤差の比較 ---\\n\")\n\n# Model Dから共同効果を直接取得\nbeta3 &lt;- summary(model_D_lm)$coefficients[\"groupA1=1, A2=1\",]\ncat(sprintf(\"Model D (ダミー) の共同効果: Estimate = %.4f, SE = %.4f\\n\", beta3[1], beta3[2]))\n\n# Model Iから共同効果を計算\n# 方法1: emmeansパッケージを使用\njoint_effect &lt;- emmeans(model_I_lm, ~ A1*A2)\ncontrast_result &lt;- contrast(joint_effect, method = list(\"Joint Effect (1,1 vs 0,0)\" = c(-1, 0, 0, 1)))\ncontrast_summary &lt;- summary(contrast_result)\ncat(sprintf(\"Model I (交互作用) の共同効果: Estimate = %.4f, SE = %.4f (emmeansを使用)\\n\",\n            contrast_summary$estimate[1], contrast_summary$SE[1]))\n\n# 方法2: 手動計算による確認\n# γ1 + γ2 + γ3 = 共同効果\ncoef_I &lt;- summary(model_I_lm)$coefficients\njoint_manual &lt;- coef_I[\"A1\", \"Estimate\"] + coef_I[\"A2\", \"Estimate\"] + coef_I[\"A1:A2\", \"Estimate\"]\ncat(sprintf(\"Model I (交互作用) の共同効果: Estimate = %.4f (手動計算)\\n\", joint_manual))\n\n# 標準誤差の手動計算（共分散行列を使用）\nvcov_matrix &lt;- vcov(model_I_lm)\n# γ1 + γ2 + γ3 の分散を計算\ncontrast_vector &lt;- c(0, 1, 1, 1)  # (Intercept, A1, A2, A1:A2)\njoint_var &lt;- t(contrast_vector) %*% vcov_matrix %*% contrast_vector\njoint_se_manual &lt;- sqrt(joint_var)\ncat(sprintf(\"Model I (交互作用) の共同効果: SE = %.4f (手動計算)\\n\", joint_se_manual))\n\n\n上記のRコードを実行すると、以下の結果が得られます。\nModel D の groupA1=1, A2=1 の係数：\n\nEstimate: 10.1044\nStd. Error: 0.5856\n\nModel I から計算したJoint Effect：\n\nemmeansを使用: Estimate = 10.1044, SE = 0.5856\n手動計算: Estimate = 10.1044, SE = 0.5856\n\nこの結果は、共同効果の点推定値と標準誤差が、小数点以下4桁まで完全に一致することを示しており、理論的な等価性を実証しています。"
  },
  {
    "objectID": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html#ロジスティック回帰モデルへの拡張",
    "href": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html#ロジスティック回帰モデルへの拡張",
    "title": "Joint Interventionにおけるモデル構築",
    "section": "",
    "text": "この等価性の原則は、ロジスティック回帰にもそのまま適用される。再パラメータ化の関係は、この線形予測子の部分で成立するため、共同効果の対数オッズ比とその標準誤差も、2つのモデルで完全に一致します。\n\n\n# 再現性のためのシード設定\nset.seed(456)\n\n# 1. ロジスティック回帰用のデータ生成\nlogit_p &lt;- -2 + 0.5*df$A1 + 0.8*df$A2 + 1.2*df$A1*df$A2\nprob &lt;- exp(logit_p) / (1 + exp(logit_p))\ndf$Y_logistic &lt;- rbinom(n, 1, prob)\n\n# 2. モデルのフィッティング\n# Model D: ダミー変数モデル\nmodel_D_glm &lt;- glm(Y_logistic ~ group, data = df, family = \"binomial\")\n# Model I: 交互作用モデル\nmodel_I_glm &lt;- glm(Y_logistic ~ A1 * A2, data = df, family = \"binomial\")\n\n# 3. 共同効果（対数オッズ比）の比較\ncat(\"\\n--- ロジスティック回帰における共同効果の比較 ---\\n\")\n\n# Model Dから直接取得\nbeta3_glm &lt;- summary(model_D_glm)$coefficients[\"groupA1=1, A2=1\",]\ncat(sprintf(\"Model D (ダミー) の共同効果(LOR): Estimate = %.4f, SE = %.4f\\n\", beta3_glm[1], beta3_glm[2]))\n\n# Model Iから計算\njoint_effect_glm &lt;- emmeans(model_I_glm, ~ A1*A2)\ncontrast_glm &lt;- contrast(joint_effect_glm, method = list(\"Joint Effect (1,1 vs 0,0)\" = c(-1, 0, 0, 1)))\ncontrast_glm_summary &lt;- summary(contrast_glm)\ncat(sprintf(\"Model I (交互作用) の共同効果(LOR): Estimate = %.4f, SE = %.4f (emmeansを使用)\\n\",\n            contrast_glm_summary$estimate[1], contrast_glm_summary$SE[1]))\n\n# 手動計算による確認\ncoef_glm &lt;- summary(model_I_glm)$coefficients\njoint_manual_glm &lt;- coef_glm[\"A1\", \"Estimate\"] + coef_glm[\"A2\", \"Estimate\"] + coef_glm[\"A1:A2\", \"Estimate\"]\ncat(sprintf(\"Model I (交互作用) の共同効果(LOR): Estimate = %.4f (手動計算)\\n\", joint_manual_glm))\n\n# 標準誤差の手動計算\nvcov_matrix_glm &lt;- vcov(model_I_glm)\ncontrast_vector &lt;- c(0, 1, 1, 1)  # (Intercept, A1, A2, A1:A2)\njoint_var_glm &lt;- t(contrast_vector) %*% vcov_matrix_glm %*% contrast_vector\njoint_se_manual_glm &lt;- sqrt(joint_var_glm)\ncat(sprintf(\"Model I (交互作用) の共同効果(LOR): SE = %.4f (手動計算)\\n\", joint_se_manual_glm))\n\n\n\nModel D (logistic) の groupA1=1, A2=1 の係数：\n\nEstimate: 1.6801\nStd. Error: 0.3567\n\nModel I (logistic) から計算したJoint Effect：\n\nemmeansを使用: Estimate = 1.6801, SE = 0.3567\n手動計算: Estimate = 1.6801, SE = 0.3567\n\nロジスティック回帰においても、共同効果の対数オッズ比とその標準誤差が完全に一致することが確認できました。"
  },
  {
    "objectID": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html#結論",
    "href": "posts/statistics/2025/Joint_interventionにおけるモデル構築.html#結論",
    "title": "Joint Interventionにおけるモデル構築",
    "section": "",
    "text": "2つの二値曝露を扱う際、4カテゴリーのダミー変数モデルと交互作用モデルは、単なる再パラメータ化の関係にあります。そのため、線形回帰・ロジスティック回帰を問わず、適切に計算すれば共同効果の点推定値と標準誤差は完全に一致します。\n実証結果のポイント：\n\n線形回帰、ロジスティック回帰ともに、ダミー変数モデルを使おうが交互作用モデルを使おうが同じ結果\n点推定値だけでなく標準誤差も小数点以下4桁まで完全に一致\n\nモデルの選択は、研究の目的に応じて行うべきです。\n\nダミー変数モデル: 各曝露パターンの効果を、共通の参照群と比較して直接的に解釈したい場合に直感的です\n交互作用モデル: 主効果からの逸脱、つまり交互作用の大きさとその有意性を直接評価したい場合に適しています\n\nJoint Intervention Exposureにおいてはどちらのモデルを使おうが結果は同じとなる。あくまで解析には興味がなく、"
  },
  {
    "objectID": "posts/statistics/2025/github.html",
    "href": "posts/statistics/2025/github.html",
    "title": "1 githubのブログ更新手順について",
    "section": "",
    "text": "GitHubでブログを更新する際の標準的な手順は以下の通りです。\n\n\nquarto render\ngit add .\ngit commit -m \"新記事追加/編集\"\ngit push origin main\n\n\n\nなお、quartoが多くなってきた場合、特定のファイルのみでrenderすることも可能である。\nquarto render\nquarto render post.qmd\n\n\n\ngit add .\n\n\n\ngit commit -m \"コミットメッセージ\"\n\n\n\ngit push origin main"
  },
  {
    "objectID": "posts/statistics/2025/github.html#一括実行",
    "href": "posts/statistics/2025/github.html#一括実行",
    "title": "1 githubのブログ更新手順について",
    "section": "",
    "text": "quarto render\ngit add .\ngit commit -m \"新記事追加/編集\"\ngit push origin main"
  },
  {
    "objectID": "posts/statistics/2025/github.html#quartoサイトをレンダリング",
    "href": "posts/statistics/2025/github.html#quartoサイトをレンダリング",
    "title": "1 githubのブログ更新手順について",
    "section": "",
    "text": "なお、quartoが多くなってきた場合、特定のファイルのみでrenderすることも可能である。\nquarto render\nquarto render post.qmd"
  },
  {
    "objectID": "posts/statistics/2025/github.html#変更内容をステージングエリアに追加",
    "href": "posts/statistics/2025/github.html#変更内容をステージングエリアに追加",
    "title": "1 githubのブログ更新手順について",
    "section": "",
    "text": "git add ."
  },
  {
    "objectID": "posts/statistics/2025/github.html#変更をコミット",
    "href": "posts/statistics/2025/github.html#変更をコミット",
    "title": "1 githubのブログ更新手順について",
    "section": "",
    "text": "git commit -m \"コミットメッセージ\""
  },
  {
    "objectID": "posts/statistics/2025/github.html#リモートリポジトリにプッシュ",
    "href": "posts/statistics/2025/github.html#リモートリポジトリにプッシュ",
    "title": "1 githubのブログ更新手順について",
    "section": "",
    "text": "git push origin main"
  },
  {
    "objectID": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html",
    "href": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html",
    "title": "抗がん剤試験におけるEstimandを考える",
    "section": "",
    "text": "Estimand は、治療、対象集団、変数、要約指標、中間事象（治療開始後に起こり測定値の存在・解釈に影響する事象）の取り扱い、という5つの要素で関心のある治療効果を明確に設定する枠組みです。従来の PICO ではレスキュー治療の開始など中間事象を治療の一部と見なし、治療効果を評価することが一般的でした。これに対し Estimand の枠組みで中間事象の取り扱いを試験計画時に整理することで、例えば「ある中間事象が起こらなかった場合の治療効果」という特定の臨床的問いに対応した治療効果を検証する試験の計画が可能です。この枠組みを試験に実装するには、医師、CRC、PM/StM、DM、MOなど多職種の協働が不可欠です。試験目的に沿った Estimand を定めるには、まず関心のある治療条件を整理し、関係者間で認識を共有します。続いて、合意した Estimand に基づき、当該治療効果の推定に必要なデータの収集可否と方法を多職種で検討します。こうした議論を行うことで収集項目の優先順位が明確になり、重要なデータが適切に収集され、試験の品質向上に繋がります。本発表では、Estimand が変わるとデータ収集戦略がどのように変化するか事例を通じて検討します。\n\n\n\nURL：https://jcog.jp/doctor/tool/manual/index.html\n\n\n\n\n試験の対象集団を規定する上でのstage や疾患の程度・拡がりを診断する規準を記載する。\n原則として、患者選択（適格規準）や割付調整因子、治療前評価項目に関係する規準や定義が該当する。\n適格規準と除外規準に分けて、選択規準を規定する。 試験の結果、治療法の有効性が示された場合にその治療を適用することが妥当とみなせる対象集団を規定するものが適格規準（inclusion criteria）であり、外的妥当性（external validity）すなわち一般化可能性 （generalizability）に関連する。一方、適格規準で示される対象集団には属するが、治療のリスクが高いために試験に組み入れることが倫理的でないか（倫理的側面）、試験で必要な有効性・安全性の評価に影響を及ぼすと判断される対象を除外 する（科学的側面）条件を規定するものが除外規準（exclusion criteria）であり、科学的側面としては内的妥当 性（internal validity）すなわち比較可能性（comparability）に関係する\n狭すぎる選択規準の試験結果は特定の患者集団にしか適用できないものとなる（一般化可能性が低い）し、 逆に広すぎると治療効果が期待できない患者が多く含まれることとなって治療効果の差が薄まってしまう（内的妥当性が低い）。試験の目的である治療効果の評価に適切な集団を選択する適格規準を設定しなければ ならない\n一般化可能性/外的妥当性について、「特定の施設に限って試験を行った場合、その結果は日本全体には外挿できないことから標準治療とは言えないのではないか」といった質問を受けることがある。例えば、手術手技の試験において、技術認定を受けた外科医に術者を限るといった場合が挙げられる。こうした制限を設けることは、まだ一般には普及していない難度の高い新しい技術を要する実験的治療に伴う患者リスクを最小化するという倫理的側面が第一義であるが、その新しい技術が有効性で真に優れたものである時に、技術が未熟な術者が加わることでその技術の真の有効性が正しく評価されないという科学的側面（比較可能性/内的妥当性）もある。\n\n\n\n\n\nプロトコール治療\n\n試験で評価する「プロトコール治療」の定義と全体像を説明した上で、個々の治療内容を群別（比較試験の場合）・モダリティ別に詳述する。比較試験では群別の記載を基本とし、放射線治療や手術等、群で共通の治療がある場合には「両群共通」として記載してもよい。\n特に複数のレジメンや複数のモダリティによる治療レジメンの場合、「プロトコール治療」の定義を明確に行う。 後治療との区別も明確に定義する。\n複数コースからなる治療レジメンの場合、何コースをもって「プロトコール治療完了」とするかを明記する。\n効果や毒性などによってコース数や次に進むレジメンが異なるような場合は、その判断規準を明確に示す。 登録後に治療を開始するまでの期間の上限を規定する。入院治療の場合は「登録後7 日以内」（登録日の翌 週の同じ曜日まで）、外来治療の場合は「登録後14 日以内」を原則とする。ただし、手術や放射線治療がプロ トコール治療に含まれる場合は、手術室予約や放射線治療計画に時間を要するため、「登録後21 日以内」 や「登録後28 日以内」なども許容される。\n原則として「コース開始規準」は第2 コース以降に適用し、第1コースの開始に際してはコース開始規準や適格規準は適用しない。\n「登録時に適格規準を満たしたが治療開始前に検査値が適格規準を満たさなくなった」という場合、治療を開始してもプロトコール逸脱/違反とはならない。そのため、登録後の治療開始までの期間は十分に短く決める 必要がある。十分短く設定しても、治療開始前に臓器機能の検査値が悪化して担当医判断により治療を開始 せず「プロトコール治療中止」となる場合もあり得るが、それが頻発するようなら適格規準を再検討する必要がある（ごく少数例生じるのは問題とならない）。なおSouthwest Oncology Group（SWOG）においては、「登録当日または登録翌日」に治療を開始しなければならないとしている。つまり治療開始予定日が登録当日か翌日でないと登録ができない。\nプロトコール治療として、使用が規定されるすべての薬剤（抗がん薬、支持療法薬）を記載すること。薬剤名は一般名（一般的名称）を記載する。ただし、用いる薬剤の剤形（錠剤と顆粒剤など）に特別な規定を 設ける場合は、その旨を記載すること。 後発医薬品（ジェネリック医薬品）の使用の可否については各医療機関の方針によるため、JCOGとしては原 則として後発医薬品の使用を制限しない。制限を加える必要がある場合には本章に記載する。\n\n化学治療\n\nランダム化試験の場合、群毎に分けて記載する。\n治療レジメンについて、薬剤名、投与量、投与法、投与日を明記する。\nコースの表現は「○週1 コースとして×コース行う」を標準とする。 （薬剤投与が1 週間、3 週1 コースのレジメンの場合、「3 週間隔で×コース」という表現に対して、4 週1 コー スと解釈したための系統的逸脱の事例がある）\n体表面積から実投与量を計算する際の、まるめ（切り上げ/切り捨て/四捨五入）の方法を明記する。同じグル ープの複数の試験でまるめの方法が異なることはミスの元となるため、切り捨てを標準とし、疾患や薬剤によ って切り上げや四捨五入が適切な場合は許容する。同一の薬剤で異なる剤形（注射薬と内服薬など）が混在 する場合は各々について明示する。 ・\n治療開始後の体重変動による投与量補正（再計算）を行うかどうかを明記する。行う場合はその方法を明記 する。体重が増加した時も減少した時も再計算を行うとする規定（±〇kg または±〇%で規定）、体重減少の 時のみ再計算を行うとする規定のいずれも可である。kg と%のどちらで規定してもよい。\n\n放射線治療\n\n放射線治療の開始時期、予定休止期間の有無、祝日などの扱いについて記載する。\n\n外科的切除術\n\n手術術式の特定、切除範囲、再建術式など、必須とされる手技や許容範囲とする手技を明確に記述する。図 示が望ましい。\n\nプロトコール治療完了基準\n\nプロトコール治療完了とみなす治療内容やコース数、原病の増悪・再発、治療中止とすべき毒性（有害事象）、 コース開始延期の許容範囲またはプロトコール治療期間全体の延長許容範囲などの判断規準を記述する。\nプロトコール治療中止理由の分類の基本は以下のとおりであるが、後述するようにプロトコール毎に詳細な表現に変更して細分類を付加することは、治療中止規準を明確にする上でも推奨される。\n\n① 治療完了：プロトコール規定の治療完了\n② 増悪/再発：原病の増悪・再発による治療中止。増悪・再発以外の状況での無効中止を設ける 場合はここに分類し、「②増悪/再発/無効」としてよい。\n③ 有害事象：担当医判断または中止規定に従った、有害事象による治療中止\n④ 拒否（有害事象）：有害事象との因果関係がある患者拒否による治療中止\n⑤ 拒否（その他）：有害事象との因果関係がない患者拒否による治療中止\n⑥ 死亡：プロトコール治療中（投与間の観察期間を含む）の死亡（治療との因果関係を問わない）\n⑦ その他：①～⑥以外の理由による治療中止 ・\n\nコースや評価期間が規定される試験の場合は、その規定コースまで治療が継続されたものを「①治療完了」 とし、規定の最終コースの治療を完了する前に増悪のため治療が中止されたものを「②増悪/再発」に分類す ることとする。その際、評価期間を明確に示すこと。\n増悪まで治療を続け、かつ「評価期間は○コースまで」と規定されない試験の場合、①と②が分離不能のた め、「①治療完了」の分類は用いない。 ・\nRECIST に従う効果判定は、他の試験との奏効割合の比較可能性を確保するために行われることから、 個々の患者の治療継続の是非の決定をRECIST に従って判定した「総合効果」に基づいて決めることは必ずしも適切ではない。従って「無効中止」の規定をCR/PR/SD/PD を用いて行うことは許容されない。\n患者拒否による中止の場合、その理由が毒性（有害事象）との因果関係があると判断される場合と、毒性と の因果関係がないと判断される場合を区別する。有害事象との因果関係あり/なしの判断は治療との因果関 係あり/なしの判断に準じて、「7.2.2. 有害事象と治療との因果関係の判定」に示した、“definite、probable、 possible”のいずれかと判断された場合は「因果関係あり」とし、“unlikely、unrelated”のいずれかと判断され た場合は「因果関係なし」とする。“possible”と“unlikely”の区別については、“reasonable possibility”の考え 方を重視し、「どちらによると考えるのがよりもっともらしいか」によって判断する、すなわち有害事象により患 者が中止を希望したと考える方がもっともらしければ“possible”、有害事象以外の要因により患者が中止を希 望したと考える方がもっともらしければ“unlikely”と判断する。\n複数のレジメンやモダリティを組み合わせたプロトコール治療では、一次治療レジメン中止の後に二次治療レジメンを行う場合もあり、「一次治療中止」、「二次治療中止」、「プロトコール治療中止」の関係が複雑となるた め注意する。次の治療レジメンへの移行の規準を明確にする。プロトコール治療が二次治療までの場合は、 「一次治療中止・完了規準」と「プロトコール治療（全治療）中止・完了規準」を別に設け、三次治療までの場合 は「一次」、「二次」、「プロトコール治療」の3 つを設けることを推奨する。\nプロトコール治療が複数のレジメンや複数のモダリティの逐次的な組み合わせである場合、それぞれのレジメンについて「完了」の定義、「中止」の規準を設けることを推奨する。\n増悪中止、毒性中止、患者拒否中止までプロトコール治療を継続する治療レジメンの場合、完了はないため、 以下のように記述する。\n\nプロトコール治療中止の基準\n\n以下のいずれかの場合、プロトコール治療を中止する。\n1） プロトコール治療無効と判断\n\n-   注）以下の②を設けない場合は「1）治療開始後に原病の増悪が認められた場合」とする。\n\n    -   ① 治療開始後に原病の増悪が認められた場合 ※画像による効果判定でPD と判定されても臨床的にプロトコール治療継続が妥当と判断される場合には原病の増悪とはせず、プロトコール治療を継続する。\n\n    -    ② ○コース終了時点までに腫瘍の縮小や症状の改善がみられない場合\n\n        -   注） 治療継続の可否を決める「治療無効」かどうかの判断は総合的な臨床判断で行う。画像に よる総合効果（CR/PR/SD/PD）はあくまでも参考とする。実際には腫瘍が縮小していても総 合効果はPD となり得るし、腫瘍が増大していても総合効果はPR となり得る。総合効果が PR の時に臨床的には無効と判断してプロトコール治療中止とすることが妥当である場合も あるし、総合効果がPD であっても治療が有効と判断して治療継続が妥当である場合もあ る。\n\n2) 有害事象によりプロトコール治療が継続できない場合\n\n① Grade 4 の非血液毒性が認められた場合 （非血液毒性：CTCAE v5.0-JCOG における「貧血」「骨髄細胞減少」「リンパ球数減少」「好中球数 減少」「白血球減少」「血小板数減少」「CD4 リンパ球減少」以外の有害事象）\n② 有害事象により次コース開始が○週間遅延した場合\n③ 治療変更規準（6.3.）でのプロトコール治療中止の規定に該当した場合\n④ 治療変更規準以外で、有害事象により、担当医がプロトコール治療中止を要すると判断した場合\n\n3） 有害事象との因果関係がある理由により、患者がプロトコール治療の中止を申し出た場合\n\n有害事象との因果関係がある（definite, probable, possible）と判断される理由の場合はこの分類 を用いる\n\n4） 有害事象との因果関係がない理由により、患者がプロトコール治療の中止を申し出た場合\n\n有害事象との因果関係がない（unlikely, unrelated）と判断される理由の場合はこの分類を用いる\n登録後、プロトコール治療開始前の患者拒否の場合 ・ プロトコール治療中の本人や家人の転居など、有害事象との因果関係がまず否定できる場合\n\n5） プロトコール治療中の死亡 ・ 他の理由によりプロトコール治療中止と判断する以前の死亡\n6） その他、登録後治療開始前の増悪（急速な増悪によりプロトコール治療が開始できなかった）、プロトコ ール違反が判明、登録後の病理診断変更などにより不適格性が判明して治療を変更、社会的理由や 安全管理上の問題によりプロトコール治療の継続が困難と判断された場合など\n\n治療変更基準\n\n毒性の種類、程度（Grade や検査値）毎に、研究者による解釈の違いが生じないよう変更規準を明確に規定 する。\n\n適切な章構成は試験によって異なるが、例として以下の章構成が考えられる。\n\n6.3.1. 用量レベル\n6.3.2. コース開始規準\n6.3.3. コース内の休止/再開規準\n6.3.4. 減量規準\n\n\n用語の一貫性について\n\nhold/suspend/halt：いったん中止して条件が揃えば再開（discontinue temporally）\nterminate：再開しない途中中止＝終了（discontinue permanently）\nskip：その時のみ投与しない complete：予定どおりすべて投与して終了＝完了 など\n\n使い分けが比較的容易だが、日本語では「中止」がさまざまな意味を有するため十分注意して記述する こと。下記の定義による「延期」「中止」「休薬」「休止」「スキップ」を用いることを推奨する。\n延期 delay\n\n投与間隔の延長、投与を規定より遅らせること。 延期可能な期間を明記すること\n\n中止 terminate\n\n治療全体または特定の薬剤やモダリティの永久的・継続的取りやめ。再開しない。\n例： 投与開始予定日より3 週を越えても次コース開始規準を満たさない場合、プロトコール治療中止と する。\n\n休薬 hold/suspend/halt\n\n-   治療薬の1 剤以上をいったん休み、再開する条件が揃うのを待つこと。 薬剤単位で規定する時に用いる。\n\n休止 hold/suspend 治療全体または特定のモダリティをいったん休み、再開する条件が揃うのを待つこと。\n\n-   条件が満たされれば再開する。モダリティ単位で用いる。\n\n-   休止の場合、再開する際には休止した時点で予定されていた治療を再開する。\n\n-   後述の「スキップ」 では予定していた治療の一部を行うことなく次（コース）に進む。\n\n    -   例： 放射線治療中、WBC＜2,000 mm3 を認めた場合は放射線治療を休止し、WBC≧2,000 mm3を確 認した後に放射線治療を再開する。 ・\n\nスキップ skip\n\n治療の一部以上を実施せず次の投与スケジュールに進むこと。\n\n\n\n治療変更規準作成のヒント\n\n治療変更規準はできる限りシンプルで明確、かつ臨床的に妥当なものでなければならないが、実際には、薬 剤の特性、治療レジメンの特徴、認可された用量・用法など、考慮すべきパラメータが非常に多く、プロトコー ル作成の中でももっとも難しい部分と言える。\n治療減量基準\n\n用量の変更（減量）\n\n減量規準を文章で表現すると、「何に対してどれくらい減量するか」を明確にすることは困難であり、 初回投与量に対して○%に減量、前コース投与量に対して○%に減量、前回（直近）の投与量（同 じコースでの前回投与を含む）に対して○%に減量、何回まで減量を行うか、等が不明確になりやすいことから、JCOGでは後述する例のように「用量レベル」を設定する減量規準とすることを推奨 する。\n\n毒性回復後の再投与や増量の可否\n\n「回復」とする定義を明確にする（例：Grade 0 に回復。治療前PaO2 －10 torr 以上に回復）。また、 再開時に「減量した投与量」を継続するのか、「減量前の投与量に戻す（再増量）」のかを明確にす る。\n\n減量後にも規定の毒性が持続または再出現する場合の投与量\n\n-    「さらに減量を行う」のか「それ以上の減量を行わず中止する」のかを明確にする。\n\n次コース開始条件・投与可能条件\n\n-   他の治療変更（減量・延期）規準、適格規準との整合性を十分検討の上で用いる。その際、開始規 準を満たさない場合の対応を明確にする。\n\n体重変動による投与量変更 コース・投与毎に体重変動により投与量変更を行うのか、体重変動によらず初回投与量を続ける のかを明記する。特定の条件下でのみ体重変動による投与量変更を行う場合、その条件を明記 すること。体重変動による投与量変更は「増量」「減量」とは呼ばない。\n\n用量レベル\nコース開始基準\n\n次コースを開始しても安全と思われる程度に臓器機能が回復していることを確認するための指標であり、開 始当日またはその前日（または3 日前など）までに満たすべき臨床検査値の値などとして決定する。\nコース開始規準（や休止規準）は、プロトコール治療との因果関係を問わない有害事象として規定する。例え ば、「発熱」があれば、プロトコール治療とは関係がない「感冒」によるものであったとしても、抗がん治療は延 期することが妥当であると考えられるためである。\n外来治療の場合、当日の検査値のうち、血算は検査値を確認してから投与の可否や減量の有無を決定でき ても、生化学検査は投与時までに検査結果が判明しないこともあるため、当日（や直近）の検査値を用いて規 定することが不適切な場合がある。この点を考慮して規定すること。\nコース開始規準は、第1 コースには適用されないことを明記すること\n\n減量中止基準\n\n基本的には、前コースで観察された有害事象がある条件を満たす場合に次のコースの投与量を減量するための規準。すべての薬剤を減量する場合と特定の薬剤のみ減量/中止する場合がある。\nコース開始規準と異なり、減量/中止規準は、プロトコール治療との因果関係がある「有害反応」として規定す るべきである。例えば、前コースでGrade 3 の下痢が見られていたとしても、それが、他の原因が明らかな食 中毒のようなものであれば、減量する必要はないと考えられるためである。\n神経毒性を治療変更規準に含める試験での注意点 CTCAE v5.0 で、末梢性運動ニューロパチーと末梢性感覚ニューロパチーは以下のように定義される。\n\n併用療法・支持療法\n\nプロトコール治療期間中の併用療法・支持療法について、\n\n「規定とする」\n「推奨される」\n「許容される」\n「推奨 されない」\n「許容されない」\n\nの区分毎に記載する。用量や用法についての条件がある場合もその旨を明記す ること。\n\n後治療\n\nプロトコール治療中止/終了後の他の治療（プロトコール治療と同一の治療の全部または一部の反復を含む） に対する制限を記載する。\nランダム化試験の場合、プロトコール治療中止/終了後に、もう片方の群の治療を行ってもよい（クロスオーバ ー）のかどうかを必ず記載すること。\n主たる解析や中間解析で、いずれかの治療群が良いと結論された場合、試験の結果を説明し、それぞれの 患者の治療歴を考慮の上、最良と考えられる治療法を提供する旨を記載する。\nまた、従来JCOG 試験においても、毒性による中止規準に該当したり患者拒否により「プロトコール治療中止」 とした後、さらにプロトコール治療と同じレジメンを「後治療」として継続している例が多くみられたが、それは 推奨されない。理由は、同じ治療レジメンであれば、担当医が「後治療」と主張したとしても、それにより生じた 有害事象はプロトコール治療の安全性評価に含めるべきであるためである。「中止規定に該当したが担当医 判断や患者の希望で同じ治療を継続」した場合は、「プロトコール治療中止→後治療」ではなく「中止規定を逸 脱した上でのプロトコール治療継続」とする。これを「後治療」として評価の枠外に置くことを許容すれば、例え ば「7 コースで都合の悪い（逸脱に引き続いて生じた）重篤な有害事象が生じたため、遡って6 コースでプロト コール治療中止として後は後治療と扱う」と言った恣意的な過小評価が可能になってしまう。\n特に、化学療法に続いて放射線治療を行うような集学的治療レジメンの場合、化学療法中止例における事後 の放射線治療をプロトコール治療の一部と扱う（毒性評価データを収集する）のか、後治療と扱う（毒性評価 データを収集しない）のかを明確に区別すること。\n主たる解析や中間解析等により試験の主たる結論が判明した場合、必要に応じて試験に登録された患者 に試験の結果を説明し、個々の患者の治療経過を考慮の上、最良と考えられる治療法を提供する。\nプロトコール治療が手術単独治療などで、「プロトコール治療中止規準には該当するが、臨床的にはプロトコ ール治療継続が妥当と判断される場合」が生じ得ない場合、以下の記載は削除してよい。\n臨床研究法に従って実施する試験ではこちらを使用する。\n\n-   また、プロトコール治療中止規準には該当するが、臨床的には「プロトコール治療継続」が妥当と判断され る場合は、原則として（時間的余裕がない場合を除いて）、担当医レベルで決定するのではなく、研究責任医 師を通じて研究事務局に相談すること。研究事務局と研究責任医師の合意の下に、「プロトコール治療中止 →後治療として治療」か、「逸脱してプロトコール治療継続」かを決定する。研究事務局との相談内容および意 思決定の経緯は、当該患者の治療終了報告や経過記録のコメント欄に詳細を入力すること。なお、「逸脱して プロトコール治療継続」が頻発する場合は、プロトコール治療中止規準が臨床的に不適切である可能性があ るため、研究事務局はグループ会議やグループメーリングリストを利用してプロトコール治療中止規準の見 直しについて検討する。\n\n人を対象とする生命科学・医学系研究に関する倫理指針に従って実施する試験ではこちらを使用する。\n\nまた、プロトコール治療中止規準には該当するが、臨床的には「プロトコール治療継続」が妥当と判断され る場合は、原則として（時間的余裕がない場合を除いて）、担当医レベルで決定するのではなく、施設研究責 任者または施設コーディネーターを通じて研究事務局に相談すること。施設研究責任者・施設コーディネータ ーの合意の下に、「プロトコール治療中止→後治療として治療」か、「逸脱してプロトコール治療継続」かを決 定する。研究事務局との相談内容および意思決定の経緯は、当該患者の治療終了報告や経過記録のコメン ト欄に詳細を入力すること。なお、「逸脱してプロトコール治療継続」が頻発する場合は、プロトコール治療中 止規準が臨床的に不適切である可能性があるため、研究事務局はグループ会議やグループメーリングリスト を利用してプロトコール治療中止規準の見直しについて検討する。\n\n\n予期される有害事象\n\n本試験において予期される有害反応は以下のとおり。\n\n併用化学療法の場合の薬物有害反応、外科手術・放射線治療の有害反応について記載する。第III 相試験 の場合は試験治療群だけでなく標準治療群についても予期される有害反応を記述する。 ・ 複数のモダリティからなるレジメンの場合、それぞれのモダリティ別に記述した上で、併用することによって増 強される可能性がある有害反応について特に注意して詳述する。頻度は文章で羅列するよりも表で簡潔にま とめることが推奨される。文献や添付文書により頻度が数値として判っている場合には数値を記述し、そうで ない場合には「しばしば」「まれに」などで記述する。 ・ 「重篤な有害反応」が予期される場合にその頻度が予期されたレベルよりも増えている時、研究代表者から 効果・安全性評価委員会への報告が必要となるため、可能な限りその頻度を数値で示しておくこと。\nなお、CTCAE では、「有害事象（Adverse Event）」とは、「治療や処置に際して観察される、あらゆる好ましく ない意図しない徴候（臨床検査値の異常も含む）、症状、疾患であり、治療や処置との因果関係は問わない。 すなわち因果関係があると判断されるものと、因果関係ありと判断されないもの両者を含む。」である。\n従って、「明らかに原疾患（がん）による」ものであっても、試験治療（プロトコール治療）本体ではなく支持療法 や併用療法により生じたと思われるものであっても、やはり「有害事象」である。\nしかし、がんの臨床試験においては、多くの場合「死亡」まで追跡がされることから、最終的には多くの登録患者において「原疾患（がん）による有害事象」が多数観察されることになり、追跡期間中の「有害事象」データ をすべて一律に収集することは現実的ではないし意味がない。\nそこで、JCOG では、有害事象データの収集ポリシーとして以下の原則を設ける。\n\n①プロトコール治療の最終治療日から30 日以内の有害事象は、因果関係によらずすべて収集する（有害事 象報告に際しては、有害事象のgrading とは別に「因果関係」が検討される）。\n②プロトコール治療の最終治療日から31 日以降の有害事象は、プロトコール治療との因果関係があり （definite, probable, possible のいずれか）と判断されるもののみ（＝有害反応・薬物有害反応）を収集する。\n\n\n\n評価項目・臨床検査・評価スケジュール\n\n原則として、「登録前」、「治療期間中」、「治療終了後」の3 つの時期別に、検査項目と頻度（間隔）を明記する。\n\n登録前評価項目\n\n登録前に必要な評価項目を列記する。\n検査日の規定については登録日より遡って何日以内までの検査を許容するかを明記すること。\n「日」で規定 するが、7 日、14 日、28 日など、週単位の規定と一致する方が望ましい。\n「登録前7 日以内」は、1 週前の登 録日と同一曜日までを含むこととする。 ・ 画像診断や内視鏡検査などの期限は、臨床的な適切さや実施可能性を考慮して決定する。JCOG の標準は 登録日を含まない28 日以内である。\nつまり登録日をday 0 としてday -28 までを意味する。ただし、進行が 遅いがん種が対象の場合はもっと長い期限を設定することもあり得る。\n\n評価期間の定義\n\n原病の増悪までプロトコール治療を継続する試験では、有害事象や治療経過を密に収集する「観察期間」と 安全性情報のみを取集する「患者追跡期間」とを定義すること。\n「観察期間」や「患者追跡期間」を定める場合は、データ収集の項目や頻度を両者で変更してもよい。\n評価期間の定義を定める場合は、以下を用いる。 ・\n観察期間は、6 か月相当（4 週×6＝24 週、3 週×9＝27 週）を基本とする。\n\n治療期間中の検査と評価\n\n治療中の毒性評価、有効性評価に必要な臨床評価項目、臨床検査、画像検査を検査間隔毎に記載する。\n検査項目別にまとめるよりも頻度や検査時期毎にまとめることを推奨する。\n観察期間を設ける試験では、患者追跡期間中は追跡調査にて有害事象を収集する。\n\n\n\n\n治療終了後の検査と評価項目\n\nプロトコール治療終了/中止後の追跡期間における評価項目や臨床検査を頻度と共に記載する。\n観察期間を設ける試験では、患者追跡期間中かつプロトコール治療終了日から30 日以内に発生した有害事 象は追跡調査に記載する。\n比較試験の場合、原則として群間で評価間隔に差が生じないように注意すること。\n放射線治療を含むレジメンの試験や、注意すべき晩期毒性を有する抗がん薬を用いている試験においては、 それらの晩期毒性が適切に評価されるように評価項目を決定すること。特に放射線関連の有害事象は「治療 開始から90 日以内」の急性毒性と「91 日以降」の遅発性反応に区別して評価されるため、期間の区分のし かたに注意すること。\n外科的切除術を含むレジメンの場合は、7 章で定義した期間の区分を用いること。\n\n治療終了後の安全性評価\n\n有害事象\n\n治療終了後は、以下の項目を評価する。\n\n治療終了後～X 年：○か月ごと\nX 年～XX 年：○か月ごと\n\n\n後治療に関する情報\n\n後治療について、以下の項目を評価する。 ・\n\n後治療の有無\n後治療の内容（後治療を行った場合）\nプロトコール治療中止後最初の後治療開始日（後治療を行った場合）\n後治療開始時のPS（後治療を行った場合）\n\n\n\n治療終了後の有効性評価\n\n画像検査について、検査内容、評価間隔について定めること。増悪/再発を認めた場合には、増悪/再発時 の全身状態（PS）、増悪/再発形式などを記録する。増悪/再発後も評価を継続するかどうかについても定める。\n効果判定のための評価は、「11.1.5. 腫瘍縮小効果の判定」と同様に、「治療開始」を起点として規定する。\n\n追跡調査\n\n上記「治療終了後の安全性評価」および「治療終了後の有効性評価」は、追跡調査時にデータ収集が行われ る。\n施設における生存確認、増悪（または再発）の有無の確認方法について記載する。\n\n![](images/paste-1.png){width=\"605\"}\n\n\n\n\n\n効果判定\n\n術後補助化学療法の試験などで、効果判定を行わない場合 「本試験では効果判定は行わない。」と記載する。\n固形がんの腫瘍縮小効果判定は原則としてNew response evaluation criteria in solid tumours［Revised RECIST guideline （version 1.1）］に従って行う\nRECISTv1.0 原著論文には、「治療継続の決定を目的とした使用は本ガイドラインの主旨ではない」と明記 されており、同様の記載はRECISTv1.1 にも引き続き下記のように明記されている。 「腫瘍専門医の多くは、日常診療で悪性腫瘍患者の経過観察のための画像検査による客観的な規準と、症状に基づく規準の双方に基づいて、治療継続の是非についての意思決定を行っている が、本改訂ガイドラインは、治療を担当する腫瘍医が適切であると判断する場合を除いて、このよ うな個々の患者における治療継続の是非についての意思決定に用いられることを意図していな い。」\n従って、RECIST ガイドラインに基づく効果判定によって決定される「総合効果」は、「薬剤またはレジメンが 開発研究を続けるに値する有望な結果を示すかどうかの判断に用いられる」べきものである。すなわち、 個々の患者における治療継続の是非の判断は、総合効果のCR/PR/SD/PD に基づいて行うのではなく、画像所見に加えて、症状や身体所見、各種検査値等を総合的に加味して行う「臨床的判断」に基づくべきである。\nそのため、画像診断に基づく効果判定による総合効果としての「PD（Progressive Disease：進行）」と判断 した時点でも、臨床的にはプロトコール治療継続が適切な場合が存在する。この場合には効果判定結果によ らず臨床的判断によってプロトコール治療継続の是非を判断すべきではあるが、無増悪生存期間のイベント 日としては総合効果PD と判断した日を採用する。これは、（i）群毎にプロトコール治療を継続すべきかどうか の判断が異なりうること、（ii）RECISTは奏効割合のみならず、無増悪生存期間の標準化をも意図した規準で あること、（iii）米国のCooperative Group の標準的な定義は総合効果がPD であれば、いかなる理由であっ ても無増悪生存期間のイベントとしていること、の3 点の理由による。\n一方、画像診断に基づく効果判定規準での「PD」には該当しなくても、画像診断によらない臨床的・総合的 な判断により「臨床的増悪」と判断した場合は、「6.2.2.プロトコール治療中止規準」に従って、プロトコール治 療を中止すべきである。「臨床的増悪」と判断された場合には効果判定で「PD」と判定されていなくとも、「臨床 的増悪」と判断された日をもって無増悪生存期間のイベントとする。これは、「臨床的増悪」と判断された後の 画像検査がしばしば予定どおりに行われないため、「臨床的増悪」をもって無増悪生存期間のイベントとしな ければ、結果的に無増悪生存期間が過大評価されるリスクが大きいからである。なお、「臨床的増悪」をもっ て無増悪生存期間の「打ち切り」と扱うことも、増悪や死亡のリスクの高い患者を打ち切りにすることになるた め（informative censoring）統計学的に正しくない。\nなお、RECISTv1.1 原著論文では、非標的病変のPD 規準の中に「明らかな増悪（unequivocal progression）」とは「全体の腫瘍量の増加として治療を中止するに十分値する程度の非標的病変の著しい増 悪」と記載されていることから、非標的病変のPD 判定には一部“個々の患者における治療継続の是非の判 断”が含まれることになり、混乱を招く記載となっている。この“unequivocal progression”はあくまでも「非標 的病変のPD」に限った判断規準であることに注意が必要である。 JCOG における「PD」、「臨床的増悪」、「増悪」、無増悪生存期間のイベントの関係は下図のようになる。\n\n\n\n\n標的病変の選択とベースライン記録\n\n登録時に認められた測定可能病変のうち、径（非リンパ節病変は長径、リンパ節病変は短径）の大きい順 に5 つまで、1 臓器あたり最大2 個までを選択して標的病変（target lesion）とする。選択の際には、測定可能 病変を有する臓器ができるだけ満遍なく含まれることと、繰り返し計測の際の再現性すなわち測りやすさ （reproducible repeated measurement）を考慮して選択する（径が大きくても測りにくい病変は避ける）。 選択した標的病変について、頭側から尾側の順に、部位（コード）、検査法、検査日、非リンパ節標的病変 の長径、リンパ節標的病変の短径、およびすべての標的病変の径の和（以下、径和）を「治療前記録-腫瘍評 価」に記録する。 腫瘍径は「mm（ミリメートル）」で記録し、小数点以下の計測値の場合は小数第二位を四捨五入して小数第 一位とする（例：計測値が25.252 mm の時は25.3 mm とする）。\n\n非標的病変のベースライン記録\n\n標的病変として選択されなかった病変は、測定可能か否かを問わずすべて非標的病変（non-target lesion） として病変の部位（コード）、検査方法、検査日を「治療前報告-腫瘍評価」に記録する。同一臓器内の複数の 非標的病変は、1 病変として記録してよい（例：複数の腫大骨盤リンパ節、多発性肝転移）。\n\n腫瘍縮小効果の判定\n\n治療開始から8 週毎に「8.3. 治療期間中の検査と評価」に従って標的病変および非標的病変の評価を登 録時と同じ検査法にて行い、標的病変の径、非標的病変の消失※1 または増悪の有無を「治療経過記録-腫瘍 評価」に記録する。\n有効性の評価は、頻度を密にすることで有効性評価に影響を及ぼす可能性が高いことから、増悪が疑わ れる場合を除いて、規定の頻度で評価を行うこと。規定された時期以外に追加で行われた検査の結果は、増 悪の有無の判断には用いるが、総合効果におけるCR/PR/SD の効果判定には用いない。\n「6.2.2.プロトコール治療中止の規準」に従って無効中止と判定された以降に行われた検査の結果は総合効果におけるCR/PR/SD の効果判定には用いない。一方、無効中止以外の理由でプロトコール治療中止と なった場合は、中止後の初回の検査までの結果を効果判定に用いる。ただし、中止後の初回の検査を実施 するまでに後治療が開始された場合は、中止後初回の検査であっても結果を効果判定には用いない。\n\n標的病変の効果判定基準\n\n・CR（Complete Response）：完全奏効\n\nすべての非リンパ節標的病変が消失し、すべてのリンパ節標的病変の短径が10 mm 未満となっ た場合。ベースラインでリンパ節標的病変が選択された場合、径和が0 mmにならない場合でも標 的病変の効果がCR となることもある。つまり、CR とPD の両方を満たした場合はCR とする（10 mm 未満のリンパ節病変のみ残存した場合に、径和が20%以上増加かつ絶対値でも5 mm 以上 増加することがあり得るが、その場合もCR とする）。\n【プロトコール規定でFDG-PET を許容する場合の記載例】 CT で標的病変が残存しているが、それらがすべて瘢痕組織と考えられる場合にはFDG-PET を CR 判定に用いることができる。その場合、すべての標的病変がFDG-PET で陰性であることをも ってCR とする。 ・\n\nPR（Partial Response）：部分奏効\n\nベースライン径和に比して、標的病変の径和が30%以上減少\n\nPD（Progressive Disease）：進行\n\n経過中の最小の径和（ベースラインが経過中の最小値である場合、これを最小の径和とする）に比 して、標的病変の径和が20%以上増加、かつ、径和が絶対値でも5 mm 以上増加\n\nSD（Stable Disease）：安定\n\nPR に相当する縮小がなくPD に相当する増大がない ・\n\nNE（Not all Evaluated）：評価の欠損あり\n\nなんらかの理由で検査が行えない場合、またはCR、PR、PD、SD いずれとも判定できない場合\n\n\n\n非標的病変についても効果判定基準は存在する。\n\n新病変出現の有無\n\nベースラインでは存在しなかった病変が治療開始後に認められた場合、「新病変」の出現ありとする。 ただし、「新病変」とするには、ベースライン評価時の検査との撮影方法の相違や画像モダリティの変更に よる画像上の変化ではないことや、腫瘍以外の病態による画像上の変化ではないことが必要である。例えば、 肝転移巣の壊死により病巣内に生じた嚢胞性病変は新病変とはしない。ベースライン（登録前評価）にて必 須としていなかった部位の検査により新たに認められた病変は新病変とする1）。\nある病変が消失し、後に再び出現した場合には「新病変」とはせず、測定を継続する。ただし、病変が再出 現した時点での効果は、他の病変の状態により異なる。総合効果がCR 後に病変が再出現した場合は、再 出現の時点でPD と判定される。一方、総合効果がPR またはSD の場合には、一度消失した病変が再出現 した場合、その病変の径が効果を算出するために残りの病変の径和に加えられることになる。すなわち、多く の病変が残存する状態では、1 つの病変が見かけ上「消失」した後に再出現したとしても、それのみでPD と は判定せず、全病変の径和がPD の規準を満たした場合にPD と判定する。これは、大半の病変は真に「消 失」するわけではなく、使用した画像モダリティの分解能の限界によって描出されないだけであるという認識 があるためである。\n新病変である可能性があるが確定できない場合は新病変とはせず、臨床的に適切な時期を空けて画像検 査を再検する。再検した画像検査にて新病変であると確定した場合、新病変と確定した時点の画像検査日を もって新病変出現とする2）。\n\n総合効果（Overall Response）\n\n総合効果（Overall response）は標的病変の効果、非標的病変の効果、新病変出現の有無の組み合わせ から、以下の表11.1.9.a に従って8 週毎※に判定する。ベースラインで非標的病変が存在しない場合の総合 効果は、標的病変の効果と新病変出現の有無により判定し、ベースラインで標的病変が存在しない場合の総 合効果は非標的病変の効果と新病変出現の有無により表11.1.9.b に従って判定する。\n\n\n\n\n最良総合効果（Best Overall Response）（confirmation を要する場合）\n\n奏効割合や完全奏効割合がprimary endpoint である非ランダム化試験において、「最良総合効果」をPR ま たはCR とするためには、それらの確定（confirmation）が必要である。 総合効果（overall response）はCR＞PR＞SD＞PD＞NE の順に「良好」であるとし、全コース（観察期間 を設ける試験では観察期間の全コース）の総合効果から以下の規準に従って最良総合効果（Best Overall Response）を判定する。複数の区分の定義に該当する場合は、CR＞PR＞SD＞PD＞NE の順に、より良好 なものに区分する。\n\n最良総合効果（Best Overall Response）（confirmation を要さない場合）"
  },
  {
    "objectID": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html#全体抄録",
    "href": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html#全体抄録",
    "title": "抗がん剤試験におけるEstimandを考える",
    "section": "",
    "text": "Estimand は、治療、対象集団、変数、要約指標、中間事象（治療開始後に起こり測定値の存在・解釈に影響する事象）の取り扱い、という5つの要素で関心のある治療効果を明確に設定する枠組みです。従来の PICO ではレスキュー治療の開始など中間事象を治療の一部と見なし、治療効果を評価することが一般的でした。これに対し Estimand の枠組みで中間事象の取り扱いを試験計画時に整理することで、例えば「ある中間事象が起こらなかった場合の治療効果」という特定の臨床的問いに対応した治療効果を検証する試験の計画が可能です。この枠組みを試験に実装するには、医師、CRC、PM/StM、DM、MOなど多職種の協働が不可欠です。試験目的に沿った Estimand を定めるには、まず関心のある治療条件を整理し、関係者間で認識を共有します。続いて、合意した Estimand に基づき、当該治療効果の推定に必要なデータの収集可否と方法を多職種で検討します。こうした議論を行うことで収集項目の優先順位が明確になり、重要なデータが適切に収集され、試験の品質向上に繋がります。本発表では、Estimand が変わるとデータ収集戦略がどのように変化するか事例を通じて検討します。"
  },
  {
    "objectID": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html#jcogプロトコールマニュアルに基づいて抗がん剤試験における試験デザインをまとめる",
    "href": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html#jcogプロトコールマニュアルに基づいて抗がん剤試験における試験デザインをまとめる",
    "title": "抗がん剤試験におけるEstimandを考える",
    "section": "",
    "text": "URL：https://jcog.jp/doctor/tool/manual/index.html"
  },
  {
    "objectID": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html#population",
    "href": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html#population",
    "title": "抗がん剤試験におけるEstimandを考える",
    "section": "",
    "text": "試験の対象集団を規定する上でのstage や疾患の程度・拡がりを診断する規準を記載する。\n原則として、患者選択（適格規準）や割付調整因子、治療前評価項目に関係する規準や定義が該当する。\n適格規準と除外規準に分けて、選択規準を規定する。 試験の結果、治療法の有効性が示された場合にその治療を適用することが妥当とみなせる対象集団を規定するものが適格規準（inclusion criteria）であり、外的妥当性（external validity）すなわち一般化可能性 （generalizability）に関連する。一方、適格規準で示される対象集団には属するが、治療のリスクが高いために試験に組み入れることが倫理的でないか（倫理的側面）、試験で必要な有効性・安全性の評価に影響を及ぼすと判断される対象を除外 する（科学的側面）条件を規定するものが除外規準（exclusion criteria）であり、科学的側面としては内的妥当 性（internal validity）すなわち比較可能性（comparability）に関係する\n狭すぎる選択規準の試験結果は特定の患者集団にしか適用できないものとなる（一般化可能性が低い）し、 逆に広すぎると治療効果が期待できない患者が多く含まれることとなって治療効果の差が薄まってしまう（内的妥当性が低い）。試験の目的である治療効果の評価に適切な集団を選択する適格規準を設定しなければ ならない\n一般化可能性/外的妥当性について、「特定の施設に限って試験を行った場合、その結果は日本全体には外挿できないことから標準治療とは言えないのではないか」といった質問を受けることがある。例えば、手術手技の試験において、技術認定を受けた外科医に術者を限るといった場合が挙げられる。こうした制限を設けることは、まだ一般には普及していない難度の高い新しい技術を要する実験的治療に伴う患者リスクを最小化するという倫理的側面が第一義であるが、その新しい技術が有効性で真に優れたものである時に、技術が未熟な術者が加わることでその技術の真の有効性が正しく評価されないという科学的側面（比較可能性/内的妥当性）もある。"
  },
  {
    "objectID": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html#intervention治療計画と治療変更基準",
    "href": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html#intervention治療計画と治療変更基準",
    "title": "抗がん剤試験におけるEstimandを考える",
    "section": "",
    "text": "プロトコール治療\n\n試験で評価する「プロトコール治療」の定義と全体像を説明した上で、個々の治療内容を群別（比較試験の場合）・モダリティ別に詳述する。比較試験では群別の記載を基本とし、放射線治療や手術等、群で共通の治療がある場合には「両群共通」として記載してもよい。\n特に複数のレジメンや複数のモダリティによる治療レジメンの場合、「プロトコール治療」の定義を明確に行う。 後治療との区別も明確に定義する。\n複数コースからなる治療レジメンの場合、何コースをもって「プロトコール治療完了」とするかを明記する。\n効果や毒性などによってコース数や次に進むレジメンが異なるような場合は、その判断規準を明確に示す。 登録後に治療を開始するまでの期間の上限を規定する。入院治療の場合は「登録後7 日以内」（登録日の翌 週の同じ曜日まで）、外来治療の場合は「登録後14 日以内」を原則とする。ただし、手術や放射線治療がプロ トコール治療に含まれる場合は、手術室予約や放射線治療計画に時間を要するため、「登録後21 日以内」 や「登録後28 日以内」なども許容される。\n原則として「コース開始規準」は第2 コース以降に適用し、第1コースの開始に際してはコース開始規準や適格規準は適用しない。\n「登録時に適格規準を満たしたが治療開始前に検査値が適格規準を満たさなくなった」という場合、治療を開始してもプロトコール逸脱/違反とはならない。そのため、登録後の治療開始までの期間は十分に短く決める 必要がある。十分短く設定しても、治療開始前に臓器機能の検査値が悪化して担当医判断により治療を開始 せず「プロトコール治療中止」となる場合もあり得るが、それが頻発するようなら適格規準を再検討する必要がある（ごく少数例生じるのは問題とならない）。なおSouthwest Oncology Group（SWOG）においては、「登録当日または登録翌日」に治療を開始しなければならないとしている。つまり治療開始予定日が登録当日か翌日でないと登録ができない。\nプロトコール治療として、使用が規定されるすべての薬剤（抗がん薬、支持療法薬）を記載すること。薬剤名は一般名（一般的名称）を記載する。ただし、用いる薬剤の剤形（錠剤と顆粒剤など）に特別な規定を 設ける場合は、その旨を記載すること。 後発医薬品（ジェネリック医薬品）の使用の可否については各医療機関の方針によるため、JCOGとしては原 則として後発医薬品の使用を制限しない。制限を加える必要がある場合には本章に記載する。\n\n化学治療\n\nランダム化試験の場合、群毎に分けて記載する。\n治療レジメンについて、薬剤名、投与量、投与法、投与日を明記する。\nコースの表現は「○週1 コースとして×コース行う」を標準とする。 （薬剤投与が1 週間、3 週1 コースのレジメンの場合、「3 週間隔で×コース」という表現に対して、4 週1 コー スと解釈したための系統的逸脱の事例がある）\n体表面積から実投与量を計算する際の、まるめ（切り上げ/切り捨て/四捨五入）の方法を明記する。同じグル ープの複数の試験でまるめの方法が異なることはミスの元となるため、切り捨てを標準とし、疾患や薬剤によ って切り上げや四捨五入が適切な場合は許容する。同一の薬剤で異なる剤形（注射薬と内服薬など）が混在 する場合は各々について明示する。 ・\n治療開始後の体重変動による投与量補正（再計算）を行うかどうかを明記する。行う場合はその方法を明記 する。体重が増加した時も減少した時も再計算を行うとする規定（±〇kg または±〇%で規定）、体重減少の 時のみ再計算を行うとする規定のいずれも可である。kg と%のどちらで規定してもよい。\n\n放射線治療\n\n放射線治療の開始時期、予定休止期間の有無、祝日などの扱いについて記載する。\n\n外科的切除術\n\n手術術式の特定、切除範囲、再建術式など、必須とされる手技や許容範囲とする手技を明確に記述する。図 示が望ましい。\n\nプロトコール治療完了基準\n\nプロトコール治療完了とみなす治療内容やコース数、原病の増悪・再発、治療中止とすべき毒性（有害事象）、 コース開始延期の許容範囲またはプロトコール治療期間全体の延長許容範囲などの判断規準を記述する。\nプロトコール治療中止理由の分類の基本は以下のとおりであるが、後述するようにプロトコール毎に詳細な表現に変更して細分類を付加することは、治療中止規準を明確にする上でも推奨される。\n\n① 治療完了：プロトコール規定の治療完了\n② 増悪/再発：原病の増悪・再発による治療中止。増悪・再発以外の状況での無効中止を設ける 場合はここに分類し、「②増悪/再発/無効」としてよい。\n③ 有害事象：担当医判断または中止規定に従った、有害事象による治療中止\n④ 拒否（有害事象）：有害事象との因果関係がある患者拒否による治療中止\n⑤ 拒否（その他）：有害事象との因果関係がない患者拒否による治療中止\n⑥ 死亡：プロトコール治療中（投与間の観察期間を含む）の死亡（治療との因果関係を問わない）\n⑦ その他：①～⑥以外の理由による治療中止 ・\n\nコースや評価期間が規定される試験の場合は、その規定コースまで治療が継続されたものを「①治療完了」 とし、規定の最終コースの治療を完了する前に増悪のため治療が中止されたものを「②増悪/再発」に分類す ることとする。その際、評価期間を明確に示すこと。\n増悪まで治療を続け、かつ「評価期間は○コースまで」と規定されない試験の場合、①と②が分離不能のた め、「①治療完了」の分類は用いない。 ・\nRECIST に従う効果判定は、他の試験との奏効割合の比較可能性を確保するために行われることから、 個々の患者の治療継続の是非の決定をRECIST に従って判定した「総合効果」に基づいて決めることは必ずしも適切ではない。従って「無効中止」の規定をCR/PR/SD/PD を用いて行うことは許容されない。\n患者拒否による中止の場合、その理由が毒性（有害事象）との因果関係があると判断される場合と、毒性と の因果関係がないと判断される場合を区別する。有害事象との因果関係あり/なしの判断は治療との因果関 係あり/なしの判断に準じて、「7.2.2. 有害事象と治療との因果関係の判定」に示した、“definite、probable、 possible”のいずれかと判断された場合は「因果関係あり」とし、“unlikely、unrelated”のいずれかと判断され た場合は「因果関係なし」とする。“possible”と“unlikely”の区別については、“reasonable possibility”の考え 方を重視し、「どちらによると考えるのがよりもっともらしいか」によって判断する、すなわち有害事象により患 者が中止を希望したと考える方がもっともらしければ“possible”、有害事象以外の要因により患者が中止を希 望したと考える方がもっともらしければ“unlikely”と判断する。\n複数のレジメンやモダリティを組み合わせたプロトコール治療では、一次治療レジメン中止の後に二次治療レジメンを行う場合もあり、「一次治療中止」、「二次治療中止」、「プロトコール治療中止」の関係が複雑となるた め注意する。次の治療レジメンへの移行の規準を明確にする。プロトコール治療が二次治療までの場合は、 「一次治療中止・完了規準」と「プロトコール治療（全治療）中止・完了規準」を別に設け、三次治療までの場合 は「一次」、「二次」、「プロトコール治療」の3 つを設けることを推奨する。\nプロトコール治療が複数のレジメンや複数のモダリティの逐次的な組み合わせである場合、それぞれのレジメンについて「完了」の定義、「中止」の規準を設けることを推奨する。\n増悪中止、毒性中止、患者拒否中止までプロトコール治療を継続する治療レジメンの場合、完了はないため、 以下のように記述する。\n\nプロトコール治療中止の基準\n\n以下のいずれかの場合、プロトコール治療を中止する。\n1） プロトコール治療無効と判断\n\n-   注）以下の②を設けない場合は「1）治療開始後に原病の増悪が認められた場合」とする。\n\n    -   ① 治療開始後に原病の増悪が認められた場合 ※画像による効果判定でPD と判定されても臨床的にプロトコール治療継続が妥当と判断される場合には原病の増悪とはせず、プロトコール治療を継続する。\n\n    -    ② ○コース終了時点までに腫瘍の縮小や症状の改善がみられない場合\n\n        -   注） 治療継続の可否を決める「治療無効」かどうかの判断は総合的な臨床判断で行う。画像に よる総合効果（CR/PR/SD/PD）はあくまでも参考とする。実際には腫瘍が縮小していても総 合効果はPD となり得るし、腫瘍が増大していても総合効果はPR となり得る。総合効果が PR の時に臨床的には無効と判断してプロトコール治療中止とすることが妥当である場合も あるし、総合効果がPD であっても治療が有効と判断して治療継続が妥当である場合もあ る。\n\n2) 有害事象によりプロトコール治療が継続できない場合\n\n① Grade 4 の非血液毒性が認められた場合 （非血液毒性：CTCAE v5.0-JCOG における「貧血」「骨髄細胞減少」「リンパ球数減少」「好中球数 減少」「白血球減少」「血小板数減少」「CD4 リンパ球減少」以外の有害事象）\n② 有害事象により次コース開始が○週間遅延した場合\n③ 治療変更規準（6.3.）でのプロトコール治療中止の規定に該当した場合\n④ 治療変更規準以外で、有害事象により、担当医がプロトコール治療中止を要すると判断した場合\n\n3） 有害事象との因果関係がある理由により、患者がプロトコール治療の中止を申し出た場合\n\n有害事象との因果関係がある（definite, probable, possible）と判断される理由の場合はこの分類 を用いる\n\n4） 有害事象との因果関係がない理由により、患者がプロトコール治療の中止を申し出た場合\n\n有害事象との因果関係がない（unlikely, unrelated）と判断される理由の場合はこの分類を用いる\n登録後、プロトコール治療開始前の患者拒否の場合 ・ プロトコール治療中の本人や家人の転居など、有害事象との因果関係がまず否定できる場合\n\n5） プロトコール治療中の死亡 ・ 他の理由によりプロトコール治療中止と判断する以前の死亡\n6） その他、登録後治療開始前の増悪（急速な増悪によりプロトコール治療が開始できなかった）、プロトコ ール違反が判明、登録後の病理診断変更などにより不適格性が判明して治療を変更、社会的理由や 安全管理上の問題によりプロトコール治療の継続が困難と判断された場合など\n\n治療変更基準\n\n毒性の種類、程度（Grade や検査値）毎に、研究者による解釈の違いが生じないよう変更規準を明確に規定 する。\n\n適切な章構成は試験によって異なるが、例として以下の章構成が考えられる。\n\n6.3.1. 用量レベル\n6.3.2. コース開始規準\n6.3.3. コース内の休止/再開規準\n6.3.4. 減量規準\n\n\n用語の一貫性について\n\nhold/suspend/halt：いったん中止して条件が揃えば再開（discontinue temporally）\nterminate：再開しない途中中止＝終了（discontinue permanently）\nskip：その時のみ投与しない complete：予定どおりすべて投与して終了＝完了 など\n\n使い分けが比較的容易だが、日本語では「中止」がさまざまな意味を有するため十分注意して記述する こと。下記の定義による「延期」「中止」「休薬」「休止」「スキップ」を用いることを推奨する。\n延期 delay\n\n投与間隔の延長、投与を規定より遅らせること。 延期可能な期間を明記すること\n\n中止 terminate\n\n治療全体または特定の薬剤やモダリティの永久的・継続的取りやめ。再開しない。\n例： 投与開始予定日より3 週を越えても次コース開始規準を満たさない場合、プロトコール治療中止と する。\n\n休薬 hold/suspend/halt\n\n-   治療薬の1 剤以上をいったん休み、再開する条件が揃うのを待つこと。 薬剤単位で規定する時に用いる。\n\n休止 hold/suspend 治療全体または特定のモダリティをいったん休み、再開する条件が揃うのを待つこと。\n\n-   条件が満たされれば再開する。モダリティ単位で用いる。\n\n-   休止の場合、再開する際には休止した時点で予定されていた治療を再開する。\n\n-   後述の「スキップ」 では予定していた治療の一部を行うことなく次（コース）に進む。\n\n    -   例： 放射線治療中、WBC＜2,000 mm3 を認めた場合は放射線治療を休止し、WBC≧2,000 mm3を確 認した後に放射線治療を再開する。 ・\n\nスキップ skip\n\n治療の一部以上を実施せず次の投与スケジュールに進むこと。\n\n\n\n治療変更規準作成のヒント\n\n治療変更規準はできる限りシンプルで明確、かつ臨床的に妥当なものでなければならないが、実際には、薬 剤の特性、治療レジメンの特徴、認可された用量・用法など、考慮すべきパラメータが非常に多く、プロトコー ル作成の中でももっとも難しい部分と言える。\n治療減量基準\n\n用量の変更（減量）\n\n減量規準を文章で表現すると、「何に対してどれくらい減量するか」を明確にすることは困難であり、 初回投与量に対して○%に減量、前コース投与量に対して○%に減量、前回（直近）の投与量（同 じコースでの前回投与を含む）に対して○%に減量、何回まで減量を行うか、等が不明確になりやすいことから、JCOGでは後述する例のように「用量レベル」を設定する減量規準とすることを推奨 する。\n\n毒性回復後の再投与や増量の可否\n\n「回復」とする定義を明確にする（例：Grade 0 に回復。治療前PaO2 －10 torr 以上に回復）。また、 再開時に「減量した投与量」を継続するのか、「減量前の投与量に戻す（再増量）」のかを明確にす る。\n\n減量後にも規定の毒性が持続または再出現する場合の投与量\n\n-    「さらに減量を行う」のか「それ以上の減量を行わず中止する」のかを明確にする。\n\n次コース開始条件・投与可能条件\n\n-   他の治療変更（減量・延期）規準、適格規準との整合性を十分検討の上で用いる。その際、開始規 準を満たさない場合の対応を明確にする。\n\n体重変動による投与量変更 コース・投与毎に体重変動により投与量変更を行うのか、体重変動によらず初回投与量を続ける のかを明記する。特定の条件下でのみ体重変動による投与量変更を行う場合、その条件を明記 すること。体重変動による投与量変更は「増量」「減量」とは呼ばない。\n\n用量レベル\nコース開始基準\n\n次コースを開始しても安全と思われる程度に臓器機能が回復していることを確認するための指標であり、開 始当日またはその前日（または3 日前など）までに満たすべき臨床検査値の値などとして決定する。\nコース開始規準（や休止規準）は、プロトコール治療との因果関係を問わない有害事象として規定する。例え ば、「発熱」があれば、プロトコール治療とは関係がない「感冒」によるものであったとしても、抗がん治療は延 期することが妥当であると考えられるためである。\n外来治療の場合、当日の検査値のうち、血算は検査値を確認してから投与の可否や減量の有無を決定でき ても、生化学検査は投与時までに検査結果が判明しないこともあるため、当日（や直近）の検査値を用いて規 定することが不適切な場合がある。この点を考慮して規定すること。\nコース開始規準は、第1 コースには適用されないことを明記すること\n\n減量中止基準\n\n基本的には、前コースで観察された有害事象がある条件を満たす場合に次のコースの投与量を減量するための規準。すべての薬剤を減量する場合と特定の薬剤のみ減量/中止する場合がある。\nコース開始規準と異なり、減量/中止規準は、プロトコール治療との因果関係がある「有害反応」として規定す るべきである。例えば、前コースでGrade 3 の下痢が見られていたとしても、それが、他の原因が明らかな食 中毒のようなものであれば、減量する必要はないと考えられるためである。\n神経毒性を治療変更規準に含める試験での注意点 CTCAE v5.0 で、末梢性運動ニューロパチーと末梢性感覚ニューロパチーは以下のように定義される。\n\n併用療法・支持療法\n\nプロトコール治療期間中の併用療法・支持療法について、\n\n「規定とする」\n「推奨される」\n「許容される」\n「推奨 されない」\n「許容されない」\n\nの区分毎に記載する。用量や用法についての条件がある場合もその旨を明記す ること。\n\n後治療\n\nプロトコール治療中止/終了後の他の治療（プロトコール治療と同一の治療の全部または一部の反復を含む） に対する制限を記載する。\nランダム化試験の場合、プロトコール治療中止/終了後に、もう片方の群の治療を行ってもよい（クロスオーバ ー）のかどうかを必ず記載すること。\n主たる解析や中間解析で、いずれかの治療群が良いと結論された場合、試験の結果を説明し、それぞれの 患者の治療歴を考慮の上、最良と考えられる治療法を提供する旨を記載する。\nまた、従来JCOG 試験においても、毒性による中止規準に該当したり患者拒否により「プロトコール治療中止」 とした後、さらにプロトコール治療と同じレジメンを「後治療」として継続している例が多くみられたが、それは 推奨されない。理由は、同じ治療レジメンであれば、担当医が「後治療」と主張したとしても、それにより生じた 有害事象はプロトコール治療の安全性評価に含めるべきであるためである。「中止規定に該当したが担当医 判断や患者の希望で同じ治療を継続」した場合は、「プロトコール治療中止→後治療」ではなく「中止規定を逸 脱した上でのプロトコール治療継続」とする。これを「後治療」として評価の枠外に置くことを許容すれば、例え ば「7 コースで都合の悪い（逸脱に引き続いて生じた）重篤な有害事象が生じたため、遡って6 コースでプロト コール治療中止として後は後治療と扱う」と言った恣意的な過小評価が可能になってしまう。\n特に、化学療法に続いて放射線治療を行うような集学的治療レジメンの場合、化学療法中止例における事後 の放射線治療をプロトコール治療の一部と扱う（毒性評価データを収集する）のか、後治療と扱う（毒性評価 データを収集しない）のかを明確に区別すること。\n主たる解析や中間解析等により試験の主たる結論が判明した場合、必要に応じて試験に登録された患者 に試験の結果を説明し、個々の患者の治療経過を考慮の上、最良と考えられる治療法を提供する。\nプロトコール治療が手術単独治療などで、「プロトコール治療中止規準には該当するが、臨床的にはプロトコ ール治療継続が妥当と判断される場合」が生じ得ない場合、以下の記載は削除してよい。\n臨床研究法に従って実施する試験ではこちらを使用する。\n\n-   また、プロトコール治療中止規準には該当するが、臨床的には「プロトコール治療継続」が妥当と判断され る場合は、原則として（時間的余裕がない場合を除いて）、担当医レベルで決定するのではなく、研究責任医 師を通じて研究事務局に相談すること。研究事務局と研究責任医師の合意の下に、「プロトコール治療中止 →後治療として治療」か、「逸脱してプロトコール治療継続」かを決定する。研究事務局との相談内容および意 思決定の経緯は、当該患者の治療終了報告や経過記録のコメント欄に詳細を入力すること。なお、「逸脱して プロトコール治療継続」が頻発する場合は、プロトコール治療中止規準が臨床的に不適切である可能性があ るため、研究事務局はグループ会議やグループメーリングリストを利用してプロトコール治療中止規準の見 直しについて検討する。\n\n人を対象とする生命科学・医学系研究に関する倫理指針に従って実施する試験ではこちらを使用する。\n\nまた、プロトコール治療中止規準には該当するが、臨床的には「プロトコール治療継続」が妥当と判断され る場合は、原則として（時間的余裕がない場合を除いて）、担当医レベルで決定するのではなく、施設研究責 任者または施設コーディネーターを通じて研究事務局に相談すること。施設研究責任者・施設コーディネータ ーの合意の下に、「プロトコール治療中止→後治療として治療」か、「逸脱してプロトコール治療継続」かを決 定する。研究事務局との相談内容および意思決定の経緯は、当該患者の治療終了報告や経過記録のコメン ト欄に詳細を入力すること。なお、「逸脱してプロトコール治療継続」が頻発する場合は、プロトコール治療中 止規準が臨床的に不適切である可能性があるため、研究事務局はグループ会議やグループメーリングリスト を利用してプロトコール治療中止規準の見直しについて検討する。\n\n\n予期される有害事象\n\n本試験において予期される有害反応は以下のとおり。\n\n併用化学療法の場合の薬物有害反応、外科手術・放射線治療の有害反応について記載する。第III 相試験 の場合は試験治療群だけでなく標準治療群についても予期される有害反応を記述する。 ・ 複数のモダリティからなるレジメンの場合、それぞれのモダリティ別に記述した上で、併用することによって増 強される可能性がある有害反応について特に注意して詳述する。頻度は文章で羅列するよりも表で簡潔にま とめることが推奨される。文献や添付文書により頻度が数値として判っている場合には数値を記述し、そうで ない場合には「しばしば」「まれに」などで記述する。 ・ 「重篤な有害反応」が予期される場合にその頻度が予期されたレベルよりも増えている時、研究代表者から 効果・安全性評価委員会への報告が必要となるため、可能な限りその頻度を数値で示しておくこと。\nなお、CTCAE では、「有害事象（Adverse Event）」とは、「治療や処置に際して観察される、あらゆる好ましく ない意図しない徴候（臨床検査値の異常も含む）、症状、疾患であり、治療や処置との因果関係は問わない。 すなわち因果関係があると判断されるものと、因果関係ありと判断されないもの両者を含む。」である。\n従って、「明らかに原疾患（がん）による」ものであっても、試験治療（プロトコール治療）本体ではなく支持療法 や併用療法により生じたと思われるものであっても、やはり「有害事象」である。\nしかし、がんの臨床試験においては、多くの場合「死亡」まで追跡がされることから、最終的には多くの登録患者において「原疾患（がん）による有害事象」が多数観察されることになり、追跡期間中の「有害事象」データ をすべて一律に収集することは現実的ではないし意味がない。\nそこで、JCOG では、有害事象データの収集ポリシーとして以下の原則を設ける。\n\n①プロトコール治療の最終治療日から30 日以内の有害事象は、因果関係によらずすべて収集する（有害事 象報告に際しては、有害事象のgrading とは別に「因果関係」が検討される）。\n②プロトコール治療の最終治療日から31 日以降の有害事象は、プロトコール治療との因果関係があり （definite, probable, possible のいずれか）と判断されるもののみ（＝有害反応・薬物有害反応）を収集する。\n\n\n\n評価項目・臨床検査・評価スケジュール\n\n原則として、「登録前」、「治療期間中」、「治療終了後」の3 つの時期別に、検査項目と頻度（間隔）を明記する。\n\n登録前評価項目\n\n登録前に必要な評価項目を列記する。\n検査日の規定については登録日より遡って何日以内までの検査を許容するかを明記すること。\n「日」で規定 するが、7 日、14 日、28 日など、週単位の規定と一致する方が望ましい。\n「登録前7 日以内」は、1 週前の登 録日と同一曜日までを含むこととする。 ・ 画像診断や内視鏡検査などの期限は、臨床的な適切さや実施可能性を考慮して決定する。JCOG の標準は 登録日を含まない28 日以内である。\nつまり登録日をday 0 としてday -28 までを意味する。ただし、進行が 遅いがん種が対象の場合はもっと長い期限を設定することもあり得る。\n\n評価期間の定義\n\n原病の増悪までプロトコール治療を継続する試験では、有害事象や治療経過を密に収集する「観察期間」と 安全性情報のみを取集する「患者追跡期間」とを定義すること。\n「観察期間」や「患者追跡期間」を定める場合は、データ収集の項目や頻度を両者で変更してもよい。\n評価期間の定義を定める場合は、以下を用いる。 ・\n観察期間は、6 か月相当（4 週×6＝24 週、3 週×9＝27 週）を基本とする。\n\n治療期間中の検査と評価\n\n治療中の毒性評価、有効性評価に必要な臨床評価項目、臨床検査、画像検査を検査間隔毎に記載する。\n検査項目別にまとめるよりも頻度や検査時期毎にまとめることを推奨する。\n観察期間を設ける試験では、患者追跡期間中は追跡調査にて有害事象を収集する。\n\n\n\n\n治療終了後の検査と評価項目\n\nプロトコール治療終了/中止後の追跡期間における評価項目や臨床検査を頻度と共に記載する。\n観察期間を設ける試験では、患者追跡期間中かつプロトコール治療終了日から30 日以内に発生した有害事 象は追跡調査に記載する。\n比較試験の場合、原則として群間で評価間隔に差が生じないように注意すること。\n放射線治療を含むレジメンの試験や、注意すべき晩期毒性を有する抗がん薬を用いている試験においては、 それらの晩期毒性が適切に評価されるように評価項目を決定すること。特に放射線関連の有害事象は「治療 開始から90 日以内」の急性毒性と「91 日以降」の遅発性反応に区別して評価されるため、期間の区分のし かたに注意すること。\n外科的切除術を含むレジメンの場合は、7 章で定義した期間の区分を用いること。\n\n治療終了後の安全性評価\n\n有害事象\n\n治療終了後は、以下の項目を評価する。\n\n治療終了後～X 年：○か月ごと\nX 年～XX 年：○か月ごと\n\n\n後治療に関する情報\n\n後治療について、以下の項目を評価する。 ・\n\n後治療の有無\n後治療の内容（後治療を行った場合）\nプロトコール治療中止後最初の後治療開始日（後治療を行った場合）\n後治療開始時のPS（後治療を行った場合）\n\n\n\n治療終了後の有効性評価\n\n画像検査について、検査内容、評価間隔について定めること。増悪/再発を認めた場合には、増悪/再発時 の全身状態（PS）、増悪/再発形式などを記録する。増悪/再発後も評価を継続するかどうかについても定める。\n効果判定のための評価は、「11.1.5. 腫瘍縮小効果の判定」と同様に、「治療開始」を起点として規定する。\n\n追跡調査\n\n上記「治療終了後の安全性評価」および「治療終了後の有効性評価」は、追跡調査時にデータ収集が行われ る。\n施設における生存確認、増悪（または再発）の有無の確認方法について記載する。\n\n![](images/paste-1.png){width=\"605\"}"
  },
  {
    "objectID": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html#効果判定とエンドポイントの定義",
    "href": "posts/statistics/2025/ESTIMAND_抗がん剤_ARO協議会.html#効果判定とエンドポイントの定義",
    "title": "抗がん剤試験におけるEstimandを考える",
    "section": "",
    "text": "効果判定\n\n術後補助化学療法の試験などで、効果判定を行わない場合 「本試験では効果判定は行わない。」と記載する。\n固形がんの腫瘍縮小効果判定は原則としてNew response evaluation criteria in solid tumours［Revised RECIST guideline （version 1.1）］に従って行う\nRECISTv1.0 原著論文には、「治療継続の決定を目的とした使用は本ガイドラインの主旨ではない」と明記 されており、同様の記載はRECISTv1.1 にも引き続き下記のように明記されている。 「腫瘍専門医の多くは、日常診療で悪性腫瘍患者の経過観察のための画像検査による客観的な規準と、症状に基づく規準の双方に基づいて、治療継続の是非についての意思決定を行っている が、本改訂ガイドラインは、治療を担当する腫瘍医が適切であると判断する場合を除いて、このよ うな個々の患者における治療継続の是非についての意思決定に用いられることを意図していな い。」\n従って、RECIST ガイドラインに基づく効果判定によって決定される「総合効果」は、「薬剤またはレジメンが 開発研究を続けるに値する有望な結果を示すかどうかの判断に用いられる」べきものである。すなわち、 個々の患者における治療継続の是非の判断は、総合効果のCR/PR/SD/PD に基づいて行うのではなく、画像所見に加えて、症状や身体所見、各種検査値等を総合的に加味して行う「臨床的判断」に基づくべきである。\nそのため、画像診断に基づく効果判定による総合効果としての「PD（Progressive Disease：進行）」と判断 した時点でも、臨床的にはプロトコール治療継続が適切な場合が存在する。この場合には効果判定結果によ らず臨床的判断によってプロトコール治療継続の是非を判断すべきではあるが、無増悪生存期間のイベント 日としては総合効果PD と判断した日を採用する。これは、（i）群毎にプロトコール治療を継続すべきかどうか の判断が異なりうること、（ii）RECISTは奏効割合のみならず、無増悪生存期間の標準化をも意図した規準で あること、（iii）米国のCooperative Group の標準的な定義は総合効果がPD であれば、いかなる理由であっ ても無増悪生存期間のイベントとしていること、の3 点の理由による。\n一方、画像診断に基づく効果判定規準での「PD」には該当しなくても、画像診断によらない臨床的・総合的 な判断により「臨床的増悪」と判断した場合は、「6.2.2.プロトコール治療中止規準」に従って、プロトコール治 療を中止すべきである。「臨床的増悪」と判断された場合には効果判定で「PD」と判定されていなくとも、「臨床 的増悪」と判断された日をもって無増悪生存期間のイベントとする。これは、「臨床的増悪」と判断された後の 画像検査がしばしば予定どおりに行われないため、「臨床的増悪」をもって無増悪生存期間のイベントとしな ければ、結果的に無増悪生存期間が過大評価されるリスクが大きいからである。なお、「臨床的増悪」をもっ て無増悪生存期間の「打ち切り」と扱うことも、増悪や死亡のリスクの高い患者を打ち切りにすることになるた め（informative censoring）統計学的に正しくない。\nなお、RECISTv1.1 原著論文では、非標的病変のPD 規準の中に「明らかな増悪（unequivocal progression）」とは「全体の腫瘍量の増加として治療を中止するに十分値する程度の非標的病変の著しい増 悪」と記載されていることから、非標的病変のPD 判定には一部“個々の患者における治療継続の是非の判 断”が含まれることになり、混乱を招く記載となっている。この“unequivocal progression”はあくまでも「非標 的病変のPD」に限った判断規準であることに注意が必要である。 JCOG における「PD」、「臨床的増悪」、「増悪」、無増悪生存期間のイベントの関係は下図のようになる。\n\n\n\n\n標的病変の選択とベースライン記録\n\n登録時に認められた測定可能病変のうち、径（非リンパ節病変は長径、リンパ節病変は短径）の大きい順 に5 つまで、1 臓器あたり最大2 個までを選択して標的病変（target lesion）とする。選択の際には、測定可能 病変を有する臓器ができるだけ満遍なく含まれることと、繰り返し計測の際の再現性すなわち測りやすさ （reproducible repeated measurement）を考慮して選択する（径が大きくても測りにくい病変は避ける）。 選択した標的病変について、頭側から尾側の順に、部位（コード）、検査法、検査日、非リンパ節標的病変 の長径、リンパ節標的病変の短径、およびすべての標的病変の径の和（以下、径和）を「治療前記録-腫瘍評 価」に記録する。 腫瘍径は「mm（ミリメートル）」で記録し、小数点以下の計測値の場合は小数第二位を四捨五入して小数第 一位とする（例：計測値が25.252 mm の時は25.3 mm とする）。\n\n非標的病変のベースライン記録\n\n標的病変として選択されなかった病変は、測定可能か否かを問わずすべて非標的病変（non-target lesion） として病変の部位（コード）、検査方法、検査日を「治療前報告-腫瘍評価」に記録する。同一臓器内の複数の 非標的病変は、1 病変として記録してよい（例：複数の腫大骨盤リンパ節、多発性肝転移）。\n\n腫瘍縮小効果の判定\n\n治療開始から8 週毎に「8.3. 治療期間中の検査と評価」に従って標的病変および非標的病変の評価を登 録時と同じ検査法にて行い、標的病変の径、非標的病変の消失※1 または増悪の有無を「治療経過記録-腫瘍 評価」に記録する。\n有効性の評価は、頻度を密にすることで有効性評価に影響を及ぼす可能性が高いことから、増悪が疑わ れる場合を除いて、規定の頻度で評価を行うこと。規定された時期以外に追加で行われた検査の結果は、増 悪の有無の判断には用いるが、総合効果におけるCR/PR/SD の効果判定には用いない。\n「6.2.2.プロトコール治療中止の規準」に従って無効中止と判定された以降に行われた検査の結果は総合効果におけるCR/PR/SD の効果判定には用いない。一方、無効中止以外の理由でプロトコール治療中止と なった場合は、中止後の初回の検査までの結果を効果判定に用いる。ただし、中止後の初回の検査を実施 するまでに後治療が開始された場合は、中止後初回の検査であっても結果を効果判定には用いない。\n\n標的病変の効果判定基準\n\n・CR（Complete Response）：完全奏効\n\nすべての非リンパ節標的病変が消失し、すべてのリンパ節標的病変の短径が10 mm 未満となっ た場合。ベースラインでリンパ節標的病変が選択された場合、径和が0 mmにならない場合でも標 的病変の効果がCR となることもある。つまり、CR とPD の両方を満たした場合はCR とする（10 mm 未満のリンパ節病変のみ残存した場合に、径和が20%以上増加かつ絶対値でも5 mm 以上 増加することがあり得るが、その場合もCR とする）。\n【プロトコール規定でFDG-PET を許容する場合の記載例】 CT で標的病変が残存しているが、それらがすべて瘢痕組織と考えられる場合にはFDG-PET を CR 判定に用いることができる。その場合、すべての標的病変がFDG-PET で陰性であることをも ってCR とする。 ・\n\nPR（Partial Response）：部分奏効\n\nベースライン径和に比して、標的病変の径和が30%以上減少\n\nPD（Progressive Disease）：進行\n\n経過中の最小の径和（ベースラインが経過中の最小値である場合、これを最小の径和とする）に比 して、標的病変の径和が20%以上増加、かつ、径和が絶対値でも5 mm 以上増加\n\nSD（Stable Disease）：安定\n\nPR に相当する縮小がなくPD に相当する増大がない ・\n\nNE（Not all Evaluated）：評価の欠損あり\n\nなんらかの理由で検査が行えない場合、またはCR、PR、PD、SD いずれとも判定できない場合\n\n\n\n非標的病変についても効果判定基準は存在する。\n\n新病変出現の有無\n\nベースラインでは存在しなかった病変が治療開始後に認められた場合、「新病変」の出現ありとする。 ただし、「新病変」とするには、ベースライン評価時の検査との撮影方法の相違や画像モダリティの変更に よる画像上の変化ではないことや、腫瘍以外の病態による画像上の変化ではないことが必要である。例えば、 肝転移巣の壊死により病巣内に生じた嚢胞性病変は新病変とはしない。ベースライン（登録前評価）にて必 須としていなかった部位の検査により新たに認められた病変は新病変とする1）。\nある病変が消失し、後に再び出現した場合には「新病変」とはせず、測定を継続する。ただし、病変が再出 現した時点での効果は、他の病変の状態により異なる。総合効果がCR 後に病変が再出現した場合は、再 出現の時点でPD と判定される。一方、総合効果がPR またはSD の場合には、一度消失した病変が再出現 した場合、その病変の径が効果を算出するために残りの病変の径和に加えられることになる。すなわち、多く の病変が残存する状態では、1 つの病変が見かけ上「消失」した後に再出現したとしても、それのみでPD と は判定せず、全病変の径和がPD の規準を満たした場合にPD と判定する。これは、大半の病変は真に「消 失」するわけではなく、使用した画像モダリティの分解能の限界によって描出されないだけであるという認識 があるためである。\n新病変である可能性があるが確定できない場合は新病変とはせず、臨床的に適切な時期を空けて画像検 査を再検する。再検した画像検査にて新病変であると確定した場合、新病変と確定した時点の画像検査日を もって新病変出現とする2）。\n\n総合効果（Overall Response）\n\n総合効果（Overall response）は標的病変の効果、非標的病変の効果、新病変出現の有無の組み合わせ から、以下の表11.1.9.a に従って8 週毎※に判定する。ベースラインで非標的病変が存在しない場合の総合 効果は、標的病変の効果と新病変出現の有無により判定し、ベースラインで標的病変が存在しない場合の総 合効果は非標的病変の効果と新病変出現の有無により表11.1.9.b に従って判定する。\n\n\n\n\n最良総合効果（Best Overall Response）（confirmation を要する場合）\n\n奏効割合や完全奏効割合がprimary endpoint である非ランダム化試験において、「最良総合効果」をPR ま たはCR とするためには、それらの確定（confirmation）が必要である。 総合効果（overall response）はCR＞PR＞SD＞PD＞NE の順に「良好」であるとし、全コース（観察期間 を設ける試験では観察期間の全コース）の総合効果から以下の規準に従って最良総合効果（Best Overall Response）を判定する。複数の区分の定義に該当する場合は、CR＞PR＞SD＞PD＞NE の順に、より良好 なものに区分する。\n\n最良総合効果（Best Overall Response）（confirmation を要さない場合）"
  },
  {
    "objectID": "posts/statistics/2025/DDEによるExcelデータの読み込み.html",
    "href": "posts/statistics/2025/DDEによるExcelデータの読み込み.html",
    "title": "DDEによるExcelデータの読み込み",
    "section": "",
    "text": "SASを使ってExcelファイルからデータを読み込む方法はいくつかありますが、今回は**DDE（Dynamic Data Exchange）**という少しレガシーながらも強力な手法をご紹介します。DDEを使えば、ExcelアプリケーションをSASから制御し、必要な範囲のデータを柔軟に読み込むことができます。これは、特に「Excelファイルを開いて特定のシートの特定のセル範囲だけを読みたい」といった場合に非常に便利です。\n\n\nDDEは、Microsoft Windowsアプリケーション間でデータを共有・交換するためのプロトコルです。SASからDDEを利用することで、Excelアプリケーションを直接操作し、ファイルを開く、特定のセル範囲を選択する、データを読み込む、ファイルを閉じる、といった一連の動作を自動化できます。\n\n\n\nDDEでExcelファイルを操作する前に、読み込みたいExcelファイルのパスとファイル名をマクロ変数で定義しておくと便利です。これは、プログラムの可読性を高め、パスの変更があった際に対応を容易にするためです。\nSAS\n%put &SettingsPath.;\n%put &&SettingsFile.;\nここでは、すでに定義されている&SettingsPath.（Excelファイルが格納されているディレクトリ）と&SettingsFile.（Excelファイル名）というマクロ変数の内容をログに出力しています。これはデバッグ時に、意図したパスとファイル名が設定されているかを確認するのに役立ちます。\n\n\n\nExcelファイルをDDEで操作するには、まずそのExcelアプリケーションを起動しておく必要があります。\n/*---------------------------*/\n/* 設定ファイルの読込         */\n/*---------------------------*/\n*--- Excel起動 ---*;\n%sysexec  \"&SettingsPath.\\&SettingsFile.\";\n\ndata _null_;\n  rc = sleep(5);\nrun;\n\n%sysexec \"&SettingsPath.\\&SettingsFile.\";: %SYSEXECマクロステートメントは、SASセッションから外部のコマンドを実行するために使用します。ここでは、指定されたパスにあるExcelファイルを直接実行しています。これにより、Excelアプリケーションが起動し、指定されたファイルが開かれます。\ndata _null_; rc = sleep(5); run;: Excelが完全に起動してファイルを開くには少し時間がかかる場合があります。SLEEP関数は、指定された秒数（この例では5秒）だけプログラムの実行を一時停止させます。これにより、SASがExcelにDDE接続する前に、Excelが準備を完了するのを待つことができます。\n\n\n\n\nExcelが起動したら、FILENAME DDEステートメントを使ってDDE接続を確立し、データを読み込みます。\n*--- Excelデータの読込 ---*;\nfilename EXC dde \"Excel|[&SettingsFile.]&SettingsSheet.!C1:C4\";\n\ndata work._SettingList;\n    length Title $100. Path $1000. FileName $500. SheetName $50.;\n    infile EXC notab dlm=\"09\"x dsd missover lrecl=1000 firstobs=2;\n    input Title Path FileName SheetName;\n\n    if Title ne \"\";\nrun;\n\nfilename EXC dde \"Excel|[&SettingsFile.]&SettingsSheet.!C1:C4\";: これがDDE接続の核心です。\n\nfilename EXC: EXCという**fileref（ファイル参照名）**を定義します。\ndde: DDEプロトコルを使用することをSASに伝えます。\n\"Excel|[&SettingsFile.]&SettingsSheet.!C1:C4\": DDEの**会話文字列（conversation string）**です。\n\nExcel: サービス名（アプリケーション名）です。\n|: サービス名とトピック名を区切ります。\n[&SettingsFile.]: トピック名（アプリケーションが提供するデータセット）です。ここでは開いているExcelファイル名を指定します。[]で囲むのがExcel DDEの慣習です。\n&SettingsSheet.!C1:C4: アイテム名（特定のデータ項目）です。&SettingsSheet.で指定されたシートのC1からC4までのセル範囲をDDE経由で読み込むことを指示しています。\n\n\ndata work._SettingList; ... run;: 通常のDATAステップとINFILEステートメントを使って、DDEで接続したExcelのデータ範囲からデータを読み込みます。\n\ninfile EXC: 上で定義したDDE接続のfileref EXCを指定します。\nnotab: タブ文字をデータの一部として扱わないようにします。\ndlm=\"09\"x: **デリミタ（区切り文字）**としてタブ文字（09xはタブの16進数表現）を指定します。Excelのセルデータは通常タブで区切られて転送されます。\ndsd: Delimited Standard Data。連続するデリミタを欠損値として扱い、引用符で囲まれた値の中のデリミタをデータの一部として扱います。\nmissover: 行の終わりに変数のデータが不足している場合、残りの変数を欠損値にします。\nlrecl=1000: 論理レコード長を1000バイトに設定します。行が長い場合に必要です。\nfirstobs=2: 読み込みを開始する行を指定します。この例ではExcelの1行目がヘッダーであるため、2行目から読み込みを開始しています。\ninput Title Path FileName SheetName;: 読み込む変数を定義します。lengthステートメントで適切な長さを設定しておくことが重要です。\nif Title ne \"\";: Titleが空でない行のみを読み込むことで、データの終端や不要な行をスキップできます。\n\n\n\n\n\nデータ読み込みが終わったら、開いたExcelファイルを閉じ、DDE接続を解放します。\n*--- Excelクローズ ---*;\nfilename SYS dde 'Excel|system';\n\ndata _null_;\nfile SYS;\nput \"[quit()]\";\nrun;\n\ndata _null_;\n    rc = sleep(3);\nrun;\n\n*--- ファイル参照を解放 ---*;\nfilename EXC clear;\nfilename SYS clear;\n\nfilename SYS dde 'Excel|system';: Excelアプリケーション全体を制御するためのDDE接続を確立します。サービス名がExcel、トピック名がsystemです。このsystemトピックを通じて、Excelにコマンドを送信できます。\ndata _null_; file SYS; put \"[quit()]\"; run;: FILE SYSでこのDDE接続に出力することで、Excelアプリケーションに対して[quit()]というコマンドを送信しています。これはExcelに終了を指示するDDEコマンドです。\ndata _null_; rc = sleep(3); run;: Excelが終了するのを待つために、再びSLEEP関数を使用します。これにより、Excelが完全に閉じられる前に次の処理が実行されるのを防ぎます。\nfilename EXC clear; filename SYS clear;: 最後に、定義したすべてのfilerefをクリアし、DDE接続を完全に解放します。これにより、リソースのリークを防ぎ、次の処理に影響を与えないようにします。\n\n\n\n\n\nGUI環境でのみ動作: DDEは、SASが稼働しているマシンにExcelがインストールされており、かつGUIモード（インタラクティブなSASセッション）で実行されている場合にのみ機能します。バッチモードやSAS Gridなどのサーバー環境では直接使用できません。\nファイルが開かれた状態: DDEで操作するExcelファイルは、SASプログラムが実行される時点で開かれていない必要があります。もし開いている場合、排他ロックによりエラーになる可能性があります。\nエラーハンドリング: DDE接続やExcel操作中にエラーが発生した場合のハンドリングが複雑になることがあります。SYSEXECやDDEの構文エラーはSASログに表示されますが、Excel内部のエラーを詳細に捕捉するのは難しい場合があります。\n代替手段: 現在では、より堅牢でクロスプラットフォームなデータ読み込み方法として、PROC IMPORT（特にDBMS=XLSXオプション）、SAS/ACCESS to PC Files、またはSAS ViyaのCASエンジン経由でのデータ読み込みが推奨されます。DDEは特定のユースケース（例：Excelのマクロ実行後データの取得）を除いて、第一選択肢とはならないかもしれません。\n\n\n\n\nDDEを使ったExcelデータの読み込みは、SASからExcelを直接制御できる強力な方法です。特に、動的に特定の範囲のデータを取得したい場合や、Excelの特定の機能を利用する必要がある場合にその真価を発揮します。ただし、その特性と注意点を理解した上で、適切に活用することが重要です。\n\n\n\n%put &SettingsPath.;\n%put &&SettingsFile.;\n\n/*---------------------------*/\n/* 設定ファイルの読込       */\n/*---------------------------*/\n*--- Excel起動 ---*;\n%sysexec  \"&SettingsPath.\\&SettingsFile.\";\n\ndata _null_;\n  rc = sleep(5);\nrun;\n\n*--- Excelデータの読込 ---*;\nfilename EXC dde \"Excel|[&SettingsFile.]&SettingsSheet.!C1:C4\";\n\ndata work._SettingList;\n    length Title $100. Path $1000. FileName $500. SheetName $50.;\n    infile EXC notab dlm=\"09\"x dsd missover lrecl=1000 firstobs=2;\n    input Title Path FileName SheetName;\n\n    if Title ne \"\";\nrun;\n\n*--- Excelクローズ ---*;\nfilename SYS dde 'Excel|system';\n\ndata _null_;\nfile SYS;\nput \"[quit()]\";\nrun;\n\ndata _null_;\n    rc = sleep(3);\nrun;\n\n*--- ファイル参照を解放 ---*;\nfilename EXC clear;\nfilename SYS clear;"
  },
  {
    "objectID": "posts/statistics/2025/DDEによるExcelデータの読み込み.html#sasでexcelデータを自在に操るddeを活用したデータ読み込み術",
    "href": "posts/statistics/2025/DDEによるExcelデータの読み込み.html#sasでexcelデータを自在に操るddeを活用したデータ読み込み術",
    "title": "DDEによるExcelデータの読み込み",
    "section": "",
    "text": "SASを使ってExcelファイルからデータを読み込む方法はいくつかありますが、今回は**DDE（Dynamic Data Exchange）**という少しレガシーながらも強力な手法をご紹介します。DDEを使えば、ExcelアプリケーションをSASから制御し、必要な範囲のデータを柔軟に読み込むことができます。これは、特に「Excelファイルを開いて特定のシートの特定のセル範囲だけを読みたい」といった場合に非常に便利です。\n\n\nDDEは、Microsoft Windowsアプリケーション間でデータを共有・交換するためのプロトコルです。SASからDDEを利用することで、Excelアプリケーションを直接操作し、ファイルを開く、特定のセル範囲を選択する、データを読み込む、ファイルを閉じる、といった一連の動作を自動化できます。\n\n\n\nDDEでExcelファイルを操作する前に、読み込みたいExcelファイルのパスとファイル名をマクロ変数で定義しておくと便利です。これは、プログラムの可読性を高め、パスの変更があった際に対応を容易にするためです。\nSAS\n%put &SettingsPath.;\n%put &&SettingsFile.;\nここでは、すでに定義されている&SettingsPath.（Excelファイルが格納されているディレクトリ）と&SettingsFile.（Excelファイル名）というマクロ変数の内容をログに出力しています。これはデバッグ時に、意図したパスとファイル名が設定されているかを確認するのに役立ちます。\n\n\n\nExcelファイルをDDEで操作するには、まずそのExcelアプリケーションを起動しておく必要があります。\n/*---------------------------*/\n/* 設定ファイルの読込         */\n/*---------------------------*/\n*--- Excel起動 ---*;\n%sysexec  \"&SettingsPath.\\&SettingsFile.\";\n\ndata _null_;\n  rc = sleep(5);\nrun;\n\n%sysexec \"&SettingsPath.\\&SettingsFile.\";: %SYSEXECマクロステートメントは、SASセッションから外部のコマンドを実行するために使用します。ここでは、指定されたパスにあるExcelファイルを直接実行しています。これにより、Excelアプリケーションが起動し、指定されたファイルが開かれます。\ndata _null_; rc = sleep(5); run;: Excelが完全に起動してファイルを開くには少し時間がかかる場合があります。SLEEP関数は、指定された秒数（この例では5秒）だけプログラムの実行を一時停止させます。これにより、SASがExcelにDDE接続する前に、Excelが準備を完了するのを待つことができます。\n\n\n\n\nExcelが起動したら、FILENAME DDEステートメントを使ってDDE接続を確立し、データを読み込みます。\n*--- Excelデータの読込 ---*;\nfilename EXC dde \"Excel|[&SettingsFile.]&SettingsSheet.!C1:C4\";\n\ndata work._SettingList;\n    length Title $100. Path $1000. FileName $500. SheetName $50.;\n    infile EXC notab dlm=\"09\"x dsd missover lrecl=1000 firstobs=2;\n    input Title Path FileName SheetName;\n\n    if Title ne \"\";\nrun;\n\nfilename EXC dde \"Excel|[&SettingsFile.]&SettingsSheet.!C1:C4\";: これがDDE接続の核心です。\n\nfilename EXC: EXCという**fileref（ファイル参照名）**を定義します。\ndde: DDEプロトコルを使用することをSASに伝えます。\n\"Excel|[&SettingsFile.]&SettingsSheet.!C1:C4\": DDEの**会話文字列（conversation string）**です。\n\nExcel: サービス名（アプリケーション名）です。\n|: サービス名とトピック名を区切ります。\n[&SettingsFile.]: トピック名（アプリケーションが提供するデータセット）です。ここでは開いているExcelファイル名を指定します。[]で囲むのがExcel DDEの慣習です。\n&SettingsSheet.!C1:C4: アイテム名（特定のデータ項目）です。&SettingsSheet.で指定されたシートのC1からC4までのセル範囲をDDE経由で読み込むことを指示しています。\n\n\ndata work._SettingList; ... run;: 通常のDATAステップとINFILEステートメントを使って、DDEで接続したExcelのデータ範囲からデータを読み込みます。\n\ninfile EXC: 上で定義したDDE接続のfileref EXCを指定します。\nnotab: タブ文字をデータの一部として扱わないようにします。\ndlm=\"09\"x: **デリミタ（区切り文字）**としてタブ文字（09xはタブの16進数表現）を指定します。Excelのセルデータは通常タブで区切られて転送されます。\ndsd: Delimited Standard Data。連続するデリミタを欠損値として扱い、引用符で囲まれた値の中のデリミタをデータの一部として扱います。\nmissover: 行の終わりに変数のデータが不足している場合、残りの変数を欠損値にします。\nlrecl=1000: 論理レコード長を1000バイトに設定します。行が長い場合に必要です。\nfirstobs=2: 読み込みを開始する行を指定します。この例ではExcelの1行目がヘッダーであるため、2行目から読み込みを開始しています。\ninput Title Path FileName SheetName;: 読み込む変数を定義します。lengthステートメントで適切な長さを設定しておくことが重要です。\nif Title ne \"\";: Titleが空でない行のみを読み込むことで、データの終端や不要な行をスキップできます。\n\n\n\n\n\nデータ読み込みが終わったら、開いたExcelファイルを閉じ、DDE接続を解放します。\n*--- Excelクローズ ---*;\nfilename SYS dde 'Excel|system';\n\ndata _null_;\nfile SYS;\nput \"[quit()]\";\nrun;\n\ndata _null_;\n    rc = sleep(3);\nrun;\n\n*--- ファイル参照を解放 ---*;\nfilename EXC clear;\nfilename SYS clear;\n\nfilename SYS dde 'Excel|system';: Excelアプリケーション全体を制御するためのDDE接続を確立します。サービス名がExcel、トピック名がsystemです。このsystemトピックを通じて、Excelにコマンドを送信できます。\ndata _null_; file SYS; put \"[quit()]\"; run;: FILE SYSでこのDDE接続に出力することで、Excelアプリケーションに対して[quit()]というコマンドを送信しています。これはExcelに終了を指示するDDEコマンドです。\ndata _null_; rc = sleep(3); run;: Excelが終了するのを待つために、再びSLEEP関数を使用します。これにより、Excelが完全に閉じられる前に次の処理が実行されるのを防ぎます。\nfilename EXC clear; filename SYS clear;: 最後に、定義したすべてのfilerefをクリアし、DDE接続を完全に解放します。これにより、リソースのリークを防ぎ、次の処理に影響を与えないようにします。\n\n\n\n\n\nGUI環境でのみ動作: DDEは、SASが稼働しているマシンにExcelがインストールされており、かつGUIモード（インタラクティブなSASセッション）で実行されている場合にのみ機能します。バッチモードやSAS Gridなどのサーバー環境では直接使用できません。\nファイルが開かれた状態: DDEで操作するExcelファイルは、SASプログラムが実行される時点で開かれていない必要があります。もし開いている場合、排他ロックによりエラーになる可能性があります。\nエラーハンドリング: DDE接続やExcel操作中にエラーが発生した場合のハンドリングが複雑になることがあります。SYSEXECやDDEの構文エラーはSASログに表示されますが、Excel内部のエラーを詳細に捕捉するのは難しい場合があります。\n代替手段: 現在では、より堅牢でクロスプラットフォームなデータ読み込み方法として、PROC IMPORT（特にDBMS=XLSXオプション）、SAS/ACCESS to PC Files、またはSAS ViyaのCASエンジン経由でのデータ読み込みが推奨されます。DDEは特定のユースケース（例：Excelのマクロ実行後データの取得）を除いて、第一選択肢とはならないかもしれません。\n\n\n\n\nDDEを使ったExcelデータの読み込みは、SASからExcelを直接制御できる強力な方法です。特に、動的に特定の範囲のデータを取得したい場合や、Excelの特定の機能を利用する必要がある場合にその真価を発揮します。ただし、その特性と注意点を理解した上で、適切に活用することが重要です。\n\n\n\n%put &SettingsPath.;\n%put &&SettingsFile.;\n\n/*---------------------------*/\n/* 設定ファイルの読込       */\n/*---------------------------*/\n*--- Excel起動 ---*;\n%sysexec  \"&SettingsPath.\\&SettingsFile.\";\n\ndata _null_;\n  rc = sleep(5);\nrun;\n\n*--- Excelデータの読込 ---*;\nfilename EXC dde \"Excel|[&SettingsFile.]&SettingsSheet.!C1:C4\";\n\ndata work._SettingList;\n    length Title $100. Path $1000. FileName $500. SheetName $50.;\n    infile EXC notab dlm=\"09\"x dsd missover lrecl=1000 firstobs=2;\n    input Title Path FileName SheetName;\n\n    if Title ne \"\";\nrun;\n\n*--- Excelクローズ ---*;\nfilename SYS dde 'Excel|system';\n\ndata _null_;\nfile SYS;\nput \"[quit()]\";\nrun;\n\ndata _null_;\n    rc = sleep(3);\nrun;\n\n*--- ファイル参照を解放 ---*;\nfilename EXC clear;\nfilename SYS clear;"
  },
  {
    "objectID": "posts/statistics/2025/ADSL_trial.html",
    "href": "posts/statistics/2025/ADSL_trial.html",
    "title": "臨床試験データ処理の実践：人口統計データ（ADSL）作成テクニック",
    "section": "",
    "text": "臨床試験において、被験者の基本情報をまとめた人口統計データ（ADSL: Analysis Data Subject Level）の作成は、全ての解析の基盤となる重要な作業です。今回は、実際の現場で使われているSASプログラミングテクニックを詳しく解説します。"
  },
  {
    "objectID": "posts/statistics/2025/ADSL_trial.html#adslデータ作成の2つのプログラム例",
    "href": "posts/statistics/2025/ADSL_trial.html#adslデータ作成の2つのプログラム例",
    "title": "臨床試験データ処理の実践：人口統計データ（ADSL）作成テクニック",
    "section": "1 ADSLデータ作成の2つのプログラム例",
    "text": "1 ADSLデータ作成の2つのプログラム例\n\n1.1 1. 人口統計データ（Demographics）の作成\ndata work.DT01 ;\n  length STUDYID $10 USUBJID $17 SUBJID ORSUBJID $6 AGE 8 AGEU $5 SEX $1 SEXN 8 RFICDTC $100 ;\n  set work.en ;\n  STUDYID = \"STUDY-XXX\" ;\n  if length(scan(_subjid, 2, '-')) = 1 then SUBJID = cats(scan(_subjid, 1, '-'), \"-0\", scan(_subjid, 2, '-')) ;\n  else SUBJID = _subjid ;\n  ORSUBJID = _subjid ;\n  USUBJID = catx(\"-\", STUDYID, SUBJID) ;\n  AGE = _age ;\n  AGEU = \"YEARS\" ;\n  if _sex = 1 then SEX = \"M\" ;\n  else if _sex = 2 then SEX = \"F\" ;\n  SEXN = _sex ;\n  RFICDTC = put(_icdat, yymmdd10.) ;\n  keep STUDYID -- RFICDTC ;\n\n\n1.2 2. プロトコルイベント（Protocol Events）データの作成\ndata work.PE01 ;\n  length SUBJID ORSUBJID $6 RFSTDTC RFXSTDTC $100 TRTSDT 8 ;\n  set work.pe ;\n  if length(scan(_subjid, 2, '-')) = 1 then SUBJID = cats(scan(_subjid, 1, '-'), \"-0\", scan(_subjid, 2, '-')) ;\n  else SUBJID = _subjid ;\n  ORSUBJID = _subjid ;\n  RFSTDTC = strip(put(_tretdat, yymmdd10.)) ;\n  RFXSTDTC = strip(put(_tretdat, yymmdd10.)) ;\n  if length(RFXSTDTC) = 10 then TRTSDT = input(RFXSTDTC, yymmdd10.) ;\n  keep SUBJID -- TRTSDT ;"
  },
  {
    "objectID": "posts/statistics/2025/ADSL_trial.html#注目すべきテクニック解説",
    "href": "posts/statistics/2025/ADSL_trial.html#注目すべきテクニック解説",
    "title": "臨床試験データ処理の実践：人口統計データ（ADSL）作成テクニック",
    "section": "2 注目すべきテクニック解説",
    "text": "2 注目すべきテクニック解説\n\n2.1 1. 被験者ID標準化の統一処理\nif length(scan(_subjid, 2, '-')) = 1 then \n    SUBJID = cats(scan(_subjid, 1, '-'), \"-0\", scan(_subjid, 2, '-')) ;\nelse SUBJID = _subjid ;\nポイント：\n\nscan関数でハイフン区切りのIDを分割\n2番目の部分が1桁の場合、ゼロパディングを実施\n「01-1」→「01-01」のような統一化\n両プログラムで完全に同じロジックを使用\n\n効果：\n\nデータの一貫性確保\n後の解析でのID照合エラー防止\n見た目の統一性向上\n\n\n\n2.2 2. 階層的ID管理システム\nORSUBJID = _subjid ;           // 元のID\nSUBJID = [標準化されたID] ;    // 標準化後のID  \nUSUBJID = catx(\"-\", STUDYID, SUBJID) ; // 試験全体でユニークなID\n設計思想：\n\nORSUBJID: 元データのトレーサビリティ確保\nSUBJID: 試験内での標準化されたID\nUSUBJID: 複数試験統合時のユニークID\n\n\n\n2.3 3. 数値・文字コードの両方保持\nif _sex = 1 then SEX = \"M\" ;\nelse if _sex = 2 then SEX = \"F\" ;\nSEXN = _sex ;\n利点：\n\nSEX: レポート表示用（M/F）\nSEXN: 統計解析用（1/2）\n用途に応じた使い分けが可能\n\n\n\n2.4 4. 日付データの多角的準備\n\n2.4.1 Demographics側：\nRFICDTC = put(_icdat, yymmdd10.) ;\nインフォームドコンセント日をISO 8601形式に変換\n\n\n2.4.2 Protocol Events側：\nRFSTDTC = strip(put(_tretdat, yymmdd10.)) ;\nRFXSTDTC = strip(put(_tretdat, yymmdd10.)) ;\nif length(RFXSTDTC) = 10 then TRTSDT = input(RFXSTDTC, yymmdd10.) ;\n巧妙な処理：\n\n同じ治療日を開始日・終了日両方に設定\nstrip関数で余分な空白を除去\n完全な日付（10桁）のみ数値変換する安全な処理\n\n\n\n\n2.5 5. CDISC準拠の変数命名\n\nSTUDYID: 試験識別子\nUSUBJID: ユニーク被験者ID\nSUBJID: 被験者ID\nRFSTDTC: 参照開始日（文字形式）\nRFXSTDTC: 参照終了日（文字形式）\nTRTSDT: 治療開始日（数値形式）"
  },
  {
    "objectID": "posts/statistics/2025/ADSL_trial.html#実用的な設計の利点",
    "href": "posts/statistics/2025/ADSL_trial.html#実用的な設計の利点",
    "title": "臨床試験データ処理の実践：人口統計データ（ADSL）作成テクニック",
    "section": "3 実用的な設計の利点",
    "text": "3 実用的な設計の利点\n\n3.1 1. 一貫性の確保\n両プログラムで同じID標準化ロジックを使用し、システム全体の整合性を保つ\n\n\n3.2 2. 用途別最適化\n\nレポート用（文字形式）\n計算用（数値形式）\n表示用（見やすい形式）\n\n\n\n3.3 3. データ品質の向上\n\n元データの保持によるトレーサビリティ\n条件分岐による安全な変換処理\n不完全データの適切な処理\n\n\n\n3.4 4. 保守性の向上\n\n統一されたコーディング規則\n変数名の体系化\n処理ロジックの標準化"
  },
  {
    "objectID": "posts/statistics/2025/ADSL_trial.html#まとめ",
    "href": "posts/statistics/2025/ADSL_trial.html#まとめ",
    "title": "臨床試験データ処理の実践：人口統計データ（ADSL）作成テクニック",
    "section": "4 まとめ",
    "text": "4 まとめ\nADSLデータの作成では、単なるデータ変換ではなく、後の解析作業全体を見据えた設計が重要です。今回紹介したテクニックにより：\n\nデータの一貫性が確保される\n国際標準に準拠したデータ構造が構築される\n効率的な解析が可能になる\nデータ品質が向上する\n\nこれらの実践的なアプローチにより、信頼性の高い臨床試験データベースの基盤を構築することができます。\n注：実際のプログラム使用の際は、各施設のSOPや規制要件に従って適切に実装してください。"
  },
  {
    "objectID": "posts/statistics/2025/ADaM1.html",
    "href": "posts/statistics/2025/ADaM1.html",
    "title": "ADaM IG1.3",
    "section": "",
    "text": "本記事では、自己学習用にADaM IGの文書を和訳する。基本的には以下の文献を読んでいただきたい。\n\n\n\nADaMIG v1.3"
  },
  {
    "objectID": "posts/statistics/2025/ADaM1.html#文献",
    "href": "posts/statistics/2025/ADaM1.html#文献",
    "title": "ADaM IG1.3",
    "section": "",
    "text": "ADaMIG v1.3"
  },
  {
    "objectID": "posts/statistics/2025/ADaM1.html#basic-data-structure-definitions",
    "href": "posts/statistics/2025/ADaM1.html#basic-data-structure-definitions",
    "title": "ADaM IG1.3",
    "section": "2.1 1.5.2 Basic Data Structure Definitions",
    "text": "2.1 1.5.2 Basic Data Structure Definitions\n\nAnalysis parameter – 共通の定義を持つ値のグループを一意に特徴づけるために使用される行識別子。ADaM解析パラメータには、関連する解析値のグループを一意に識別するために必要なすべての情報が含まれていることに注意してください。対照的に、SDTM –TEST列は、関連する値のグループを識別するために–STRESU、–POS、–LOC、–SPECなどの修飾子列と組み合わせる必要がある場合があります。本文書では、「パラメータ」という語は「解析パラメータ」の同義語として使用されています。例：主要有効性解析パラメータは「座位収縮期血圧（mmHg）」です。\nAnalysis timepoint – 解析パラメータ内の値を解析に使用される時間的または概念的グループに分類するために使用される行識別子。これらのグループ化は、観測された、計画された、または導出されたものである可能性があります。例：主要有効性解析は、第2週、第6週、およびエンドポイント解析タイムポイントで実施されました。\nAnalysis value – （1）解析パラメータによって記述される数値（AVAL）または文字（AVALC）値。解析値は、入力データに存在する、入力データ値のカテゴリ化、または導出されたものである可能性があります。例：パラメータ「体格指数」の解析値は、収集された身長と体重からADaMデータセットで導出されました。（2）さらに、特定の関数の値は解析値とみなされます。例：ベースライン値（BASE）、ベースラインからの変化量（CHG）。 Parameter-variant – AVAL（またはAVALC）の関数として導出される列で、データセットで変数が設定される一部のパラメータに対して異なって計算される場合、パラメータ可変となります。したがって、列がパラメータ可変であるのは、その導出方法が行にあるパラメータに依存する場合です。例えば、AVALCATyはAVAL（またはAVALC）をカテゴリ化します。カテゴリはパラメータによって異なる可能性があり、これによりAVALCATyがパラメータ可変となります。\nParameter-invariant – AVAL（またはAVALC）の関数として導出される列で、データセットで変数が設定されるすべてのパラメータに対して同じ方法で計算される場合、パラメータ不変となります。したがって、列がパラメータ不変であるのは、その導出方法が行にあるパラメータに依存しない場合です。パラメータ不変の導出は、適用されないパラメータに対してはnullのままにされる可能性がありますが、すべてのパラメータにわたって同じままです。例えば、ベースラインからの変化量変数の導出はCHG = AVAL-BASEであり、これはすべてのパラメータで同じ式です。したがって、CHGはパラメータ不変変数です。パラメータ不変性の概念は、BDSの整合性にとって不可欠であり、第4.2節「導出列の作成対導出行の作成」で定義されたルールの不可欠な構成要素であり、モデルが代わりに新しい行が必要であることを示している場合にプロデューサーによる「水平化」（新しい列の作成）を禁止するものです。"
  },
  {
    "objectID": "posts/statistics/2025/ADaM1.html#analysis-datasets-and-adam-datasets",
    "href": "posts/statistics/2025/ADaM1.html#analysis-datasets-and-adam-datasets",
    "title": "ADaM IG1.3",
    "section": "2.2 Analysis Datasets and ADaM Datasets",
    "text": "2.2 Analysis Datasets and ADaM Datasets\n現在、ADaMには3つの構造があります： - ADSL（SUBJECT LEVEL ANALYSIS DATASET） - BDS　（BASIC DATA STRUCTURE） - OCCDS（OCCURRENCE DATA STRUCTURE）\nADaMの基本原則およびその他のADaM規則に従うが、定義された3つの構造（ADSL、BDS、OCCDS）のいずれにも従わない解析データセットは、ADAM OTHERクラスのADaMデータセットと見なされます。解析データセットメタデータのクラス要素の統制用語は、http://www.cdisc.org/terminology でダウンロードできます。\n解析データセットのカテゴリを示す図では、最上位レベルで「Analysis Datasets」があり、その下に「ADaM Datasets」と「Non-ADaM Analysis Datasets」に分かれています。 - ADaM Datasets ADaM Datasetsは以下の4つのカテゴリに分類されます：\n\nADSL: ADSL（被験者レベル解析データセット）\nBDS: ADLB、ADEFF、ADTTE*（基本データ構造の例）\nOCCDS: ADAE、ADCM（発生データ構造の例）\nOTHER: ADMV*（その他のADaMデータセットの例）\n\n\nNon-ADaM Analysis Datasets Non-ADaM Analysis Datasetsには：PATP、AXEVT（ADaMの基本原則に従わずに開発されたデータセットの例）"
  },
  {
    "objectID": "posts/statistics/2025/ADaM1.html#fundamental-principles",
    "href": "posts/statistics/2025/ADaM1.html#fundamental-principles",
    "title": "ADaM IG1.3",
    "section": "2.3 Fundamental Principles",
    "text": "2.3 Fundamental Principles\nADaMデータセットは、ADaMモデル文書に記載されている特定の基本原則に従う必要があります：\n\nADaMデータセットおよび関連するメタデータは、臨床試験で実施される統計解析を支援するデータセットの内容とソースを明確かつ曖昧さなく伝達しなければなりません。\nADaMデータセットおよび関連するメタデータは、値または変数のソースまたは導出を示すトレーサビリティを提供しなければなりません（すなわち、データの系譜または値とその前身との関係）。メタデータは、解析データがいつ、どのように導出または補完されたかを特定しなければなりません。\nADaMデータセットは、一般的に利用可能なソフトウェアツールで容易に使用できなければなりません。\nADaMデータセットは、明確で曖昧さのないコミュニケーションを促進するためのメタデータと関連付けられなければなりません。理想的には、メタデータは機械可読であることが望ましいです。\nADaMデータセットは、最小限のプログラミングで統計解析を実行できる構造と内容を持つべきです。このようなデータセットは「Analysi Ready」と表現されます。ADaMデータセットには、特定の統計解析のレビューと再作成に必要なデータが含まれています。データリスティングやその他の非解析的な表示をサポートするためだけに、データを解析準備完了データセットに照合する必要はありません。"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "坂本航太（さかもと こうた）です。私は岡山大学病院 新医療研究開発センター データサイエンス部 統計解析室にて、生物統計家を目指して勤務しています。\n私の関心・興味は以下の通りです。"
  },
  {
    "objectID": "about.html#学歴",
    "href": "about.html#学歴",
    "title": "About",
    "section": "1 学歴",
    "text": "1 学歴\n\n2015.3 私立中央大学杉並高校卒業\n2015.4 中央大学理工学部人間総合理工学科入学\n2017.8 香港城市大学 交換留学開始\n2018.5 香港城市大学 交換留学終了\n2019.3 中央大学理工学部人間総合理工学科卒業\n2019.4 東京大学学際情報学府学際情報学専攻 生物統計情報学コース 入学\n2021.3 東京大学学際情報学府学際情報学専攻 生物統計情報学コース 修了\n2024.4 岡山大学大学院医歯薬学研究科医歯薬学専攻入学\n2028.3 岡山大学大学院医歯薬学研究科医歯薬学専攻修了予定"
  },
  {
    "objectID": "about.html#学位",
    "href": "about.html#学位",
    "title": "About",
    "section": "2 学位",
    "text": "2 学位\n\n博士（医学）（2028.3 岡山大学（取得予定））\n修士（学際情報学）（2021.3 東京大学）\n学士（理工学）（2019.3 中央大学）"
  },
  {
    "objectID": "about.html#職歴",
    "href": "about.html#職歴",
    "title": "About",
    "section": "3 職歴",
    "text": "3 職歴\n\n2021.4 - 現在 岡山大学病院新医療研究開発センター データサイエンス部 統計解析室 助教"
  },
  {
    "objectID": "about.html#資格",
    "href": "about.html#資格",
    "title": "About",
    "section": "4 資格",
    "text": "4 資格\n\n日本統計学会 統計検定1級（統計数理、統計応用（医薬生物学）） 2024.11"
  },
  {
    "objectID": "about.html#所属学会",
    "href": "about.html#所属学会",
    "title": "About",
    "section": "5 所属学会",
    "text": "5 所属学会\n\n日本計量生物学会"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kota Sakamoto",
    "section": "",
    "text": "本サイトは個人の学習記録であり、内容の正確性は保証いたしません。所属組織とは無関係の個人的見解です。\n当サイトのご利用により生じたいかなる損害・トラブルについて当サイトでは一切の責任を負いかねます事をご了承ください。\n連絡先\n本ブログ等について、誤り/疑問点がありましたら以下までご連絡ください。\n\nkota.sakamoto0514@gmail.com"
  },
  {
    "objectID": "posts/statistics/2025/ADaM作成においてDataステップにおける便利な関数.html",
    "href": "posts/statistics/2025/ADaM作成においてDataステップにおける便利な関数.html",
    "title": "ADaM作成においてDataステップにおける便利な関数",
    "section": "",
    "text": "0.1 SASプログラマ必見！ADaM作成を効率化するデータステップ重要機能6選\nこんにちは！臨床試験のSASプログラマの皆さん、日々の業務でADaMデータセットの作成に多くの時間を費やしていませんか？\nADaMの仕様は複雑で、元となるSDTMデータから多くの新しい変数を導出したり、レコードを跨いだ計算を行ったりする必要があります。こうした処理を愚直にコーディングすると、プログラムが長くなり、ミスも起こりがちです。\nしかし、SASデータステップには、こうした複雑なデータ加工を驚くほどスマートに解決するための強力な機能が備わっています。\nこの記事では、あなたのADaM作成業務を劇的に効率化する、6つの重要な関数とステートメントを厳選し、具体的な活用例とともに徹底解説します。\n\n\n0.2 【第1部】データステップの基本機能とADaM作成への応用\nまずは、データ加工の基本となる6つの機能の役割と、ADaM作成における具体的な使い方を見ていきましょう。\n\n0.2.1 1 & 2. LENGTH関数とSUBSTR関数 - 文字列操作の基本\n\nLENGTH関数: 文字列の長さを返します。\nSUBSTR関数: 文字列の一部を抜き出（抽出）します。\n\n\n0.2.1.1 実行可能なサンプルコード\nPROC TRANSPOSEで生成されるようなデータ（変数名_NAME_を持つ）から、パラメータ(PARAM)と訪問番号(VISITNUM)を分割する例です。\n/* 1. サンプルデータ作成 */\ndata transposed_data;\n  input _NAME_ $ AVAL;\n  cards;\nTC_1 212\nHDL_1 50\nTC_2 224\nHDL_2 64\n;\nrun;\n\n/* 2. LENGTHとSUBSTRを使った処理 */\ndata parsed_data;\n  set transposed_data;\n\n  /* LENGTH関数で全体の長さを取得 */\n  NAME_LEN = length(_NAME_); \n\n  /* SUBSTR関数で文字列を分割 */\n  PARAM    = substr(_NAME_, 1, NAME_LEN - 2); \n  VISITNUM = input(substr(_NAME_, NAME_LEN, 1), 8.); \n\n  drop NAME_LEN;\nrun;\n\n/* 3. 結果表示 */\ntitle \"LENGTHとSUBSTRによる変数名分割の結果\";\nproc print data=parsed_data;\nrun;\ntitle;\n\n\n\n\n0.3 3. RETAINステートメント - 値を次の行へ引き継ぐ魔法\n\n0.3.0.1 「RETAIN」とは？\nRETAINステートメントで指定された変数は、データステップのループを越えて値を保持します。 通常、変数はループの開始時に欠損値にリセットされますが、\nRETAINを使うと前の行の値を引き継ぐことができます。\n\n\n0.3.0.2 実行可能なサンプルコード\n1から5までの累積和（1, 1+2, 1+2+3, …）を計算する例です。\n/* 1. サンプルデータ作成 (入力データは不要) */\n\n/* 2. RETAINを使った処理 */\ndata sample_sum;\n  retain SUM 0; /* SUM変数の値を保持し、初期値を0に設定 */ \n\n  do i = 1 to 5;\n    SUM = SUM + i; /* 前のループのSUMに現在のiを足す */ \n    output;\n  end;\nrun;\n\n/* 3. 結果表示 */\ntitle \"RETAINによる累積和の計算結果\";\nproc print data=sample_sum;\nrun;\ntitle;\n\n\n\n0.4 4 & 5. first.by / last.by変数 - グループ処理の案内人\n\n0.4.0.1 「first.by」「last.by」とは？\nBYステートメントと一緒に使う特殊な一時変数です。 BYグループ内で、\n\nfirst.by変数: グループの最初の行である場合に 1 になります。\nlast.by変数: グループの最後の行である場合に 1 になります。\n\n\n\n0.4.0.2 実行可能なサンプルコード\nBDSデータセットで、被験者ごと、パラメータごとにレコード番号（ASEQ）を振る例です。\n/* 1. サンプルデータ作成 */\ndata ADVS;\n  input USUBJID $ PARAMCD $ AVAL;\n  cards;\nP01 HR 70\nP01 HR 72\nP01 DIABP 80\nP02 HR 65\nP02 HR 68\nP02 DIABP 75\n;\nrun;\n\n/* 2. first.byを使った処理 */\nproc sort data=ADVS;\n  by USUBJID PARAMCD;\nrun;\n\ndata ADVS_ASEQ;\n  set ADVS;\n  by USUBJID PARAMCD;\n\n  if first.PARAMCD then ASEQ = 0; /* パラメータが切り替わったらASEQを0にリセット */\n  ASEQ = ASEQ + 1;\nrun;\n\n/* 3. 結果表示 */\ntitle \"first.byによるグループ内連番(ASEQ)の作成結果\";\nproc print data=ADVS_ASEQ;\nrun;\ntitle;\n\n\n\n0.5 6. VNAME関数 - 変数名を文字列として取得\n\n0.5.0.1 「VNAME」とは？\nVNAME関数は、引数に指定した変数の名前を文字列として返します。\n\n\n0.5.0.2 実行可能なサンプルコード\nARRAYステートメントと組み合わせて、横持ちデータから縦持ちデータを作成する際に、PARAMCDを動的に生成する例です。\n/* 1. サンプルデータ作成 (横持ち) */\ndata source_data;\n  input USUBJID $ TC_1 HDL_1 TC_2 HDL_2;\n  cards;\nP01 212 50 224 64\n;\nrun;\n\n/* 2. VNAMEを使った処理 */\ndata vertical_data;\n  set source_data;\n  array aval_group[*] TC_1 HDL_1 TC_2 HDL_2; /* アスタリスク(*)で変数を指定 */\n\n  do i = 1 to dim(aval_group);\n    AVAL = aval_group{i};\n\n    /* VNAMEで変数名(例: \"TC_1\")を取得 */\n    VAR_NAME = VNAME(aval_group{i});\n\n    /* 変数名からPARAMCDとVISITNUMを動的に生成 */\n    PARAMCD  = scan(VAR_NAME, 1, '_');\n    VISITNUM = input(scan(VAR_NAME, 2, '_'), 8.);\n    \n    output;\n  end;\n  keep USUBJID PARAMCD VISITNUM AVAL;\nrun;\n\n/* 3. 結果表示 */\ntitle \"VNAMEによる動的な縦持ち変換の結果\";\nproc print data=vertical_data;\nrun;\ntitle;\n\n\n\n0.6 【第2部】さらにステップアップ！各機能の応用テクニック\n基本を理解したところで、次はいよいよ実践です。これらの機能を組み合わせることで、どのような強力な処理が実現できるか見ていきましょう。\n\n0.6.1 応用例1：RETAINとfirst.byによるグループ情報の引き継ぎ（Fill Down）\nシナリオ: ADSLにしかない被験者レベルの情報（例: 治験薬群 ARM）を、ADVSのような測定データセットの全レコードにコピーします。これにより、ADVSデータセット単体で、治験薬群による層別解析が可能になります。\n解説: MERGEでデータを結合しただけでは、ARMの値は各被験者の最初のレコードにしか入りません。そこでRETAINを使い、first.USUBJID（被験者の最初の行）のタイミングでARMの値を一度キャッチし、その値をlast.USUBJID（被験者の最後の行）まで保持し続けることで、グループ内の全レコードに値を「引き継ぐ」ことができます。\n/* ADSLとADVSをマージし、RETAINでARM情報を引き継ぐ */\ndata ADVS_with_ARM;\n  merge ADSL(keep=USUBJID ARM) ADVS(in=in_vs);\n  by USUBJID;\n\n  retain RETAINED_ARM;\n  if first.USUBJID then RETAINED_ARM = ARM;\n  ARM = RETAINED_ARM;\n  \n  if in_vs;\n  drop RETAINED_ARM;\nrun;\n\n\n0.6.2 応用例2：RETAINとfirst.byによるAUCの計算\nシナリオ: 薬物動態データ（ADPC）において、台形公式を用いて血中濃度時間曲線下面積（AUC）を算出します。これは重要な薬物動態パラメータの一つです。\n解説: 台形公式 (値1 + 値2) * (時間2 - 時間1) / 2 を計算するには、現在行の値/時間に加え、前の行の値/時間が必要です。RETAINを使って前の行の時間（PREV_ATPT）と値（PREV_AVAL）を保持します。first.byでパラメータが切り替わるタイミングを検知し、累積AUCや保持している変数を初期化することで、パラメータごとに正しく計算を実行できます。\n/* ADPCデータでAUCを計算 */\ndata ADPC_AUC;\n  set ADPC;\n  by USUBJID PARAMCD;\n\n  retain PREV_ATPT PREV_AVAL AUC_CUM;\n\n  if first.PARAMCD then do;\n    AUC_CUM = 0;\n    call missing(PREV_ATPT, PREV_AVAL);\n  end;\n\n  if not missing(PREV_AVAL) then do;\n    AUC_INTERVAL = (AVAL + PREV_AVAL) * (ATPT - PREV_ATPT) / 2;\n    AUC_CUM + AUC_INTERVAL;\n  end;\n\n  PREV_ATPT = ATPT;\n  PREV_AVAL = AVAL;\nrun;\n\n\n0.6.3 応用例3：last.byとRETAINによるグループ集計\nシナリオ: PROC MEANSやPROC SQLとMERGEを組み合わせる複数ステップの処理を、1回のDATAステップで実現し、各被験者のバイタルサイン（AVAL）の最大値を求めます。\n解説: このテクニックのキモはlast.byです。RETAINとmax関数を使い、各レコードを読み進めるごと に、その時点での最大値を計算し保持し続けます。そして、if last.USUBJID then output;とすることで、計算は全レコードで行いつつ、最終的な集計結果は各被験者の最後のレコードを処理したタイミングで一度だけ出力します。これにより、プログラムのステップを削減し、処理を効率化できます。\n/* 各被験者のバイタルサインの最大値を求める */\ndata VS_MAX;\n  set ADVS_SORTED; /* 事前にUSUBJIDでソート済み */\n  by USUBJID;\n  retain MAX_AVAL;\n\n  if first.USUBJID then MAX_AVAL = .;\n  \n  MAX_AVAL = max(MAX_AVAL, AVAL);\n  \n  if last.USUBJID then output;\nrun;\n\n\n0.6.4 応用例4：VNAMEによる汎用的なデータ品質チェック（QC）\nシナリオ: 複数の変数（AVAL, BASE, CHG）に負の値が含まれていないかチェックし、問題があれば具体的な変数名と情報をログに出力します。\n解説: データ品質チェック（QC）では、同じロジックを多くの変数に適用することが多々あります。ARRAYでチェック対象の変数をグループ化し、VNAMEでエラーが発生した変数名を動的に取得することで、保守性の高いプログラムを作成できます。もし将来、チェック対象の変数が追加されても、ARRAYステートメントに変数名を加えるだけで対応でき、ログ出力部分のコードを修正する必要がありません。\n/* 汎用的なQCプログラム */\ndata _NULL_; /* データセットを作成しない場合は _NULL_ を使う */\n  set ADVS;\n  array checks[*] AVAL BASE CHG;\n\n  do i = 1 to dim(checks);\n    if checks[i] &lt; 0 then do;\n      VAR_NAME = VNAME(checks[i]);\n      put \"ERROR: Negative value found in \" VAR_NAME= \"at OBS=\" _N_ \"for USUBJID=\" USUBJID;\n    end;\n  end;\nrun;\n\n\n\n0.7 まとめ：機能を組み合わせて、データ加工の達人へ\n今回ご紹介した6つの機能は、それぞれが強力ですが、真価は組み合わせることで発揮されます。BYステートメント、RETAIN、first.byを組み合わせれば、複雑なグループ処理が1つのデータステップで完結します。ARRAY、VNAME、SUBSTRを組み合わせれば、保守性の高い動的なプログラムが実現できます。\nこれらの武器を使いこなし、日々のADaM作成業務をより速く、より正確に進めていきましょう！\n\n\n0.8 Appendix: 応用テクニックのサンプルプログラム集\n記事の第2部で紹介した応用テクニックを、実際に動作させて試せる完全なサンプルプログラムです。\n\n0.8.1 応用例1：RETAINによるグループ情報の引き継ぎ（Fill Down）\n/* 1. サンプルデータ作成 */\ndata ADSL;\n  input USUBJID $ ARM $;\n  cards;\nP01 Drug A\nP02 Drug B\n;\nrun;\ndata ADVS;\n  input USUBJID $ VISIT $ AVAL;\n  cards;\nP01 VISIT 1 120\nP01 VISIT 2 125\nP02 VISIT 1 110\nP02 VISIT 2 112\n;\nrun;\n/* 2. メイン処理 */\nproc sort data=ADVS; by USUBJID; run;\nproc sort data=ADSL; by USUBJID; run;\ndata ADVS_with_ARM;\n  merge ADSL(keep=USUBJID ARM) ADVS(in=in_vs);\n  by USUBJID;\n  retain RETAINED_ARM;\n  if first.USUBJID then RETAINED_ARM = ARM;\n  ARM = RETAINED_ARM;\n  if in_vs;\n  drop RETAINED_ARM;\nrun;\n/* 3. 結果表示 */\ntitle \"応用例1: ARM情報を全レコードに引き継いだ結果\";\nproc print data=ADVS_with_ARM;\nrun;\ntitle;\n\n\n0.8.2 応用例2：RETAINとfirst.byによるAUCの計算\n/* 1. サンプルデータ作成 */\ndata ADPC;\n  input USUBJID $ PARAMCD $ ATPT AVAL;\n  cards;\nPK-01 AUC 0 0\nPK-01 AUC 1 50\nPK-01 AUC 2 40\nPK-01 AUC 4 20\nPK-01 AUC 8 5\n;\nrun;\n/* 2. メイン処理 */\ndata ADPC_AUC;\n  set ADPC;\n  by USUBJID PARAMCD;\n  retain PREV_ATPT PREV_AVAL AUC_CUM;\n  if first.PARAMCD then do;\n    AUC_CUM = 0;\n    call missing(PREV_ATPT, PREV_AVAL);\n  end;\n  if not missing(PREV_AVAL) then do;\n    AUC_INTERVAL = (AVAL + PREV_AVAL) * (ATPT - PREV_ATPT) / 2;\n    AUC_CUM + AUC_INTERVAL;\n  end;\n  PREV_ATPT = ATPT;\n  PREV_AVAL = AVAL;\nrun;\n/* 3. 結果表示 */\ntitle \"応用例2: AUCを計算した結果\";\nproc print data=ADPC_AUC;\nrun;\ntitle;\n\n\n0.8.3 応用例3：last.byとRETAINによるグループ集計\n/* 1. サンプルデータ作成 */\ndata ADVS_SORTED;\n  input USUBJID $ AVAL;\n  cards;\nP01 120\nP01 135\nP01 128\nP02 110\nP02 105\n;\nrun;\n/* 2. メイン処理 */\ndata VS_MAX;\n  set ADVS_SORTED;\n  by USUBJID;\n  retain MAX_AVAL;\n  if first.USUBJID then MAX_AVAL = .;\n  MAX_AVAL = max(MAX_AVAL, AVAL);\n  if last.USUBJID then output;\n  drop AVAL;\nrun;\n/* 3. 結果表示 */\ntitle \"応用例3: 各被験者の最大値を求めた結果\";\nproc print data=VS_MAX;\nrun;\ntitle;\n\n\n0.8.4 応用例4：VNAMEによる汎用的なデータ品質チェック（QC）\n/* 1. サンプルデータ作成 (意図的に負の値を含む) */\ndata ADVS_QC;\n  input USUBJID $ AVAL BASE CHG;\n  cards;\nP01 120 120 0\nP01 110 120 -10\nP02 100 105 -5\n;\nrun;\n/* 2. メイン処理 (結果はデータセットではなくログに出力) */\ntitle \"応用例4: QCチェックの結果 (SASログを確認してください)\";\ndata _NULL_;\n  set ADVS_QC;\n  array checks[*] AVAL BASE CHG;\n  do i = 1 to dim(checks);\n    if checks[i] &lt; 0 then do;\n      VAR_NAME = VNAME(checks[i]);\n      put \"ERROR: Negative value found in \" VAR_NAME= \"at OBS=\" _N_ \"for USUBJID=\" USUBJID;\n    end;\n  end;\nrun;\ntitle;"
  },
  {
    "objectID": "posts/statistics/2025/Agreement_1.html",
    "href": "posts/statistics/2025/Agreement_1.html",
    "title": "研究メモ",
    "section": "",
    "text": "Measurement Agreement Models, Methods, and Applicationsの書籍について、自己学習用にまとめる。\n書籍は以下の通り"
  },
  {
    "objectID": "posts/statistics/2025/Agreement_1.html#measurement-agreement",
    "href": "posts/statistics/2025/Agreement_1.html#measurement-agreement",
    "title": "研究メモ",
    "section": "",
    "text": "Measurement Agreement Models, Methods, and Applicationsの書籍について、自己学習用にまとめる。\n書籍は以下の通り"
  },
  {
    "objectID": "posts/statistics/2025/English.html",
    "href": "posts/statistics/2025/English.html",
    "title": "大人の英語学習勉強1",
    "section": "",
    "text": "本業がある社会人こそ、英文法を利用することで効率良く英語を使えるようになります。交渉での細かいニュアンスには、「時制・助動詞」の力が絶対に必要になりますし、複雑な説明をするときには「比較・関係詞」を駆使することになります。英文法が崩れたプレゼンテーションはもやは聞いてもらえないでしょう。\n「大人の会話」であるだけに、中学英文法では足りず、高校レベルまでの英文法の全範囲が必須となります。何より、きちんとした英文法を使うことは、ビジネスにおいての「信用」につながります！\n私にとっては、研究者としての立場もあり、論文投稿に加えて、「論文を読む」という動作において、正しく著者の考えを理解するために、「英文法」は必須です。基礎を固めておきましょう。\nここでは、高校英文法をひとつひとつ分かりやすく。という本を基に勉強していく！"
  },
  {
    "objectID": "posts/statistics/2025/English.html#関先生の真英文法の目次",
    "href": "posts/statistics/2025/English.html#関先生の真英文法の目次",
    "title": "大人の英語学習勉強1",
    "section": "2.1 関先生の真英文法の目次",
    "text": "2.1 関先生の真英文法の目次\n\n時制\n\n現在形と過去形\n時制の細かい用法\n進行形\n\n時制\n\n現在完了形\n過去完了形\n未来完了形（will have p.p.）\n現在完了形の応用\n\n接続詞\n\n等位接続詞（and型）\n従属接続詞（when/if型）\n名詞節もつくる従属接続詞\n時・条件を表す副詞節内での「現在形の特別用法」\n\n仮定法\n\n仮定法の公式\n公式通りではない仮定法\n仮定法の「慣用表現」\n\n助動詞\n\n基本助動詞の詳説\n助動詞の過去形\nmayの慣用表現、shouldの特別用法\nその他の助動詞と助動詞相当表現\n助動詞 vs 代表表現\n助動詞＋have p.p.\n\n冠詞\n\n定冠詞 the\n不定冠詞 a\n\n名詞\n\n可算名詞\n不可算名詞\n可算・不可算の感覚を磨く\n「集合」という感覚を持つ名詞\n\n代名詞\n\n人称代名詞\nit/one/otherなど\nその他の代名詞\n\n形容詞\n\n形容詞の用法\n「意味の識別」が問われる形容詞\n\n副詞\n\n副詞という「品詞」の意識\n「位置」が狙われる副詞\n「まぎらわしい副詞」の判別\n「場所・時間の副詞」に関する副詞の特別用法\n\n文型\n\n動詞の詳述\n第1・2・3文型\n第4文型\n第5文型\n\n不定詞\n\n不定詞の基本3用法\n不定詞の様々な形\n不定詞を使った慣用表現\ntoが持つ「未来志向性」\n\n動名詞\n\n動名詞の基本\n目的語にくる”to” vs “ing”\n動名詞の慣用表現"
  },
  {
    "objectID": "posts/statistics/2025/English.html#英語の基本ルール",
    "href": "posts/statistics/2025/English.html#英語の基本ルール",
    "title": "大人の英語学習勉強1",
    "section": "2.2 英語の基本ルール",
    "text": "2.2 英語の基本ルール\n英語の文章を作るパーツ"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html",
    "title": "How to check the simulation study",
    "section": "",
    "text": "以下の論文を基にRでのシミュレーション研究の方法を学ぶ\n\n\n\n\n\n\nHow to check a simulation study Open Access\n\n\n\nIan R White , Tra My Pham , Matteo Quartagno , Tim P Morris\nInternational Journal of Epidemiology, Volume 53, Issue 1, February 2024, dyad134, https://doi.org/10.1093/ije/dyad134"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#論文紹介",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#論文紹介",
    "title": "How to check the simulation study",
    "section": "",
    "text": "以下の論文を基にRでのシミュレーション研究の方法を学ぶ\n\n\n\n\n\n\nHow to check a simulation study Open Access\n\n\n\nIan R White , Tra My Pham , Matteo Quartagno , Tim P Morris\nInternational Journal of Epidemiology, Volume 53, Issue 1, February 2024, dyad134, https://doi.org/10.1093/ije/dyad134"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#abstract",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#abstract",
    "title": "How to check the simulation study",
    "section": "0.2 Abstract",
    "text": "0.2 Abstract\n\n\n\n\n\n\nノート\n\n\n\nSimulation studies are powerful tools in epidemiology and biostatistics, but they can be hard to conduct successfully. Sometimes unexpected results are obtained. We offer advice on how to check a simulation study when this occurs, and how to design and conduct the study to give results that are easier to check. Simulation studies should be designed to include some settings in which answers are already known. They should be coded in stages, with data-generating mechanisms checked before simulated data are analysed. Results should be explored carefully, with scatterplots of standard error estimates against point estimates surprisingly powerful tools. Failed estimation and outlying estimates should be identified and dealt with by changing data-generating mechanisms or coding realistic hybrid analysis procedures. Finally, we give a series of ideas that have been useful to us in the past for checking unexpected results. Following our advice may help to prevent errors and to improve the quality of published simulation studies."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#本文",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#本文",
    "title": "How to check the simulation study",
    "section": "0.3 本文",
    "text": "0.3 本文\n\nWe can divide the execution of a simulation study into three stages:\n\n‘design’ (identifying the aims, data-generating mechanisms, estimands, methods of analysis and performance measures)\n‘conduct’ (writing the code to simulate multiple data sets and analyse each one, yielding a data set of estimates);\n‘analysis’ (computing the performance measures from the estimates data set)."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#table-1.-terms-used-in-simulation-studies",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#table-1.-terms-used-in-simulation-studies",
    "title": "How to check the simulation study",
    "section": "0.4 Table 1. Terms used in simulation studies",
    "text": "0.4 Table 1. Terms used in simulation studies\n\n\n\n\n\n\n\nTerm\nExplanation\n\n\n\n\nAspects of a simulation study\n\n\n\nAims\nWhat question(s) the simulation study addresses\n\n\nData-generating mechanisms\nHow the simulated data sets are to be generated\n\n\nEstimands\nThe quantity or quantities to be estimated by the analysis of each simulated data set\n\n\nMethods of analysis\nHow the simulated data sets are to be analysed: typically producing a point estimate and a CI\n\n\nPerformance measures\nHow the performance of the methods of analysis is to be summarized\n\n\nImplementation\nHow the simulation is to be performed, including the software used, the number of repetitions and the handling of random number states\n\n\nData sets involved in a simulation study\n\n\n\nSimulated data set\nA data set produced by one of the data-generating mechanisms in one repetition\n\n\nEstimates data set\nA data set containing results of each method of analysis for each simulated data set across many repetitions, used to estimate performance\n\n\nStates data set\nA data set containing random number states for each simulated data set, that can be used to recreate any simulated data set\n\n\nPerformance measures data set\nA data set containing the estimated performance measures for each data-generating mechanism and each method of analysis\n\n\nSome performance measures\n\n\n\nBias\nHow the mean point estimate differs from the true estimand value\n\n\nEmpirical standard error\nThe standard deviation of the point estimates in an estimates data set\n\n\nModel-based standard error\nThe average^ standard error estimate in an estimates data set\n\n\nRelative error in model-based standard error\nThe difference between the model-based standard error and the empirical standard error, expressed as a fraction of the latter\n\n\nCoverage\nThe proportion of CIs that include the true estimand value\n\n\n\n^ Strictly, the root mean square of the standard error estimates.\n\n\n\n\n\n\n\nAim\nTo compare multiple imputation with complete case analysis\n\n\n\n\nData-generating methods\nQuantitative confounder C is drawn from a standard Normal distribution. Binary exposure E and binary outcome D are drawn from logistic models depending on C (so E does not cause D). Values of C are made missing, initially using a missing completely at random model. Parameters to be varied are the marginal probabilities of E and D, the strength of the dependence of E and D on C, and the missing data mechanism. The sample size of 500 is fixed.\n\n\nEstimand\nThe log odds ratio between E and D, conditional on C. Its true value is zero.\n\n\nMethods\nLogistic regression of D on E and C, using:\ni) full data before data deletion in C\nii) complete cases (excluding cases with missing C)\niii) multiply imputed data to handle missing values of C—various imputation models may be used.\n\n\nPerformance measures\n・Bias\n・empirical standard error\n・ relative error in model-based standard error\n・ coverage.\n\n\nImplementation\n1000 repetitions advice on choosing this is available.\n\n\n\n^ Specific values used in the data-generating mechanisms can be found in the code"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#include-a-setting-with-known-properties",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#include-a-setting-with-known-properties",
    "title": "How to check the simulation study",
    "section": "1.1 (1) include a setting with known properties",
    "text": "1.1 (1) include a setting with known properties\nWe frequently do simulation studies to learn the properties of methods of analysis, but often we can identify a particular data-generating mechanism and a particular method of analysis where we know some aspect of the answer. This requires knowledge of the statistical properties of the methods. We can then check our results against these known answers.\nExample:\nIn the multiple imputation simulation study, we included analysis of the full data before data deletion. Assuming the analysis procedure is correct, we expect this to be unbiased with correct coverage and more precise than any methods of analysis of the incomplete data. We also include a complete case analysis in a scenario with data missing completely at random, which is expected to be unbiased with correct coverage and less precise than multiple imputation methods of analysis. The above statements about lack of bias are not strictly correct, since logistic regression is biased in small samples. We could therefore include a further setting with a larger sample size.\nコメント：\nlogistic regressinモデルが最尤推定量の漸近一致性しかないことを理解しておかないといけない。"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#write-well-structured-code",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#write-well-structured-code",
    "title": "How to check the simulation study",
    "section": "2.1 (2) Write well-structured code",
    "text": "2.1 (2) Write well-structured code\nIt is helpful to write code that separates data generation, data analysis and computation of performance measures, so that each part can be studied separately. Code should be well commented to help collaborators and the coder’s future self; the final code should later be published and comments will then help the general reader.\nExample\nThe Stata script simcheck02.do and the R script simcheck02.R separate out data generation and data analysis. We will add the calculation of performance measures later."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#study-a-single-very-large-data-set",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#study-a-single-very-large-data-set",
    "title": "How to check the simulation study",
    "section": "2.2 (3) Study a single very large data set",
    "text": "2.2 (3) Study a single very large data set\nThe code for data generation should next be used to generate a single very large data set. Viewing descriptive statistics (e.g. histogram of a continuous outcome, cross-tabulation of exposure by outcome event) allows a check that the data match one’s intentions. In many cases, the model used to generate the data can be directly fitted to the simulated data. This should recover the parameters of the data-generating model such that CIs for the results usually include the known true values. Then the simulation code should be used to do the analyses.The results should be carefully checked for correct back- ground information (e.g. is the number of observations correct? ) and for credibility. Again, where methods are known to be correct, CIs for the results should usually include the known true values.\nExample:\nThe scripts simcheck03.do and simcheck03.R generate a sin- gle data set of size 100 000 using a particular data-generating\nmechanism. They include standard descriptive statistics such as a cross-tabulation of D against E, showing that there is an unconditional association between D and E (log odds ratio 1⁄4 0.74 in Stata and 0.70 in R). This is important because one way in which imputation procedures could perform badly is by failing to control for confounding. The scripts also show that the logistic regressions of E and D on C have coefficient values very near to the values in the data-generating mechanism and that the analysis program runs successfully."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#run-the-simulation-with-a-small-number-of-repetitions",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#run-the-simulation-with-a-small-number-of-repetitions",
    "title": "How to check the simulation study",
    "section": "2.3 (4) Run the simulation with a small number of repetitions",
    "text": "2.3 (4) Run the simulation with a small number of repetitions\nThe simulation code should next be checked with a small number of repetitions: three repetitions are often enough at this stage. The screen output should be switched on so that it can be studied. The size of the simulated data set should be checked for each repetition. It is useful to verify that the second and third repetitions produce different data and results. Sometimes simulation code wrongly sets the random number state after starting the first repetition. If (for example) this is at the end of a repetition, then the second and third repetitions would produce identical data and results.\nThis is the first time that we have created an estimates data set so it is timely to check that the estimates data set has the right structure, is indexed with the correct simulation repetition number and contains values that match the values reported in the screen output. For example, sometimes users store a variance when they meant to store a standard error estimate. Confusion can arise if screen output displays exponentiated parameters (odds ratios) but estimates are stored on the estimation scale (log odds ratios).\nExample\nThe scripts simcheck04.do and simcheck04.R run three repe- titions of the simulation. The results look sensible and match the screen output."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#anticipate-analysis-failures",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#anticipate-analysis-failures",
    "title": "How to check the simulation study",
    "section": "2.4 (5) Anticipate analysis failures",
    "text": "2.4 (5) Anticipate analysis failures\nIf a certain method of analysis can be anticipated to cause an error in some simulated data sets (e.g. perfect prediction in a logistic regression), the code should be written to capture the error so that the simulation does not halt. The method failure (with error code) should be stored in the estimates data set. Strategies for handling failures are described in later points.\nExample\nThe scripts simcheck05.do and simcheck05.R recognize that either the imputation step or the model fitting to the imputed data may fail. They therefore detect either of these failures and post missing values to the estimates data set."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#make-it-easy-to-recreate-any-simulated-data-set",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#make-it-easy-to-recreate-any-simulated-data-set",
    "title": "How to check the simulation study",
    "section": "2.5 (6) Make it easy to recreate any simulated data set",
    "text": "2.5 (6) Make it easy to recreate any simulated data set\nThe estimates data set should include an identifier for the simulated data set alongside every estimate. If we can recreate the simulated data set for any particular identifier, then we can explore method failures and outliers (see points below). There are two ways to do this. One way is to store the random number state at the start of each data generation in a states data set so that the user can recreate any simulated data set. The alternative is to store every simulated data set.\nSome analyses, such as multiple imputation, also use random numbers. Recreating such analyses requires resetting the appropriate random number state. One way is to recreate the simulated data set (as above) and then repeat the analysis; if multiple analyses use random numbers then they must all be repeated in the original order. The alternative is to store the random number state at the start of each stochastic analysis.\nExample\nThe scripts simcheck06.do and simcheck06.R store the random number states in a separate file. They then show how to reconstruct the third simulated data set."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#count-and-understand-method-failures.",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#count-and-understand-method-failures.",
    "title": "How to check the simulation study",
    "section": "3.1 (7) Count and understand method failures.",
    "text": "3.1 (7) Count and understand method failures.\nFor each data-generating mechanism and method, the user should identify what fraction of repetitions led to failed esti mation. The reasons for failed estimation need to be under- stood and the code should be improved if possible. That is, a bad optimization routine should be improved if it makes a method appear to fail. Unexpected results may be specific to particular software or packages.\nExample\nThe scripts simcheck07.do and simcheck07.R use a different data-generating mechanism from previous runs. Inspecting the estimates data set shows four method failures in Stata and two in R. On closer exploration of the simulated data sets, we find that these data are very sparse, having either no exposed individuals or no outcome events in the individuals with observed C (a random positivity violation that may not be intended), and this is causing the complete case analysis to fail. The R function glm behaves differently and returns estimates even in the absence of outcome events. The sensible conclusion (in our setting in which positivity violations are not of main interest) is that the data-generating mechanism is too extreme and should be changed to generate"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#look-for-outliers",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#look-for-outliers",
    "title": "How to check the simulation study",
    "section": "3.2 (8) Look for outliers",
    "text": "3.2 (8) Look for outliers\nIt is important to examine the estimates data set carefully. A useful visual device is a scatter plot of the standard error estimate against the point estimate over all repetitions, separated by the data-generating mechanism and method. This scatter plot can identify the presence of outliers. Estimates can be outliers for the point estimate or the model-based standard error (or both). Such outliers are frequent causes, respectively, of unexpected bias and of unexpected error in the model- based standard error. A small number of outliers by them- selves do not affect coverage and often researchers are puzzled by, for example, a model-based standard error being apparently very large without any impact on coverage.\nExample\nThe scripts simcheck08.do and simcheck08.R explore the estimates data sets produced by simcheck07.do and simcheck07.R. They plot the standard error estimates against the point estimates by method of analysis. Results differ be- tween the packages. In Stata (Figure 2, upper part), a substantial number of data sets have standard error estimates equal to zero, which indicates a problem with the analysis. Further inspection shows these data sets also have estimated coefficients equal to zero. In R (Figure 2, lower part), a substantial number of data sets have very large standard error estimate (2000–5000). These also have large point estimates, mostly between –10 and –20, some near þ20. We could change these outlying standard error estimates and their associated point estimates to missing values, but it is more important to understand their cause. We do this next."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#understand-outliers.",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#understand-outliers.",
    "title": "How to check the simulation study",
    "section": "3.3 (9) Understand outliers.",
    "text": "3.3 (9) Understand outliers.\nIf outliers are found, it is helpful to open or recreate one or more of the corresponding simulated data sets. The user should verify the outlying estimate and explore the reasons for it, such as by checking details of the analysis output and by supplementary analysis such as exploring model residuals or imputed data values.\nExample\nScripts simcheck09.do and simcheck09.R each pull out one particular problem data set identified in point (viii). In both cases, the problem data set has no events among individuals with observed confounders. The different outputs from the two software packages (Figure 2) arise from the packages’ dif- ferent handling of this problem. Stata has detected perfect prediction,3 has dropped the exposure variable E from the model and has reported zero values for the point estimate and its standard error estimate. R has performed estimation regardless, has found the parameter estimate going towards plus or minus infinity without achieving convergence and has reported values when approximate convergence is achieved. Solutions to these problems are discussed next."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#deal-with-outliers",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#deal-with-outliers",
    "title": "How to check the simulation study",
    "section": "3.4 (10) Deal with outliers",
    "text": "3.4 (10) Deal with outliers\nAny outliers are likely to strongly affect estimates of performance. It may be appropriate to change the data-generating mechanism to avoid outlying estimates. Otherwise, if outlying estimates would not be believed or reported in practice—that is, if the issue would be detected and the method of analysis not used—then they should not be included in the analysis of the simulation results. One way to do this is to exclude simulated data sets that result in outlying estimates. However, this can introduce a selection bias because the excluded simulated data sets are unlikely to be representative. An alternative is to code an automatic ‘backup procedure’ when a method returns an absurd result. This changes the method being investigated from a pure method to a hybrid procedure and it should make performance measures more relevant to practice. Alongside any of these approaches, the number of method failures or outliers is a useful additional performance measure.\nExample\nOne way to avoid the problems seen in the multiple imputation simulation study is to increase the proportion of exposed away from the sparse case seen. We adopt this approach in the remaining points. Alternatively, if we were interested in the sparse case, we should ask whether outlying estimates might make sense in practice. An analyst might accept a log odds ratio of –15 and report an estimated odds ratio of zero, which implies either no events in the exposed group or no non-events in the unexposed. However, they should certainly not accept the very wide 95% CI. Instead they would probably use exact methods to generate a more correct CI. We could therefore code such exact methods into our simulation study as a backup procedure if extreme estimates are found. A different way to fix the analysis, and a more convenient solution for the simulation study, is to handle perfect prediction by using penalized logistic regression.5 This could be done as a backup procedure in analyses exhibiting a problem or in all analyses. The latter is illustrated in simcheck10.do and simcheck10.R."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#check-monte-carlo-errors",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#check-monte-carlo-errors",
    "title": "How to check the simulation study",
    "section": "4.1 (11) Check Monte Carlo errors",
    "text": "4.1 (11) Check Monte Carlo errors\nSometimes, some simulation findings are hard to believe: for example, a method selected to be unbiased appears to be biased or one method appears to be more precise than another when it should be less precise. In this case, it is important to look at the Monte Carlo errors and decide whether the findings are compatible with Monte Carlo error.\nExample\nThe scripts simcheck11.do and simcheck11.R are our first complete runs of a simulation study avoiding sparse data. Results for bias suggest a larger bias in complete case analysis (e.g. in Stata –0.094) than in the other methods (–0.068 full data, ＋0.063 multiple imputation). Given that we are simulating under missing completely at random, we expect complete case analysis to be unbiased. Instead of concluding that our code is wrong, we should spot that these results are compatible with Monte Carlo error—that is, the observed bias for complete case analysis is &lt;2 Monte Carlo standard errors (0.049 ×2 = 0.098) and hence is perfectly compatible with zero bias. In fact these results were produced with just 100 repetitions. To detect a bias of this magnitude, more repetitions (say 1000) are needed."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#why-are-model-based-standard-errors-wrong",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#why-are-model-based-standard-errors-wrong",
    "title": "How to check the simulation study",
    "section": "4.2 (12) Why are model-based standard errors wrong?",
    "text": "4.2 (12) Why are model-based standard errors wrong?\nIf model-based standard errors disagree with the empirical standard errors, it is worth considering whether the sources of variation in the data-generating mechanism and analysis correspond. For example, in a missing data simulation study, if each repetition under a given data-generating mechanism starts from the same full data set, then uncertainty due to the full data set will be reflected in the model-based standard error but not in the empirical standard error, making them not comparable regardless of the analysis used.\nExample\nThe scripts simcheck12.do and simcheck12.R demonstrate this issue. Previous scripts drew a new full data set for each repetition, but these scripts create each simulated data set by deleting values from the same full data set. The model-based standard errors (e.g. in Stata 0.52 and 0.46 for complete case analysis and multiple imputation, respectively) are found to be substantially larger than the empirical standard errors (0.27 and 0.17). This is because the model-based standard errors account for sampling variation in the full data set whereas the sampling variation in the full data set does not exist in the simulation study so is not reflected in the empirical standard error. One solution here is to generate a new full data set for each repetition."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#why-is-coverage-poor",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#why-is-coverage-poor",
    "title": "How to check the simulation study",
    "section": "4.3 (13) Why is coverage poor?",
    "text": "4.3 (13) Why is coverage poor?\nIf coverage is poor, it is helpful to identify whether it is driven by bias, by intervals of the wrong width or both. Zip plots are a useful visualization devices for this purpose. They plot each interval, ordered according to compatibility with the true value, giving the impression of a ‘zip’ (or ‘zipper’).1"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#why-do-unexpected-findings-occur",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#why-do-unexpected-findings-occur",
    "title": "How to check the simulation study",
    "section": "4.4 (14) Why do unexpected findings occur?",
    "text": "4.4 (14) Why do unexpected findings occur?\nSomewhat different errors can arise when focusing on test characteristics. Some pitfalls are to interchange power and type I error, or to mistake one-sided and two-sided type I tests."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#when-do-unexpected-findings-occur",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#when-do-unexpected-findings-occur",
    "title": "How to check the simulation study",
    "section": "4.5 (15) When do unexpected findings occur?",
    "text": "4.5 (15) When do unexpected findings occur?\nIf some findings remain hard to believe, it may be helpful to ask under what settings these findings occur. For example, if they occur only when the data-generating mechanism includes a particular source of variation, then maybe analyses are not allowing for this source of variation."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#general-checking-method",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#general-checking-method",
    "title": "How to check the simulation study",
    "section": "4.6 (15) General checking method",
    "text": "4.6 (15) General checking method\nIf after the above steps the results of the simulation study are still in doubt, it can be useful to recode the simulation study in a different statistical package or have a different person code it. Sometimes another closely related simulation study that has been published with code is helpful. The user should first check that the published code does reproduce the published results. They can then change the published data-generating mechanisms to match those in the current simulation study (as closely as possible), run them and com- pare the results. Alternatively they can change their own data- generating mechanism parameters to match the published ones (as closely as possible) and run them and compare the results. This should help to narrow down where any errors are occurring. However, if code has been carefully checked and still gives an unexpected result, it is important to consider that the findings may be genuine showing that the theory may be wrong."
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simulation-studyの結果の表の見せ方",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simulation-studyの結果の表の見せ方",
    "title": "How to check the simulation study",
    "section": "4.7 Simulation Studyの結果の表の見せ方",
    "text": "4.7 Simulation Studyの結果の表の見せ方"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#doall.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#doall.r",
    "title": "How to check the simulation study",
    "section": "5.1 doall.R",
    "text": "5.1 doall.R\n# simcheck: run all R files\n# IW 6sep2023\n\n# run in the \"R\" directory\n\n# R packages needed are: \n#   foreign, boot, mice, foreach, dplyr, ggplot2, logistf, miceafter, rsimsum\n\n# do\nsource(\"simcheck02.R\")\nsource(\"simcheck03.R\")\nsource(\"simcheck04.R\")\nsource(\"simcheck05.R\")\nsource(\"simcheck06.R\")\nsource(\"simcheck07.R\")\nsource(\"simcheck08.R\")\nsource(\"simcheck09.R\")\nsource(\"simcheck10.R\")\nsource(\"simcheck11.R\")\nsource(\"simcheck12.R\")\nsource(\"simcheck99.R\")\n最後はまとめて一括実行できるように準備する。\n実施に必要なパッケージも記載されている。"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck02.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck02.r",
    "title": "How to check the simulation study",
    "section": "5.2 simcheck02.r",
    "text": "5.2 simcheck02.r\n###################################\n### simcheck02.R: script for    ###\n### simulating and analysing    ###\n### data.                       ###\n###################################\n\n# Load relevant libraries\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\n\n# First, we write a function to generate a single data set:\n\n#比較演算子\nD &lt;- runif(10) &lt; inv.logit(eval(parse(text=\"-1\")))\n\ngendata &lt;- function( obs, logite, logitd, pmiss ) {\n  \n  Ctrue &lt;- rnorm(obs)\n  E &lt;- runif(obs) &lt; inv.logit(eval(parse(text=logite)))\n  D &lt;- runif(obs) &lt; inv.logit(eval(parse(text=logitd)))\n  Cobs&lt;-Ctrue\n  Cobs[runif(obs)&lt;pmiss]&lt;-NA\n  data.out&lt;-data.frame(Cobs,D,E,Ctrue)\n  \n  return(data.out)  # for clarity, but redundant \n  \n}\n\n# Next we provide a function to analyse partially observed data.set:\n\nanadata &lt;- function( dataframe, rep) {\n  \n  # Method 1: full data before data deletion  \n  fit.fd&lt;-glm(D~E+Ctrue, family=binomial(link=\"logit\"), data=dataframe, singular.ok=F, epsilon = 1e-14)\n  res&lt;-data.frame(\n    rep &lt;- rep,\n    method &lt;- \"Full\",\n    est &lt;- coef(fit.fd)[\"ETRUE\"],\n    se &lt;- coef(summary(fit.fd))[\"ETRUE\",2],\n    N &lt;- nobs(fit.fd),\n    df &lt;- NA, # df is only needed for MI but must be included for all\n  row.names = NULL)\n  \n  # Method 2: CCA \n  fit.cca&lt;-glm(D~E+Cobs, family=binomial(link=\"logit\"), data=dataframe, singular.ok=F, epsilon = 1e-14)\n  res&lt;-rbind(res,c(\n   rep,\n    \"CCA\",\n    coef(fit.cca)[\"ETRUE\"],\n    coef(summary(fit.cca))[\"ETRUE\",2],\n    nobs(fit.cca),\n    NA # df is only needed for MI but must be included for all\n    ),\n  row.names=NULL)\n  \n  # Method 3: MI \n  df.mice&lt;-dataframe[,c(\"Cobs\", \"D\", \"E\")]\n  df.mice$int&lt;-df.mice$D*df.mice$E\n  imp &lt;- mice(df.mice, method = \"norm\", m = 5, printFlag = F)\n  fit &lt;- with(data = imp, exp = glm(D~E+Cobs, family=binomial(link=\"logit\"), singular.ok=F, epsilon = 1e-14))\n  rub.rul&lt;-summary(pool(fit))\n  res&lt;-rbind(res,c(\n    rep,\n    \"MI\",\n    rub.rul[rub.rul$term==\"ETRUE\",\"estimate\"],\n    rub.rul[rub.rul$term==\"ETRUE\",\"std.error\"],\n    nobs(fit.fd),\n    rub.rul[rub.rul$term==\"ETRUE\",\"df\"]),\n  row.names=NULL)\n  \n  colnames(res) &lt;- c( \"rep\", \"method\", \"est\", \"se\", \"N\", \"df\")\n  return(res)\n  \n}"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck3.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck3.r",
    "title": "How to check the simulation study",
    "section": "5.3 simcheck3.r",
    "text": "5.3 simcheck3.r\n###################################\n### simcheck03.R: Simulate      ###\n### and analyse a single data   ###\n### set.                        ###\n###################################\n\n# Load relevant libraries and source simcheck02.R\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\nsource(\"simcheck02.R\")\n\n# Set seed\nset.seed(576819506)\n\n# generate a single large data set\ndataframe &lt;- gendata( obs = 100000, logite = \"-1+Ctrue\", logitd = \"-1+Ctrue\", pmiss = 0.3)\n\n# summarise the data\nsummary(dataframe)\naddmargins(table(dataframe[is.na(dataframe$Cobs),\"D\"],dataframe[is.na(dataframe$Cobs),\"E\"])) \naddmargins(table(dataframe[!is.na(dataframe$Cobs),\"D\"],dataframe[!is.na(dataframe$Cobs),\"E\"])) \nfit.unad &lt;- glm(D~E, data=dataframe, family=binomial(link=\"logit\"))\nsummary(fit.unad)\nconfint(fit.unad)\n\n# fit the data generating models\nglm(E~Ctrue, data=dataframe, family=binomial(link=\"logit\"))\nglm(D~Ctrue, data=dataframe, family=binomial(link=\"logit\"))\n\n# analyse the data\nresults &lt;- anadata(dataframe, 1)\nresults"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck04.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck04.r",
    "title": "How to check the simulation study",
    "section": "5.4 simcheck04.r",
    "text": "5.4 simcheck04.r\n###################################\n### simcheck04.R: Run a few     ###\n### iterations.                 ###\n###################################\n\n# Load relevant libraries and source simcheck02.R\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\nlibrary(foreach) # for using foreach loop\nsource(\"simcheck02.R\")\n\n# Set seed\nset.seed(576819506)\n\n# run 3 repetitions\nresults &lt;- foreach( i = 1:3, .combine=\"rbind\") %do% {\n  dataframe &lt;- gendata( obs = 500, logite = \"-1+Ctrue\", logitd = \"-1+Ctrue\", pmiss = 0.3)\n  anadata(dataframe, i)\n\n}\n\n# view results\n# Vies関数をかましておくことで、チェックができる！\nView(results)"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck05.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck05.r",
    "title": "How to check the simulation study",
    "section": "5.5 simcheck05.r",
    "text": "5.5 simcheck05.r\n###################################\n### simcheck05.R: script for    ###\n### anticipating analysis       ###\n### failures.                   ###\n###################################\n\n# Load relevant libraries\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\n\n# First, we write a function to generate a single data set:\n\ngendata &lt;- function( obs, logite, logitd, pmiss ) {\n  \n  Ctrue &lt;- rnorm(obs)\n  E &lt;- runif(obs) &lt; inv.logit(eval(parse(text=logite)))\n  D &lt;- runif(obs) &lt; inv.logit(eval(parse(text=logitd)))\n  Cobs&lt;-Ctrue\n  Cobs[runif(obs)&lt;pmiss]&lt;-NA\n  data.out&lt;-data.frame(Cobs,D,E,Ctrue)\n  \n  return(data.out)  # for clarity, but redundant \n  \n}\n\n# Next we provide a function to analyse partially observed data.set anticipating possible failures:\n\nanadata &lt;- function( dataframe, rep, print.output=F) {\n  \n  # Method 1: full data before data deletion  \n  fit.fd&lt;-try(glm(D~E+Ctrue, family=binomial(link=\"logit\"), data=dataframe, singular.ok=F))\n  if (print.output) print(summary(fit.fd)) # To print output when checking on few iterations\n  if (!inherits(fit.fd, \"try-error\")) {\n    res&lt;-data.frame(\n      rep &lt;- rep,\n      method &lt;- \"Full\",\n      est &lt;- coef(fit.fd)[\"ETRUE\"],\n      se &lt;- coef(summary(fit.fd))[\"ETRUE\",2],\n      N &lt;- nobs(fit.fd),\n      df &lt;- NA, # df is only needed for MI but must be included for all\n      row.names = NULL)\n  } else {\n    res&lt;-data.frame(\n      rep &lt;- rep,\n      method &lt;- \"Full\",\n      est &lt;- NA,\n      se &lt;- NA,\n      N &lt;- NA,\n      df &lt;- NA, # df is only needed for MI but must be included for all\n      row.names = NULL)\n    \n  }\n  \n  # Method 2: CCA \n  fit.cca&lt;-try(glm(D~E+Cobs, family=binomial(link=\"logit\"), data=dataframe, singular.ok=F))\n  if (print.output) print(summary(fit.cca)) # To print output when checking on few iterations\n  if (!inherits(fit.cca, \"try-error\")) {\n    res&lt;-rbind(res,c(\n      rep,\n      \"CCA\",\n      coef(fit.cca)[\"ETRUE\"],\n      coef(summary(fit.cca))[\"ETRUE\",2],\n      nobs(fit.cca),\n      NA # df is only needed for MI but must be included for all\n    ),\n    row.names=NULL)\n  } else {\n    res&lt;-rbind(res,c(\n      rep,\n      \"CCA\",\n      NA,\n      NA,\n      NA,\n      NA # df is only needed for MI but must be included for all\n    ),\n    row.names=NULL)\n  }\n\n  \n  # Method 3: MI \n  df.mice&lt;-dataframe[,c(\"Cobs\", \"D\", \"E\")]\n  df.mice$int&lt;-df.mice$D*df.mice$E\n  imp &lt;- try(mice(df.mice, method = \"norm\", m = 5, printFlag = F))\n  if (!inherits(imp, \"try-error\")) {\n    fit &lt;- with(data = imp, exp = glm(D~E+Cobs, family=binomial(link=\"logit\"), singular.ok=F))\n    if (!inherits(fit, \"try-error\")) {\n      rub.rul&lt;-summary(pool(fit))\n      if (print.output) print(rub.rul) # To print output when checking on few iterations\n      res&lt;-rbind(res,c(\n        rep,\n        \"MI\",\n        rub.rul[rub.rul$term==\"ETRUE\",\"estimate\"],\n        rub.rul[rub.rul$term==\"ETRUE\",\"std.error\"],\n        nobs(fit.fd),\n        rub.rul[rub.rul$term==\"ETRUE\",\"df\"]),\n        row.names=NULL)\n    } else {\n      \n      res&lt;-rbind(res,c(\n        rep,\n        \"MI\",\n        NA,\n        NA,\n        NA,\n        NA),\n        row.names=NULL)\n    }\n  } else {\n    \n    res&lt;-rbind(res,c(\n      rep,\n      \"MI\",\n      NA,\n      NA,\n      NA,\n      NA),\n      row.names=NULL)\n  }\n  \n  colnames(res) &lt;- c( \"rep\", \"method\", \"est\", \"se\", \"N\", \"df\")\n  \n  return(res)\n  \n}"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck06.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck06.r",
    "title": "How to check the simulation study",
    "section": "5.6 simcheck06.r",
    "text": "5.6 simcheck06.r\n###################################\n### simcheck06.R: Make it easy  ###\n### to re-create any simulated  ###\n### data set.                   ###\n###################################\n\n# Load relevant libraries and source simcheck05.R\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\nlibrary(foreach) # for using foreach loop\nsource(\"simcheck05.R\")\n\n# Set seed\nset.seed(576819506)\n\n# run 3 repetitions\nresults &lt;- foreach( i = 1:3) %do% {    # Note that in order to save the seed we return results as a list\n  dataframe &lt;- gendata( obs = 500, logite = \"-1+Ctrue\", logitd = \"-1+Ctrue\", pmiss = 0.3)\n  resul&lt;-anadata(dataframe, i, print.output = T) # Printing output to double check we are storing correct estimates\n  attr(resul, \"seed\")&lt;-.Random.seed  # Now store the seed after running the sim study, in case you want to continue from here later\n  resul\n}\n\n# view stored results for 3rd repetition\nView(results[[3]])\n\n# reconstruct data set for 3rd repetition and check it gives same results\n.Random.seed &lt;- attr(results[[2]], \"seed\") # Need the seed status after running 2 repetitions\ndataframe &lt;- gendata( obs = 500, logite = \"-1+Ctrue\", logitd = \"-1+Ctrue\", pmiss = 0.3)\nresul&lt;-anadata(dataframe, i)\nView(resul)\n# can verify that these results are the same"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck07.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck07.r",
    "title": "How to check the simulation study",
    "section": "5.7 simcheck07.r",
    "text": "5.7 simcheck07.r\n###################################\n### simcheck07.R: Assess method ###\n### failures.                   ###\n###################################\n  \n# Load relevant libraries and source simcheck05.R\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\nlibrary(foreach) # for using foreach loop\nlibrary(dplyr) # To transform list for data frame in single data frame\nsource(\"simcheck05.R\")\n\n# Set seed\nset.seed(576819506)\n\n# run simulation study\n# we increase reps to 120 for illustrative purposes\nresults &lt;- foreach( i = 1:120) %do% {    # Note that in order to save the seed we return results as a list\n  dataframe &lt;- gendata( obs = 200, logite = \"-4+Ctrue\", logitd = \"-4+Ctrue\", pmiss = 0.3)\n  resul&lt;-anadata(dataframe, i)\n  attr(resul, \"seed\")&lt;-.Random.seed  # Now store the seed after running the sim study, in case you want to continue from here later\n  cat(\".\")  # To create progress bar. This can also be done with utils::txtProgressBar()` \n  if (i%%50==0) cat(\"\\n\")\n  resul\n}\nresults.df&lt;-bind_rows(results, .id = \"rep\")\nresults.df[,3]&lt;-as.numeric(results.df[,3])\nresults.df[,4]&lt;-as.numeric(results.df[,4])\nresults.df[,5]&lt;-as.numeric(results.df[,5])\nresults.df[,6]&lt;-as.numeric(results.df[,6])\nsummary(results.df)\n\n# Inspect results for method failures\nView(results.df[is.na(results.df$est),])\nView(results.df[results.df$rep==110,])\n\n# Reconstruct one data set with method failures\n.Random.seed &lt;- attr(results[[109]], \"seed\") # Need the seed status after running 109 repetitions\ndataframe &lt;- gendata( obs = 200, logite = \"-4+Ctrue\", logitd = \"-4+Ctrue\", pmiss = 0.3)\n\n# and explore it\naddmargins(table(dataframe[!is.na(dataframe$Cobs),\"D\"],dataframe[!is.na(dataframe$Cobs),\"E\"])) \nfit.ad &lt;- glm(D~E+Cobs, data=dataframe, family=binomial(link=\"logit\"))\nsummary(fit.ad)\n\n# Save results\nsave(results, results.df, file=\"simcheck07_results.RData\")\n\n# Export results for Stata\nlibrary(foreign) # read/write data to different formats\nwrite.dta(results.df, \"simcheck07_Rresults.dta\") # to Stata format"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck08.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck08.r",
    "title": "How to check the simulation study",
    "section": "5.8 simcheck08.r",
    "text": "5.8 simcheck08.r\n###################################\n### simcheck08.R: Check for     ###\n### outliers.                   ###\n###################################\n\n# Load relevant libraries, source simcheck05.R and load simcheck08_results.RData\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\nlibrary(ggplot2)\n\nsource(\"simcheck05.R\")\nload(\"simcheck07_results.RData\")\n\n# Find where very large se occurs\nlarge.ses&lt;-unique(results.df[results.df$se&gt;100,\"rep\"])\n\n# Re-level so that Full is reference:\nresults.df$method&lt;-factor(results.df$method, levels=c(\"Full\", \"CCA\", \"MI\"))\n\n# Scatterplot of SE against estimate\nggplot(results.df,aes(x=est,y=se)) + \n  geom_point(size = 2) + \n  facet_wrap(~method, ncol = 3) + \n  labs(x=\"Point estimate\", y=\"Standard error estimate\")"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck09.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck09.r",
    "title": "How to check the simulation study",
    "section": "5.9 simcheck09.r",
    "text": "5.9 simcheck09.r\n###################################\n### simcheck09.R: Investigate   ###\n### outliers.                   ###\n###################################\n\n# Load relevant libraries and source simcheck05.R\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\nlibrary(foreach) # for using foreach loop\nlibrary(dplyr) # To transform list fo data frame in single data frame\nsource(\"simcheck05.R\")\n\nload(\"simcheck07_results.RData\")\n\n# Find where very large se occurs\nlarge.ses&lt;-unique(results.df[results.df$se&gt;100,\"rep\"])\n\n# let's pick out the second rep\nresults.df[results.df$rep==2,]\n\n# Reconstruct this data set with method failures\n.Random.seed &lt;- attr(results[[1]], \"seed\") # Need the seed status after running 1 repetitions\ndataframe &lt;- gendata( obs = 200, logite = \"-4+Ctrue\", logitd = \"-4+Ctrue\", pmiss = 0.3)\n\n# and explore it\naddmargins(table(dataframe[!is.na(dataframe$Cobs),\"D\"],dataframe[!is.na(dataframe$Cobs),\"E\"])) \nfit.ad &lt;- glm(D~E+Cobs, data=dataframe, family=binomial(link=\"logit\"), singular.ok=F, epsilon = 1e-14)\nsummary(fit.ad)"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck10.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck10.r",
    "title": "How to check the simulation study",
    "section": "5.10 simcheck10.r",
    "text": "5.10 simcheck10.r\n###################################\n### simcheck10.R: Deal with     ###\n### Outliers.                   ###\n###################################\n\n# Load relevant libraries and source simcheck05.R\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\nlibrary(foreach) # for using foreach loop\nlibrary(dplyr) # To transform list fo data frame in single data frame\nlibrary(logistf) # To use Firth correction\nlibrary(miceafter) # To apply Rubin s rules after using logistf\nsource(\"simcheck05.R\")\n\n# Set seed\nset.seed(576819506)\n\n# Changed program to analyse the data using Firth correction:\n\nanadata &lt;- function( dataframe, rep) {\n  \n  # Method 1: full data before data deletion  \n  fit.fd&lt;-try(logistf(D~E+Ctrue, data=dataframe, singular.ok=F, epsilon = 1e-14))\n  if (!inherits(fit.fd, \"try-error\")) {\n    res&lt;-data.frame(\n      rep &lt;- rep,\n      method &lt;- \"Full\",\n      est &lt;- coef(fit.fd)[\"ETRUE\"],\n      se &lt;- sqrt(diag(summary(fit.fd)$var))[2],\n      N &lt;- nobs(fit.fd),\n      df &lt;- NA, # df is only needed for MI but must be included for all\n      row.names = NULL)\n  } else {\n    res&lt;-data.frame(\n      rep &lt;- rep,\n      method &lt;- \"Full\",\n      est &lt;- NA,\n      se &lt;- NA,\n      N &lt;- NA,\n      df &lt;- NA, # df is only needed for MI but must be included for all\n      row.names = NULL)\n    \n  }\n  \n  # Method 2: CCA \n  fit.cca&lt;-try(logistf(D~E+Cobs, data=dataframe, singular.ok=F, epsilon = 1e-14))\n  if (!inherits(fit.cca, \"try-error\")) {\n    res&lt;-rbind(res,c(\n      rep,\n      \"CCA\",\n      coef(fit.cca)[\"ETRUE\"],\n      sqrt(diag(summary(fit.cca)$var))[2],\n      nobs(fit.cca),\n      NA # df is only needed for MI but must be included for all\n    ),\n    row.names=NULL)\n  } else {\n    res&lt;-rbind(res,c(\n      rep,\n      \"CCA\",\n      NA,\n      NA,\n      NA,\n      NA # df is only needed for MI but must be included for all\n    ),\n    row.names=NULL)\n  }\n  \n  \n  # Method 3: MI \n  df.mice&lt;-dataframe[,c(\"Cobs\", \"D\", \"E\")]\n  df.mice$int&lt;-df.mice$D*df.mice$E\n  imp &lt;- try(mice(df.mice, method = \"norm\", m = 5, printFlag = F))\n  if (!inherits(imp, \"try-error\")) {\n    fit &lt;- with(data = imp, exp = logistf(D~E+Cobs, singular.ok=F, epsilon = 1e-14))\n    if (!inherits(fit, \"try-error\")) {\n      coefs&lt;-c(\n        fit$analyses[[1]]$coefficients[\"ETRUE\"],\n        fit$analyses[[2]]$coefficients[\"ETRUE\"],\n        fit$analyses[[3]]$coefficients[\"ETRUE\"],\n        fit$analyses[[4]]$coefficients[\"ETRUE\"],\n        fit$analyses[[5]]$coefficients[\"ETRUE\"]\n      )\n      ses&lt;-c(\n        sqrt(diag(fit$analyses[[1]]$var))[2],\n        sqrt(diag(fit$analyses[[2]]$var))[2],\n        sqrt(diag(fit$analyses[[3]]$var))[2],\n        sqrt(diag(fit$analyses[[4]]$var))[2],\n        sqrt(diag(fit$analyses[[5]]$var))[2]\n      )\n      rub.rul &lt;- pool_scalar_RR(coefs, ses, dfcom=200-3) # mice pool could not be used with logistf\n      res&lt;-rbind(res,c(\n        rep,\n        \"MI\",\n        rub.rul$pool_est,\n        rub.rul$pool_se,\n        nobs(fit.fd),\n        rub.rul$v_adj), \n        row.names=NULL)\n    } else {\n      \n      res&lt;-rbind(res,c(\n        rep,\n        \"MI\",\n        NA,\n        NA,\n        NA,\n        NA),\n        row.names=NULL)\n    }\n  } else {\n    \n    res&lt;-rbind(res,c(\n      rep,\n      \"MI\",\n      NA,\n      NA,\n      NA,\n      NA),\n      row.names=NULL)\n  }\n  \n  colnames(res) &lt;- c( \"rep\", \"method\", \"est\", \"se\", \"N\", \"df\")\n  \n  return(res)\n  \n}\n\n# run simulation study\nresults &lt;- foreach( i = 1:120) %do% {    # Note that in order to save the seed we return results as a list\n  dataframe &lt;- gendata( obs = 200, logite = \"-4+Ctrue\", logitd = \"-4+Ctrue\", pmiss = 0.3)\n  resul&lt;-anadata(dataframe, i)\n  attr(resul, \"seed\")&lt;-.Random.seed  # Now store the seed after running the sim study, in case you want to continue from here later\n  cat(\".\")\n  if (i%%50==0) cat(\"\\n\")\n  resul\n}\nresults.df&lt;-bind_rows(results, .id = \"rep\")\nresults.df[,3]&lt;-as.numeric(results.df[,3])\nresults.df[,4]&lt;-as.numeric(results.df[,4])\nresults.df[,5]&lt;-as.numeric(results.df[,5])\nresults.df[,6]&lt;-as.numeric(results.df[,6])\nsummary(results.df)\nView(results.df)"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck11.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck11.r",
    "title": "How to check the simulation study",
    "section": "5.11 simcheck11.r",
    "text": "5.11 simcheck11.r\n###################################\n### simcheck11.R: Check Monte   ###\n### Carlo errors.               ###\n###################################\n\n# Load relevant libraries and source simcheck05.R\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\nlibrary(foreach) # for using foreach loop\nlibrary(dplyr) # To transform list fo data frame in single data frame\nlibrary(rsimsum) # For computing performance measures with MCSE\nsource(\"simcheck05.R\")\n\n# Set seed\nset.seed(576819506)\n\n# run simulation study\nresults &lt;- foreach( i = 1:100) %do% {    # Note that in order to save the seed we return results as a list\n  dataframe &lt;- gendata( obs = 500, logite = \"-3+Ctrue\", logitd = \"-1+Ctrue\", pmiss = 0.3)\n  resul&lt;-anadata(dataframe, i)\n  attr(resul, \"seed\")&lt;-.Random.seed  # Now store the seed after running the sim study, in case you want to continue from here later\n  cat(\".\")\n  if (i%%50==0) cat(\"\\n\")\n  resul\n}\nresults.df&lt;-bind_rows(results, .id = \"rep\")\nresults.df[,3]&lt;-as.numeric(results.df[,3])\nresults.df[,4]&lt;-as.numeric(results.df[,4])\nresults.df[,5]&lt;-as.numeric(results.df[,5])\nresults.df[,6]&lt;-as.numeric(results.df[,6])\nsummary(results.df)\n\ncolnames(results.df)[which(colnames(results.df)==\"est\")]&lt;-\"logOR\"\ns &lt;- simsum(data = results.df, estvarname = \"logOR\", true = 0, se = \"se\", methodvar = \"method\", ref = \"Full\")\nsummary(s)"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck12.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck12.r",
    "title": "How to check the simulation study",
    "section": "5.12 simcheck12.r",
    "text": "5.12 simcheck12.r\n###################################\n### simcheck12.R: Why are       ###\n### Model-based SEs wrong?      ###\n###################################\n\n# Load relevant libraries \nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\nlibrary(foreach) # for using foreach loop\nlibrary(dplyr) # To transform list fo data frame in single data frame\nlibrary(rsimsum) # For computing performance measures with MCSE\n\n# First, we write a function to generate complete data:\n\ngendata &lt;- function( obs, logite, logitd) {\n  \n  Ctrue &lt;- rnorm(obs)\n  E &lt;- runif(obs) &lt; inv.logit(eval(parse(text=logite)))\n  D &lt;- runif(obs) &lt; inv.logit(eval(parse(text=logitd)))\n  data.out&lt;-data.frame(D,E,Ctrue)\n  \n  return(data.out)  # for clarity, but redundant \n  \n}\n\n# Second, program to impose the missing data:\n\ngendata2 &lt;- function( dat, pmiss) {\n  \n  data.out&lt;-dat\n  data.out$Cobs&lt;-dat$Ctrue\n  data.out$Cobs[runif(nrow(dat))&lt;pmiss]&lt;-NA\n  \n  return(data.out)  # for clarity, but redundant \n  \n}\n\n# Next we provide a function to analyse partially observed data.set:\n\nanadata &lt;- function( dataframe, rep) {\n  \n  # Method 2: CCA  \n  fit.cca&lt;-try(glm(D~E+Cobs, family=binomial(link=\"logit\"), data=dataframe, singular.ok=F, epsilon = 1e-14))\n  if (!inherits(fit.cca, \"try-error\")) {\n    res&lt;-data.frame(\n      rep &lt;- rep,\n      method &lt;- \"CCA\",\n      est &lt;- coef(fit.cca)[\"ETRUE\"],\n      se &lt;- coef(summary(fit.cca))[\"ETRUE\",2],\n      N &lt;- nobs(fit.cca),\n      df &lt;- NA, # df is only needed for MI but must be included for all\n      row.names = NULL)\n  } else {\n    res&lt;-data.frame(\n      rep &lt;- rep,\n      method &lt;- \"CCA\",\n      est &lt;- NA,\n      se &lt;- NA,\n      N &lt;- NA,\n      df &lt;- NA, # df is only needed for MI but must be included for all\n      row.names = NULL)\n    \n  }\n  \n  # Method 3: MI \n  df.mice&lt;-dataframe[,c(\"Cobs\", \"D\", \"E\")]\n  df.mice$int&lt;-df.mice$D*df.mice$E\n  imp &lt;- try(mice(df.mice, method = \"norm\", m = 5, printFlag = F))\n  if (!inherits(imp, \"try-error\")) {\n    fit &lt;- with(data = imp, exp = glm(D~E+Cobs, family=binomial(link=\"logit\"), singular.ok=F, epsilon = 1e-14))\n    if (!inherits(fit, \"try-error\")) {\n      rub.rul&lt;-summary(pool(fit))\n      res&lt;-rbind(res,c(\n        rep,\n        \"MI\",\n        rub.rul[rub.rul$term==\"ETRUE\",\"estimate\"],\n        rub.rul[rub.rul$term==\"ETRUE\",\"std.error\"],\n        nobs(fit.fd),\n        rub.rul[rub.rul$term==\"ETRUE\",\"df\"]),\n        row.names=NULL)\n    } else {\n      \n      res&lt;-rbind(res,c(\n        rep,\n        \"MI\",\n        NA,\n        NA,\n        NA,\n        NA),\n        row.names=NULL)\n    }\n  } else {\n    \n    res&lt;-rbind(res,c(\n      rep,\n      \"MI\",\n      NA,\n      NA,\n      NA,\n      NA),\n      row.names=NULL)\n  }\n  \n  colnames(res) &lt;- c( \"rep\", \"method\", \"est\", \"se\", \"N\", \"df\")\n  \n  return(res)\n  \n}\n\n# Create complete data and find true value \n\nfull.data &lt;- gendata( obs = 500, logite = \"-3+Ctrue\", logitd = \"-1+Ctrue\")\nfit.fd&lt;-try(glm(D~E+Ctrue, family=binomial(link=\"logit\"), data=full.data, singular.ok=F, epsilon = 1e-14))\nif (class(fit.fd)[1]!= \"try-error\") {\n  true&lt;-coef(fit.fd)[\"ETRUE\"]\n} else {\n  true&lt;-0\n}\n\n# Perform simulation (1000 repetitions)\n\nresults &lt;- foreach( i = 1:1000) %do% {    # Note that in order to save the seed we return results as a list\n  dataframe &lt;- gendata2( dat=full.data, pmiss = 0.3)\n  resul&lt;-anadata(dataframe, i)\n  attr(resul, \"seed\")&lt;-.Random.seed  # Now store the seed after running the sim study, in case you want to continue from here later\n  cat(\".\")\n  if (i%%50==0) cat(\"\\n\")\n  resul\n}\nresults.df&lt;-bind_rows(results, .id = \"rep\")\nresults.df[,3]&lt;-as.numeric(results.df[,3])\nresults.df[,4]&lt;-as.numeric(results.df[,4])\nresults.df[,5]&lt;-as.numeric(results.df[,5])\nresults.df[,6]&lt;-as.numeric(results.df[,6])\nsummary(results.df)\n\ncolnames(results.df)[which(colnames(results.df)==\"est\")]&lt;-\"logOR\"\ns &lt;- simsum(data = results.df, estvarname = \"logOR\", true = true, se = \"se\", methodvar = \"method\", ref = \"CCA\")\nsummary(s)"
  },
  {
    "objectID": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck99.r",
    "href": "posts/statistics/2025/IJE_How_to_Check_the_simulation.html#simcheck99.r",
    "title": "How to check the simulation study",
    "section": "5.13 simcheck99.r",
    "text": "5.13 simcheck99.r\n###################################\n### simcheck99.R: A complete    ###\n### successful simulation study.###\n###################################\n\n# Load relevant libraries and source simcheck05.R\nlibrary(boot) # Contains inv.logit function\nlibrary(mice) # for MI\nlibrary(foreach) # for using foreach loop\nlibrary(dplyr) # To transform list of data frame in single data frame\nlibrary(rsimsum) # For computing performance measures with MCSE\nsource(\"simcheck05.R\")\n\n# First, we write a function to generate complete data:\n\ngendata &lt;- function( obs, logite, logitd) {\n  \n  Ctrue &lt;- rnorm(obs)\n  E &lt;- runif(obs) &lt; inv.logit(eval(parse(text=logite)))\n  D &lt;- runif(obs) &lt; inv.logit(eval(parse(text=logitd)))\n  data.out&lt;-data.frame(D,E,Ctrue)\n  \n  return(data.out)  # for clarity, but redundant \n  \n}\n\n# Second, program to impose the missing data:\n\ngendata2 &lt;- function( dat, pmiss) {\n  \n  data.out&lt;-dat\n  data.out$Cobs&lt;-dat$Ctrue\n  data.out$Cobs[runif(nrow(dat))&lt;pmiss]&lt;-NA\n  \n  return(data.out)  # for clarity, but redundant \n  \n}\n\n# Set seed\nset.seed(576819506)\n\n# run simulation study\nresults &lt;- foreach( i = 1:1000) %do% {    # Note that in order to save the seed we return results as a list\n  \n  full.data &lt;- gendata( obs = 500, logite = \"-3+Ctrue\", logitd = \"-1+Ctrue\")\n  \n  # First, MCAR:\n  dataframe &lt;- gendata2( dat = full.data, pmiss=0.3)\n  resul1&lt;-anadata(dataframe, i)\n  resul1$dgm&lt;-\"MCAR\"\n  \n  # Then, MAR:\n  dataframe2 &lt;- gendata2( dat = full.data, pmiss=.1+.2*full.data$E+.2*full.data$D)\n  resul2&lt;-anadata(dataframe2, i)\n  resul2$dgm&lt;-\"MAR\"\n\n  # Finally MNAR:\n  dataframe3 &lt;- gendata2( dat = full.data, pmiss=.2+.2*full.data$Ctrue)\n  resul3&lt;-anadata(dataframe3, i)\n  resul3$dgm&lt;-\"MNAR\"\n  \n  resul&lt;-rbind(resul1,resul2,resul3)\n  \n  attr(resul, \"seed\")&lt;-.Random.seed  # Now store the seed after running the sim study, in case you want to continue from here later\n  cat(\".\")\n  if (i%%50==0) cat(\"\\n\")\n  resul\n}\nresults.df&lt;-bind_rows(results, .id = \"rep\")\nresults.df[,3]&lt;-as.numeric(results.df[,3])\nresults.df[,4]&lt;-as.numeric(results.df[,4])\nresults.df[,5]&lt;-as.numeric(results.df[,5])\nresults.df[,6]&lt;-as.numeric(results.df[,6])\nsummary(results.df)\n\ncolnames(results.df)[which(colnames(results.df)==\"est\")]&lt;-\"logOR\"\n\n# correct one outlier to missing\nresults.df[results.df$se&gt;10,]\nresults.df$logOR[results.df$se&gt;10] &lt;- NA\nresults.df$se[results.df$se&gt;10] &lt;- NA\n\ns &lt;- simsum(data = results.df, estvarname = \"logOR\", true = 0, se = \"se\", methodvar = \"method\", ref = \"CCA\", by=\"dgm\")\nsummary(s)"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html",
    "href": "posts/statistics/2025/Markdown記法1.html",
    "title": "Markdown記法について",
    "section": "",
    "text": "R Quartoでは、Markdownを使って文書を作成し、Rコードと組み合わせて美しいレポートや論文を生成できます。本記事では、Quarto環境で効果的に使えるMarkdown記法を体系的に解説します。\n\n\n\n私たちのR　再現可能な研究24.Quarto［基礎］\n私たちのR　再現可能な研究25.Quarto［文書］\n私たちのR　再現可能な研究25.Quarto［スライド］\n私たちのR　Appendix F — R Markdown [基礎]\n私たちのR　Appendix G — R Markdown [応用]\n私たちのR　Appendix H — Quarto入門\n\n\n\n\nMarkdownは、プレーンテキストで記述した文書を構造化された文書に変換するためのマークアップ言語です。R Quartoでは、このMarkdownとRコードを組み合わせて、データ分析レポートや学術論文を作成できます。\n\n\n\n可読性が高い：マークアップが最小限で、プレーンテキストでも内容が理解しやすい\n学習コストが低い：基本的な記法は数時間で習得可能\nQuartoとの親和性：Rコードチャンクとシームレスに統合\n多様な出力形式：HTML、PDF、Word、PowerPointなど\n\n\n\n\n\n\n\nコードを美しく表示するには、バッククオート3つ（```）でコードを囲みます。 これだけだとSAS/Rに限らず、プログラムは実行はされないが、サンプルとして提示する際に便利である。\n# Rコードの例\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# データの読み込みと前処理\ndata &lt;- mtcars %&gt;%\n  mutate(efficiency = ifelse(mpg &gt; 20, \"High\", \"Low\"))\n\n# 散布図の作成\nggplot(data, aes(x = wt, y = mpg, color = efficiency)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"車重と燃費の関係\",\n       x = \"車重 (1000 lbs)\",\n       y = \"燃費 (mpg)\")\nQuartoでRプログラムも実行させたい場合は以下のように記載する。なお、SASは実行させない前提とする。 なお、SASの設定環境をQuartoに構築したらSASも実行可能である。\nプログラムも実行させるには、バッククオート3つ（```）でコードを囲み、{r}と書く。そうすると、Rプログラムの実行できる。\nオプションとしてRプログラムを非表示にしたり、表や図を表示する際は、2つの図表を横に並べたりとオプションは様々ある。それらは、こちらのブログを参考にしていただきたい。デフォルトではプログラムが表示されてしまうので、非表示にする場合は、\n\n\nコード\n1+1\n\n\n[1] 2\n\n\nQuartoでの頻用するであろうオプション記法：\n\n\n\n\n実行制御：このコードを実際に実行するかを指定\ntrue：コードを実行する（デフォルト）\nfalse：コードを実行せず、表示のみ\n\n\n\n\n\n出力形式：コードの実行結果をそのまま（as-is）出力\n通常はコードの出力結果が整形されますが、asisでは生の形式で出力\nHTMLタグやMarkdown記法をそのまま文書に挿入したい場合に使用\n\n\n\n\n\nコード表示制御：コードブロックを折りたたみ状態で表示する\ntrue：コードを折りたたんで、クリック可能なボタンで展開\nfalse：コードを通常通り表示（デフォルト）\n読者が必要に応じてコードの詳細を確認できる柔軟性を提供\n\n\n\n\n\n折りたたみボタンのラベル：折りたたまれたコードを展開するボタンのテキストを設定\nデフォルトでは「Show code」や「コードを表示」が表示される\nカスタムテキストで、そのコードブロックの内容を説明できる\n絵文字や詳細な説明文を使用して、読みやすさを向上させる\n\n以下のプログラムを回すと、その下の結果が得らえる。プログラムが表示されないので結果だけを提示する際には有用である。\n#| eval: true\n#| output: asis\n#| code-fold: true\n#| code-summary: \"Show Code\"\n\n1 + 1\n\nShow Code\n1 + 1\n\n[1] 2\n\n\n\n本文中にRの結果を直接入れることができます！これをインラインコードと呼びます。 “r 引数”で本文中に簡単にRの出力結果を入れることができる。これは論文作成の文章案を作成するときに便利であろう。\n以下のように書くことでできます。普通はRチャンクで計算したものを引用するのがよいだろう。\n年齢の平均は r mean(mtcars$mpg) です。\nサンプルサイズは r nrow(mtcars) でした。\n最大値は r max(mtcars$hp) 馬力です。\n年齢の平均は 20.090625 です。 サンプルサイズは 32 でした。 最大値は 335 馬力です。\n\n\n\n\n\nコード\n# データの事前計算\nmean_age &lt;- round(mean(mtcars$mpg), 1)\nsd_age &lt;- round(sd(mtcars$mpg), 2)\nn_cars &lt;- nrow(mtcars)\n\n\nここで、上で事前にRチャンクで計算をしておく。今回は練習のためプログラムを表示しているが、Rプログラムを非表示にしてもよいだろう。記載としては以下のように書けばよい。\n\n\nコード\n本研究では `r n_cars` 台の自動車を分析しました。\n燃費の平均は `r mean_age`mpg（標準偏差 = `r sd_age`）でした。\n\n\n上記のように書くとこのように出力できる。\n本研究では 32 台の自動車を分析しました。\n燃費の平均は 20.1mpg（標準偏差 = 6.03）でした。\n\n\n\n\nMarkdownにおける改行はやや特殊だ。特殊といっても難しいことはない。普段よりもう一行改行するだけだ。Markdownの場合、1回の改行は改行として判定されず、同じ行の連続と認識する。結構難しい。\n文章1 文章2\n文章1\n文章2\n\n\n\nWebページを作成する際、ブラウザが理解できる言語がHTMLです。例えば、ブログ記事でリンクを作成したい場合、HTMLでは以下のように記述します：文章中に簡単にURLを参照できます。\n例：私のブログ\n[私のブログ](https://example-blog.com)\nまた、以下のように{}内に.externalを付けると、リンクのテキストの右側にアイコンを付く。\n[私のブログ](https://example-blog.com){.external target=\"_blank\"}\n例：私のブログ\n\n\n\n文章中でコードや関数名を表示する場合は、バッククオート1つで囲みます。単純にかっこいい。\n例：ggplot()関数やdplyr::filter()を使用してデータを処理します。平均値はmean()で計算できます。\n\n\n\n\n\n見出しは#の数で階層を表現します。学術文書では、適切な階層構造が重要です。 ちなみに#は6つまで使える。\n# 1. はじめに（H1）\n## 1.1 研究背景（H2）\n### 1.1.1 先行研究（H3）\n#### データの特徴（H4）\n##### 変数の詳細（H5）\n###### 補足事項（H6）\n\n\n\n\n\n\n\n重要な結果：**重要な結果**\n統計的有意：*統計的有意*\n仮説は棄却：~~仮説は棄却~~\nアンダーライン：アンダーラインはHTMLタグを使う。\n\n\n\n\n\n\n\n`-`を書いて、blankを入れるだけで順序なしリストができます。\n- データ収集\n  - アンケート調査\n  - 実験データ\n  - 公開データセット\n- データ前処理\n  - 欠損値処理\n  - 外れ値検出\n  - 変数変換\n- 分析手法\n  - 記述統計\n  - 回帰分析\n  - 機械学習\n結果：\n\nデータ収集\n\nアンケート調査\n実験データ\n公開データセット\n\nデータ前処理\n\n欠損値処理\n外れ値検出\n変数変換\n\n分析手法\n\n記述統計\n回帰分析\n機械学習\n\n\n\n\n\n普通に1.みたいにかけばよいだけ。単純。.の付け忘れに注意しよう！\n1. 研究目的の設定\n2. データ収集計画の策定\n   1. サンプルサイズの決定\n   2. 測定項目の選択\n   3. 倫理的配慮\n3. データ収集の実施\n4. 統計解析\n5. 結果の解釈\n6. 考察と結論\n\n\n\n\nQuartoで画像を入れるには![代替テキスト](ファイルのパス名 or URL)と入力します。[代替テキスト]は画像を読み込めなかった場合のテキストを意味します。これは画像が読み込めなかった場合の代替テキストでもあるが、視覚障害者用のウェブブラウザーのためにも使われる。これらのウェブブラウザーはテキストのみ出力されるものが多く、画像の代わりには代替テキストが読み込まれる。\n例えば、Figsフォルダー内のex.pngというファイルを読み込むとしたら以下のように書く。\n![画像](Figs/ex.png)\n\n\n相対パス（推奨）が最も一般的で推奨される方法です。Quartoファイル（.qmd）からの相対位置で指定します。以下のように結果の図を記載するのが楽であろう。絶対パスでも可能であるが、あまりお勧めはできない。\n#相対パス\n![図1: データの分布](images/distribution.png)\n![図2: 回帰分析結果](figs/regression_plot.png)\n![図3: 比較グラフ](../shared_images/comparison.png)\n\n#絶対パス\n![画像](/Users/username/Documents/project/images/plot.png)\n![Windows例](C:\\Users\\username\\Documents\\project\\images\\plot.png)\n\n\n\n\n脚注は[^固有識別子]と[^固有識別子]: 脚注内容の2つの要素が必要だ。まず、文末脚注を入れる箇所に[^xxxx]を挿入する。xxxxは任意の文字列で構わない。しかし、同じQuarto文書内においてこの識別子は被らないようにすること。実際の脚注の内容は[^xxxx]: 内容のように入力する。これはどこに位置しても構わない。文書の途中でも、最後に入れても、脚注の内容は文末に位置する。ただし、脚注を入れる段落のすぐ後の方が作成する側としては読みやすいだろう。\n統計的有意性[^1]は重要な概念ですが、効果量[^2]も同様に考慮すべきです。\n\n[^1]: p値が設定した有意水準（通常0.05）を下回ること。\n\n[^2]: 統計的有意性とは独立した、実際的な重要性を示す指標。\n統計的有意性1は重要な概念ですが、効果量2も同様に考慮すべきです。\n\n\n\nテーブルを自分で書くことはないと思う。生成AIに書いてもらおう。Rでもkableを使えば出てくる。\n\n\n| 変数名 | データ型 | 欠損値 | 説明 |\n|:-------|:---------|:------:|:-----|\n| age | numeric | 0 | 年齢（歳） |\n| gender | factor | 2 | 性別（M/F） |\n| income | numeric | 15 | 年収（万円） |\n| education | factor | 3 | 教育レベル |\n結果：\n\n\n\n変数名\nデータ型\n欠損値\n説明\n\n\n\n\nage\nnumeric\n0\n年齢（歳）\n\n\ngender\nfactor\n2\n性別（M/F）\n\n\nincome\nnumeric\n15\n年収（万円）\n\n\neducation\nfactor\n3\n教育レベル\n\n\n\nRでの例\n\n\nコード\nlibrary(knitr)\nkable(head(mtcars))\n\n\n\n\n表 1: mtcarsデータの基本統計量\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&gt; 統計学における最も重要な概念の一つは、\n&gt; サンプルから母集団について推論を行うことである。\n&gt; この過程では、不確実性を適切に評価することが不可欠である。\n&gt; \n&gt; &gt; データは語るが、解釈は人間が行うものである。\n結果：\n\n統計学における最も重要な概念の一つは、 サンプルから母集団について推論を行うことである。 この過程では、不確実性を適切に評価することが不可欠である。\n\nデータは語るが、解釈は人間が行うものである。\n\n\n\n\n\n\nGFMは数式に対応していないが、$数式$でインライン数式を埋め込むことができる。Quartoの数式はMathJaxと呼ばれるJavaScriptのライブラリによってレンダリングされる。このMathJaxライブラリはHTMLにデフォルトで埋め込まれるわけではではないため、インターネットに接続せずにHTMLファイルを開くと数式が正しく出力されないため、インターネット接続を忘れないこと。MathJaxの記法は とほぼ変わらない。Texでの数式の書き方は別途まとめる。\n\n\n回帰係数は $\\beta_1 = 0.73$ で統計的に有意でした（$p &lt; 0.001$）。 決定係数は $R^2 = 0.85$ でした。\n表示は以下の通り。\n回帰係数は \\beta_1 = 0.73 で統計的に有意でした（p &lt; 0.001）。 決定係数は R^2 = 0.85 でした。\n\n\n\n数式を独立した行として出力する場合は、の代わりに$を使用する。\n$$\ny_i \\sim \\mbox{Normal}(X \\beta, \\sigma).\n$$\n\ny_i \\sim \\mbox{Normal}(X \\beta, \\sigma).\n\n\n\n\nもし数式が複数の行で構成されている場合は$$内にaligned環境（\\begin{aligned}〜\\end{aligned}）を使用する。むろん、 Latexと記法は同じだ。\n\\begin{align}\nY_i &= \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_i \\\\\n\\epsilon_i &\\sim N(0, \\sigma^2) \\\\\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}\n\\end{align}\n複数の行にわたる数式の書き方\n\n\\begin{aligned}\n  Y_i      & \\sim \\text{Bernoulli}(\\theta_i), \\\\\n  \\theta_i & = \\text{logit}^{-1}(y_i^*), \\\\\n  y_i^*    & = \\beta_0 + \\beta_1 x_1 + \\beta_2 z_1.\n\\end{aligned}\n\n\n\n\n\nRの場合、#でコメントを付けられるように、Quartoでもコメントを付けることができる。とりあえず書いたが要らなくなった段落や文章があって、消すことがもったいない場合はコメントアウトするのも1つの方法だろう。ただし、Rのように#は使えない。なぜなら#は見出しを意味する体。QuartoのコメントはHTMLと同様、で囲まれた領域がコメント扱いとなり、レンダリングに影響を与えない。\n例\n文章1\n\n&lt;!--\nここはコメントです。\n--&gt;\n\n文章2\n\n\n\nQuartoを使う意義 以上の内容まで抑えると、Quartoを使って、簡単な文法のみで構造化された文書が作成できるでしょう。しかし、これまでの内容はQuartoの良さではなく、Markdownの良さです。別にQuartoでなくても、TyporaやGhostwriterのようなMarkdownエディターを使えば良いでしょう。Quartoを使う真の意義は、文章とコード、結果が統合されることです。それではQuarto文書にRコードを入れる方法について解説します。 チャンク（Chunk） Quarto文書にRコードを入れる方法は2つあります：\n\nチャンクにRコードを入れる方法\nインラインコードを入れる方法\n\nチャンク内のRコードは独立した段落にコードと結果が両方出力されます。一方、インラインコードは文中に結果のみ出力されます。\n\n\nチャンクが始まるとの宣言は {r}、終わるとの宣言は です。つまり、{r} と ちょんちょんの間にRコードを入れるだけです。前の方にも書きました。\n“Hello World!”を出力するコード\n\n\nコード\nprint(\"Hello World!\")\n\n\n[1] \"Hello World!\"\n\n\n\n\n\nインラインコードの基本概念 他にもインラインコードを使って文中にRコードを埋め込むことも可能です。ただし、Rコードは出力されず、結果のみが出力されます。例えば、ベクトル X &lt;- c(2, 3, 5, 7, 12) があり、この平均値を文中で示したいとしましょう。むろん、文中に「5.8」と直接書いても問題ありません。しかし、Xの入力ミスが見つかり、実は c(2, 3, 5, 7, 11) になったらどうでしょうか。この「5.8」と書いた箇所を見つけて「5.6」と修正しなければいけません。これは非常に面倒な作業であり、ミスも起こりやすいです。絶対やめましょう。\n\nインラインコードの利点\n\n文中に mean(X) の結果を埋め込めるならこういったミスを未然に防ぐことができ、文書のメンテナンスも楽になるでしょう。インラインコードの記法文中でRコードを入れるためには r と ` の間にRコードを入力すれば良いです。\nこうかけばいいのです。\n\n\nコード\nmean(X)の実行結果：`r mean(X)`\n\n\n出力は以下\nmean(X)の実行結果：5.6\nコードスパンとインラインコードの違い mean(X) のように r でなく、単に `` だけで囲まれたコードは実行されません。文中に短いコードを入れたり、オブジェクト名を表記する際などに使う機能です。つまり、\n\n`コード` = コードを文字として見せるだけ\n`R コード` = コードを実行して結果を表示 （r コード）\n\n\n\n\n\nオプションの基本構文\nここではチャンクに指定可能なオプションについて紹介します。実際は本記事で紹介する内容の十数倍のオプションが用意されていますが、あまりにも膨大すぎるため、ここではよく使う機能のみを紹介します。 チャンクオプションはチャンク内の最上段に #| 仮引数: 実引数 のように表記します。 基本例：\n\n\nコード\n#| eval: false\n1+1\n\n\n[1] 2\n\n\neval は true か false の値が指定できます。evalは「コードを実行するかどうか」を決めるオプションです。\n\n\n\n\nチャンク名は #| label: チャンク名 で指定します。これはチャンクに名前を付けるオプションですが、多くの場合分析に影響を与えることはありません（それでもチャンク名は指定することを強く推奨します）。\nラベルの例は以下の通り。\n\n\nコード\n1+1\n\n\n[1] 2\n\n\n\n\nコード\n1+1\n\n\n[1] 2\n\n\n\n\nコード\n1+1\n\n\n[1] 2\n\n\n\n\nコード\n1+1\n\n\n[1] 2\n\n\n\n\n\nこのチャンク名が重要となるのは cache オプションを付ける場合です。\ncache オプションは処理結果を保存しておくことを意味します。チャンク内のコードはrenderする度に計算されますが、演算にかなりの時間を必要とするコードが含まれている場合、renderの時間も長くなります。\n\n\nコード\n1+1\n\n\n[1] 2\n\n\n時間のかかる処理cache: true オプションを付けておくと、最初のrender時に結果を別途のファイルとして保存しておき、次回からはその結果を読み込むだけとなります。基本的にはこのオプションはおすすめしない。\n\n\n\n\n次は「コードだけ見せたい」、「結果だけ見せたい」場合に使うオプションを紹介します。これは技術書、授業用資料、スライドでよく使う機能です。\n\n\n\n\n\nオプション\n説明\nデフォルト値\n\n\n\n\necho\nコードの出力有無\ntrue\n\n\neval\nコードの実行有無\ntrue\n\n\ninclude\nコードと結果両方の表示有無\ntrue\n\n\n\n\n\n\nコードのみ出力（実行なし）：\n\n\nコード\nこのコードは表示されるが実行されない\n\n\n結果のみ出力（コード非表示）：\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nコードと結果を両方隠す：\nパッケージの読み込みコードやメタ変数の作成の際に include: false は有用なオプションです。\n\n\n\n\n既に見てきた通り、Quartoは作図の結果も出力してくれます。図のサイズや解像度を変えることもできます。\n\n\n\n\n\nオプション名\n説明\n値の例\n\n\n\n\nfig-height\n図の高さ（インチ）\n数値\n\n\nfig-width\n図の幅（インチ）\n数値\n\n\nfig-align\n図の位置\n“left”, “center”, “right”\n\n\nfig-cap\n図のキャプション\n文字列\n\n\ndpi\n図の解像度（印刷用なら300以上を推奨）\n数値\n\n\n\n\n\n\n\n\nコード\nlibrary(ggplot2)\nlibrary(dplyr)\n\niris %&gt;%\n  mutate(Species2 = recode(Species,\n                           \"setosa\"     = \"セトナ\",\n                           \"versicolor\" = \"バーシクル\",\n                           \"virginica\"  = \"バージニカ\")) %&gt;%\n  ggplot() +\n  geom_point(aes(x = Sepal.Length, y = Sepal.Width, color = Species2)) +\n  labs(x = \"萼片の長さ (cm)\", y = \"萼片の幅 (cm)\", color = \"品種\") +\n  theme_minimal()\n\n\n\n\n\nirisデータセットの可視化\n\n\n\n\n\n\n\n\n\n\n自分だけが見るコードなら別に推奨されない書き方でも問題ないかもしれませんが、Quarto文書は他人と共有するケースが多いため、読みやすいコードを書くのも大事でしょう。\nここで便利なオプションが tidy オプションです。tidy: true を加えると、自動的にコードを読みやすい形に調整してくれます。\n\n\n\ntidy: false（デフォルト）の場合：\n\n\nコード\nfor(i in 1:10){\nprint(i*2)\n}\n\n\ntidy: TRUEの場合： Quarto文書は他人と共有するケースが多いため、読みやすいコードを書くのも大事だろう。ここで便利なオプションがtidyオプションだ。tidy: trueを加えると、自動的にコードを読みやすい形に調整してくれる。たとえば、以下のコードは字下げもなく、スペースもほとんど入れていないダメなコードだが、tidy: trueを付けた場合と付けなかった場合の出力結果の違いを見てみよう。tidy: trueを付けただけで、読みやすいコードになった。ちなみにtidyオプションを使うためには事前に{formatR}パッケージをインストールしておく必要がある。ただし、{formatR}パッケージはQuarto文書内にて読み込んでおく必要はない。また、{formatR}パッケージは万能ではないため、普段から読みやすいコードを書くように心がけよう。\n\n\nコード\nfor (i in 1:10) {\n    print(i * 2)\n}\n\n\nR Quartoでのデータ分析レポート作成において、Markdownの適切な使用は以下のメリットをもたらします：\n\n構造化された文書：見出しとセクションで論理的な流れを作成\n美しい数式表示：LaTeX記法による専門的な数式表現\n効果的な表現：テーブル、リスト、引用による情報整理\n再現可能性：コードと文章の統合による透明性の確保\n\nこれらの記法を活用して、読みやすく、理解しやすいデータ分析レポートを作成しましょう。"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#参考文献",
    "href": "posts/statistics/2025/Markdown記法1.html#参考文献",
    "title": "Markdown記法について",
    "section": "",
    "text": "私たちのR　再現可能な研究24.Quarto［基礎］\n私たちのR　再現可能な研究25.Quarto［文書］\n私たちのR　再現可能な研究25.Quarto［スライド］\n私たちのR　Appendix F — R Markdown [基礎]\n私たちのR　Appendix G — R Markdown [応用]\n私たちのR　Appendix H — Quarto入門"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#markdownとは何か",
    "href": "posts/statistics/2025/Markdown記法1.html#markdownとは何か",
    "title": "Markdown記法について",
    "section": "",
    "text": "Markdownは、プレーンテキストで記述した文書を構造化された文書に変換するためのマークアップ言語です。R Quartoでは、このMarkdownとRコードを組み合わせて、データ分析レポートや学術論文を作成できます。\n\n\n\n可読性が高い：マークアップが最小限で、プレーンテキストでも内容が理解しやすい\n学習コストが低い：基本的な記法は数時間で習得可能\nQuartoとの親和性：Rコードチャンクとシームレスに統合\n多様な出力形式：HTML、PDF、Word、PowerPointなど"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#コードの記述方法",
    "href": "posts/statistics/2025/Markdown記法1.html#コードの記述方法",
    "title": "Markdown記法について",
    "section": "",
    "text": "コードを美しく表示するには、バッククオート3つ（```）でコードを囲みます。 これだけだとSAS/Rに限らず、プログラムは実行はされないが、サンプルとして提示する際に便利である。\n# Rコードの例\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# データの読み込みと前処理\ndata &lt;- mtcars %&gt;%\n  mutate(efficiency = ifelse(mpg &gt; 20, \"High\", \"Low\"))\n\n# 散布図の作成\nggplot(data, aes(x = wt, y = mpg, color = efficiency)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"車重と燃費の関係\",\n       x = \"車重 (1000 lbs)\",\n       y = \"燃費 (mpg)\")\nQuartoでRプログラムも実行させたい場合は以下のように記載する。なお、SASは実行させない前提とする。 なお、SASの設定環境をQuartoに構築したらSASも実行可能である。\nプログラムも実行させるには、バッククオート3つ（```）でコードを囲み、{r}と書く。そうすると、Rプログラムの実行できる。\nオプションとしてRプログラムを非表示にしたり、表や図を表示する際は、2つの図表を横に並べたりとオプションは様々ある。それらは、こちらのブログを参考にしていただきたい。デフォルトではプログラムが表示されてしまうので、非表示にする場合は、\n\n\nコード\n1+1\n\n\n[1] 2\n\n\nQuartoでの頻用するであろうオプション記法：\n\n\n\n\n実行制御：このコードを実際に実行するかを指定\ntrue：コードを実行する（デフォルト）\nfalse：コードを実行せず、表示のみ\n\n\n\n\n\n出力形式：コードの実行結果をそのまま（as-is）出力\n通常はコードの出力結果が整形されますが、asisでは生の形式で出力\nHTMLタグやMarkdown記法をそのまま文書に挿入したい場合に使用\n\n\n\n\n\nコード表示制御：コードブロックを折りたたみ状態で表示する\ntrue：コードを折りたたんで、クリック可能なボタンで展開\nfalse：コードを通常通り表示（デフォルト）\n読者が必要に応じてコードの詳細を確認できる柔軟性を提供\n\n\n\n\n\n折りたたみボタンのラベル：折りたたまれたコードを展開するボタンのテキストを設定\nデフォルトでは「Show code」や「コードを表示」が表示される\nカスタムテキストで、そのコードブロックの内容を説明できる\n絵文字や詳細な説明文を使用して、読みやすさを向上させる\n\n以下のプログラムを回すと、その下の結果が得らえる。プログラムが表示されないので結果だけを提示する際には有用である。\n#| eval: true\n#| output: asis\n#| code-fold: true\n#| code-summary: \"Show Code\"\n\n1 + 1\n\nShow Code\n1 + 1\n\n[1] 2\n\n\n\n本文中にRの結果を直接入れることができます！これをインラインコードと呼びます。 “r 引数”で本文中に簡単にRの出力結果を入れることができる。これは論文作成の文章案を作成するときに便利であろう。\n以下のように書くことでできます。普通はRチャンクで計算したものを引用するのがよいだろう。\n年齢の平均は r mean(mtcars$mpg) です。\nサンプルサイズは r nrow(mtcars) でした。\n最大値は r max(mtcars$hp) 馬力です。\n年齢の平均は 20.090625 です。 サンプルサイズは 32 でした。 最大値は 335 馬力です。\n\n\n\n\n\nコード\n# データの事前計算\nmean_age &lt;- round(mean(mtcars$mpg), 1)\nsd_age &lt;- round(sd(mtcars$mpg), 2)\nn_cars &lt;- nrow(mtcars)\n\n\nここで、上で事前にRチャンクで計算をしておく。今回は練習のためプログラムを表示しているが、Rプログラムを非表示にしてもよいだろう。記載としては以下のように書けばよい。\n\n\nコード\n本研究では `r n_cars` 台の自動車を分析しました。\n燃費の平均は `r mean_age`mpg（標準偏差 = `r sd_age`）でした。\n\n\n上記のように書くとこのように出力できる。\n本研究では 32 台の自動車を分析しました。\n燃費の平均は 20.1mpg（標準偏差 = 6.03）でした。"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#改行",
    "href": "posts/statistics/2025/Markdown記法1.html#改行",
    "title": "Markdown記法について",
    "section": "",
    "text": "Markdownにおける改行はやや特殊だ。特殊といっても難しいことはない。普段よりもう一行改行するだけだ。Markdownの場合、1回の改行は改行として判定されず、同じ行の連続と認識する。結構難しい。\n文章1 文章2\n文章1\n文章2"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#urlの挿入",
    "href": "posts/statistics/2025/Markdown記法1.html#urlの挿入",
    "title": "Markdown記法について",
    "section": "",
    "text": "Webページを作成する際、ブラウザが理解できる言語がHTMLです。例えば、ブログ記事でリンクを作成したい場合、HTMLでは以下のように記述します：文章中に簡単にURLを参照できます。\n例：私のブログ\n[私のブログ](https://example-blog.com)\nまた、以下のように{}内に.externalを付けると、リンクのテキストの右側にアイコンを付く。\n[私のブログ](https://example-blog.com){.external target=\"_blank\"}\n例：私のブログ"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#インラインコード",
    "href": "posts/statistics/2025/Markdown記法1.html#インラインコード",
    "title": "Markdown記法について",
    "section": "",
    "text": "文章中でコードや関数名を表示する場合は、バッククオート1つで囲みます。単純にかっこいい。\n例：ggplot()関数やdplyr::filter()を使用してデータを処理します。平均値はmean()で計算できます。"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#見出しと文書構造",
    "href": "posts/statistics/2025/Markdown記法1.html#見出しと文書構造",
    "title": "Markdown記法について",
    "section": "",
    "text": "見出しは#の数で階層を表現します。学術文書では、適切な階層構造が重要です。 ちなみに#は6つまで使える。\n# 1. はじめに（H1）\n## 1.1 研究背景（H2）\n### 1.1.1 先行研究（H3）\n#### データの特徴（H4）\n##### 変数の詳細（H5）\n###### 補足事項（H6）"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#テキストの装飾とフォーマット",
    "href": "posts/statistics/2025/Markdown記法1.html#テキストの装飾とフォーマット",
    "title": "Markdown記法について",
    "section": "",
    "text": "重要な結果：**重要な結果**\n統計的有意：*統計的有意*\n仮説は棄却：~~仮説は棄却~~\nアンダーライン：アンダーラインはHTMLタグを使う。"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#リストとチェックボックス",
    "href": "posts/statistics/2025/Markdown記法1.html#リストとチェックボックス",
    "title": "Markdown記法について",
    "section": "",
    "text": "`-`を書いて、blankを入れるだけで順序なしリストができます。\n- データ収集\n  - アンケート調査\n  - 実験データ\n  - 公開データセット\n- データ前処理\n  - 欠損値処理\n  - 外れ値検出\n  - 変数変換\n- 分析手法\n  - 記述統計\n  - 回帰分析\n  - 機械学習\n結果：\n\nデータ収集\n\nアンケート調査\n実験データ\n公開データセット\n\nデータ前処理\n\n欠損値処理\n外れ値検出\n変数変換\n\n分析手法\n\n記述統計\n回帰分析\n機械学習\n\n\n\n\n\n普通に1.みたいにかけばよいだけ。単純。.の付け忘れに注意しよう！\n1. 研究目的の設定\n2. データ収集計画の策定\n   1. サンプルサイズの決定\n   2. 測定項目の選択\n   3. 倫理的配慮\n3. データ収集の実施\n4. 統計解析\n5. 結果の解釈\n6. 考察と結論"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#画像の挿入",
    "href": "posts/statistics/2025/Markdown記法1.html#画像の挿入",
    "title": "Markdown記法について",
    "section": "",
    "text": "Quartoで画像を入れるには![代替テキスト](ファイルのパス名 or URL)と入力します。[代替テキスト]は画像を読み込めなかった場合のテキストを意味します。これは画像が読み込めなかった場合の代替テキストでもあるが、視覚障害者用のウェブブラウザーのためにも使われる。これらのウェブブラウザーはテキストのみ出力されるものが多く、画像の代わりには代替テキストが読み込まれる。\n例えば、Figsフォルダー内のex.pngというファイルを読み込むとしたら以下のように書く。\n![画像](Figs/ex.png)\n\n\n相対パス（推奨）が最も一般的で推奨される方法です。Quartoファイル（.qmd）からの相対位置で指定します。以下のように結果の図を記載するのが楽であろう。絶対パスでも可能であるが、あまりお勧めはできない。\n#相対パス\n![図1: データの分布](images/distribution.png)\n![図2: 回帰分析結果](figs/regression_plot.png)\n![図3: 比較グラフ](../shared_images/comparison.png)\n\n#絶対パス\n![画像](/Users/username/Documents/project/images/plot.png)\n![Windows例](C:\\Users\\username\\Documents\\project\\images\\plot.png)"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#脚注",
    "href": "posts/statistics/2025/Markdown記法1.html#脚注",
    "title": "Markdown記法について",
    "section": "",
    "text": "脚注は[^固有識別子]と[^固有識別子]: 脚注内容の2つの要素が必要だ。まず、文末脚注を入れる箇所に[^xxxx]を挿入する。xxxxは任意の文字列で構わない。しかし、同じQuarto文書内においてこの識別子は被らないようにすること。実際の脚注の内容は[^xxxx]: 内容のように入力する。これはどこに位置しても構わない。文書の途中でも、最後に入れても、脚注の内容は文末に位置する。ただし、脚注を入れる段落のすぐ後の方が作成する側としては読みやすいだろう。\n統計的有意性[^1]は重要な概念ですが、効果量[^2]も同様に考慮すべきです。\n\n[^1]: p値が設定した有意水準（通常0.05）を下回ること。\n\n[^2]: 統計的有意性とは独立した、実際的な重要性を示す指標。\n統計的有意性1は重要な概念ですが、効果量2も同様に考慮すべきです。"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#テーブルの活用",
    "href": "posts/statistics/2025/Markdown記法1.html#テーブルの活用",
    "title": "Markdown記法について",
    "section": "",
    "text": "テーブルを自分で書くことはないと思う。生成AIに書いてもらおう。Rでもkableを使えば出てくる。\n\n\n| 変数名 | データ型 | 欠損値 | 説明 |\n|:-------|:---------|:------:|:-----|\n| age | numeric | 0 | 年齢（歳） |\n| gender | factor | 2 | 性別（M/F） |\n| income | numeric | 15 | 年収（万円） |\n| education | factor | 3 | 教育レベル |\n結果：\n\n\n\n変数名\nデータ型\n欠損値\n説明\n\n\n\n\nage\nnumeric\n0\n年齢（歳）\n\n\ngender\nfactor\n2\n性別（M/F）\n\n\nincome\nnumeric\n15\n年収（万円）\n\n\neducation\nfactor\n3\n教育レベル\n\n\n\nRでの例\n\n\nコード\nlibrary(knitr)\nkable(head(mtcars))\n\n\n\n\n表 1: mtcarsデータの基本統計量\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#引用とノート",
    "href": "posts/statistics/2025/Markdown記法1.html#引用とノート",
    "title": "Markdown記法について",
    "section": "",
    "text": "&gt; 統計学における最も重要な概念の一つは、\n&gt; サンプルから母集団について推論を行うことである。\n&gt; この過程では、不確実性を適切に評価することが不可欠である。\n&gt; \n&gt; &gt; データは語るが、解釈は人間が行うものである。\n結果：\n\n統計学における最も重要な概念の一つは、 サンプルから母集団について推論を行うことである。 この過程では、不確実性を適切に評価することが不可欠である。\n\nデータは語るが、解釈は人間が行うものである。"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#数式の表示",
    "href": "posts/statistics/2025/Markdown記法1.html#数式の表示",
    "title": "Markdown記法について",
    "section": "",
    "text": "GFMは数式に対応していないが、$数式$でインライン数式を埋め込むことができる。Quartoの数式はMathJaxと呼ばれるJavaScriptのライブラリによってレンダリングされる。このMathJaxライブラリはHTMLにデフォルトで埋め込まれるわけではではないため、インターネットに接続せずにHTMLファイルを開くと数式が正しく出力されないため、インターネット接続を忘れないこと。MathJaxの記法は とほぼ変わらない。Texでの数式の書き方は別途まとめる。\n\n\n回帰係数は $\\beta_1 = 0.73$ で統計的に有意でした（$p &lt; 0.001$）。 決定係数は $R^2 = 0.85$ でした。\n表示は以下の通り。\n回帰係数は \\beta_1 = 0.73 で統計的に有意でした（p &lt; 0.001）。 決定係数は R^2 = 0.85 でした。\n\n\n\n数式を独立した行として出力する場合は、の代わりに$を使用する。\n$$\ny_i \\sim \\mbox{Normal}(X \\beta, \\sigma).\n$$\n\ny_i \\sim \\mbox{Normal}(X \\beta, \\sigma).\n\n\n\n\nもし数式が複数の行で構成されている場合は$$内にaligned環境（\\begin{aligned}〜\\end{aligned}）を使用する。むろん、 Latexと記法は同じだ。\n\\begin{align}\nY_i &= \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_i \\\\\n\\epsilon_i &\\sim N(0, \\sigma^2) \\\\\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}\n\\end{align}\n複数の行にわたる数式の書き方\n\n\\begin{aligned}\n  Y_i      & \\sim \\text{Bernoulli}(\\theta_i), \\\\\n  \\theta_i & = \\text{logit}^{-1}(y_i^*), \\\\\n  y_i^*    & = \\beta_0 + \\beta_1 x_1 + \\beta_2 z_1.\n\\end{aligned}"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#quart内でのコメントアウト",
    "href": "posts/statistics/2025/Markdown記法1.html#quart内でのコメントアウト",
    "title": "Markdown記法について",
    "section": "",
    "text": "Rの場合、#でコメントを付けられるように、Quartoでもコメントを付けることができる。とりあえず書いたが要らなくなった段落や文章があって、消すことがもったいない場合はコメントアウトするのも1つの方法だろう。ただし、Rのように#は使えない。なぜなら#は見出しを意味する体。QuartoのコメントはHTMLと同様、で囲まれた領域がコメント扱いとなり、レンダリングに影響を与えない。\n例\n文章1\n\n&lt;!--\nここはコメントです。\n--&gt;\n\n文章2"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#quartoにおけるrコードの挿入と活用法",
    "href": "posts/statistics/2025/Markdown記法1.html#quartoにおけるrコードの挿入と活用法",
    "title": "Markdown記法について",
    "section": "",
    "text": "Quartoを使う意義 以上の内容まで抑えると、Quartoを使って、簡単な文法のみで構造化された文書が作成できるでしょう。しかし、これまでの内容はQuartoの良さではなく、Markdownの良さです。別にQuartoでなくても、TyporaやGhostwriterのようなMarkdownエディターを使えば良いでしょう。Quartoを使う真の意義は、文章とコード、結果が統合されることです。それではQuarto文書にRコードを入れる方法について解説します。 チャンク（Chunk） Quarto文書にRコードを入れる方法は2つあります：\n\nチャンクにRコードを入れる方法\nインラインコードを入れる方法\n\nチャンク内のRコードは独立した段落にコードと結果が両方出力されます。一方、インラインコードは文中に結果のみ出力されます。\n\n\nチャンクが始まるとの宣言は {r}、終わるとの宣言は です。つまり、{r} と ちょんちょんの間にRコードを入れるだけです。前の方にも書きました。\n“Hello World!”を出力するコード\n\n\nコード\nprint(\"Hello World!\")\n\n\n[1] \"Hello World!\"\n\n\n\n\n\nインラインコードの基本概念 他にもインラインコードを使って文中にRコードを埋め込むことも可能です。ただし、Rコードは出力されず、結果のみが出力されます。例えば、ベクトル X &lt;- c(2, 3, 5, 7, 12) があり、この平均値を文中で示したいとしましょう。むろん、文中に「5.8」と直接書いても問題ありません。しかし、Xの入力ミスが見つかり、実は c(2, 3, 5, 7, 11) になったらどうでしょうか。この「5.8」と書いた箇所を見つけて「5.6」と修正しなければいけません。これは非常に面倒な作業であり、ミスも起こりやすいです。絶対やめましょう。\n\nインラインコードの利点\n\n文中に mean(X) の結果を埋め込めるならこういったミスを未然に防ぐことができ、文書のメンテナンスも楽になるでしょう。インラインコードの記法文中でRコードを入れるためには r と ` の間にRコードを入力すれば良いです。\nこうかけばいいのです。\n\n\nコード\nmean(X)の実行結果：`r mean(X)`\n\n\n出力は以下\nmean(X)の実行結果：5.6\nコードスパンとインラインコードの違い mean(X) のように r でなく、単に `` だけで囲まれたコードは実行されません。文中に短いコードを入れたり、オブジェクト名を表記する際などに使う機能です。つまり、\n\n`コード` = コードを文字として見せるだけ\n`R コード` = コードを実行して結果を表示 （r コード）"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#チャンクオプション2",
    "href": "posts/statistics/2025/Markdown記法1.html#チャンクオプション2",
    "title": "Markdown記法について",
    "section": "",
    "text": "オプションの基本構文\nここではチャンクに指定可能なオプションについて紹介します。実際は本記事で紹介する内容の十数倍のオプションが用意されていますが、あまりにも膨大すぎるため、ここではよく使う機能のみを紹介します。 チャンクオプションはチャンク内の最上段に #| 仮引数: 実引数 のように表記します。 基本例：\n\n\nコード\n#| eval: false\n1+1\n\n\n[1] 2\n\n\neval は true か false の値が指定できます。evalは「コードを実行するかどうか」を決めるオプションです。\n\n\n\n\nチャンク名は #| label: チャンク名 で指定します。これはチャンクに名前を付けるオプションですが、多くの場合分析に影響を与えることはありません（それでもチャンク名は指定することを強く推奨します）。\nラベルの例は以下の通り。\n\n\nコード\n1+1\n\n\n[1] 2\n\n\n\n\nコード\n1+1\n\n\n[1] 2\n\n\n\n\nコード\n1+1\n\n\n[1] 2\n\n\n\n\nコード\n1+1\n\n\n[1] 2\n\n\n\n\n\nこのチャンク名が重要となるのは cache オプションを付ける場合です。\ncache オプションは処理結果を保存しておくことを意味します。チャンク内のコードはrenderする度に計算されますが、演算にかなりの時間を必要とするコードが含まれている場合、renderの時間も長くなります。\n\n\nコード\n1+1\n\n\n[1] 2\n\n\n時間のかかる処理cache: true オプションを付けておくと、最初のrender時に結果を別途のファイルとして保存しておき、次回からはその結果を読み込むだけとなります。基本的にはこのオプションはおすすめしない。\n\n\n\n\n次は「コードだけ見せたい」、「結果だけ見せたい」場合に使うオプションを紹介します。これは技術書、授業用資料、スライドでよく使う機能です。\n\n\n\n\n\nオプション\n説明\nデフォルト値\n\n\n\n\necho\nコードの出力有無\ntrue\n\n\neval\nコードの実行有無\ntrue\n\n\ninclude\nコードと結果両方の表示有無\ntrue\n\n\n\n\n\n\nコードのみ出力（実行なし）：\n\n\nコード\nこのコードは表示されるが実行されない\n\n\n結果のみ出力（コード非表示）：\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nコードと結果を両方隠す：\nパッケージの読み込みコードやメタ変数の作成の際に include: false は有用なオプションです。\n\n\n\n\n既に見てきた通り、Quartoは作図の結果も出力してくれます。図のサイズや解像度を変えることもできます。\n\n\n\n\n\nオプション名\n説明\n値の例\n\n\n\n\nfig-height\n図の高さ（インチ）\n数値\n\n\nfig-width\n図の幅（インチ）\n数値\n\n\nfig-align\n図の位置\n“left”, “center”, “right”\n\n\nfig-cap\n図のキャプション\n文字列\n\n\ndpi\n図の解像度（印刷用なら300以上を推奨）\n数値\n\n\n\n\n\n\n\n\nコード\nlibrary(ggplot2)\nlibrary(dplyr)\n\niris %&gt;%\n  mutate(Species2 = recode(Species,\n                           \"setosa\"     = \"セトナ\",\n                           \"versicolor\" = \"バーシクル\",\n                           \"virginica\"  = \"バージニカ\")) %&gt;%\n  ggplot() +\n  geom_point(aes(x = Sepal.Length, y = Sepal.Width, color = Species2)) +\n  labs(x = \"萼片の長さ (cm)\", y = \"萼片の幅 (cm)\", color = \"品種\") +\n  theme_minimal()\n\n\n\n\n\nirisデータセットの可視化\n\n\n\n\n\n\n\n\n\n\n自分だけが見るコードなら別に推奨されない書き方でも問題ないかもしれませんが、Quarto文書は他人と共有するケースが多いため、読みやすいコードを書くのも大事でしょう。\nここで便利なオプションが tidy オプションです。tidy: true を加えると、自動的にコードを読みやすい形に調整してくれます。\n\n\n\ntidy: false（デフォルト）の場合：\n\n\nコード\nfor(i in 1:10){\nprint(i*2)\n}\n\n\ntidy: TRUEの場合： Quarto文書は他人と共有するケースが多いため、読みやすいコードを書くのも大事だろう。ここで便利なオプションがtidyオプションだ。tidy: trueを加えると、自動的にコードを読みやすい形に調整してくれる。たとえば、以下のコードは字下げもなく、スペースもほとんど入れていないダメなコードだが、tidy: trueを付けた場合と付けなかった場合の出力結果の違いを見てみよう。tidy: trueを付けただけで、読みやすいコードになった。ちなみにtidyオプションを使うためには事前に{formatR}パッケージをインストールしておく必要がある。ただし、{formatR}パッケージはQuarto文書内にて読み込んでおく必要はない。また、{formatR}パッケージは万能ではないため、普段から読みやすいコードを書くように心がけよう。\n\n\nコード\nfor (i in 1:10) {\n    print(i * 2)\n}\n\n\nR Quartoでのデータ分析レポート作成において、Markdownの適切な使用は以下のメリットをもたらします：\n\n構造化された文書：見出しとセクションで論理的な流れを作成\n美しい数式表示：LaTeX記法による専門的な数式表現\n効果的な表現：テーブル、リスト、引用による情報整理\n再現可能性：コードと文章の統合による透明性の確保\n\nこれらの記法を活用して、読みやすく、理解しやすいデータ分析レポートを作成しましょう。"
  },
  {
    "objectID": "posts/statistics/2025/Markdown記法1.html#footnotes",
    "href": "posts/statistics/2025/Markdown記法1.html#footnotes",
    "title": "Markdown記法について",
    "section": "脚注",
    "text": "脚注\n\n\np値が設定した有意水準（通常0.05）を下回ること。↩︎\n統計的有意性とは独立した、実際的な重要性を示す指標。↩︎"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contentsによる_nameの作成.html",
    "href": "posts/statistics/2025/Proc_Contentsによる_nameの作成.html",
    "title": "Proc Contentsを利用したRawデータの変数を_varにするマクロ",
    "section": "",
    "text": "SASで解析プロジェクトを進める際、最初に行う作業の一つが生データの加工です。特に、生データの変数名がSASの命名規則に厳密に準拠していなかったり、特定のプレフィックスやサフィックスを追加・削除したい場合が多くあります。\n今回ご紹介するSASマクロ%rawdataは、この「生データの変数名を自動で一括変換する」という、非常に実用的な処理を実現します。このテクニックをマスターすれば、手作業での変数名変更の手間を大幅に削減し、より効率的なデータ準備が可能になります。\n\n\nこのマクロの主な目的は、入力データセットの全変数に対して、以下のような処理を自動で適用することです。\n\nデータセットの変数情報（定義情報）を取得する\n変数情報を基に、各変数の古い名前と新しい名前のペアを作成する\nそのペアを使って、データセットの全変数名を一括で変更する\n\n特に注目すべきは、変数名の変更ロジックが_subjidのように、元の変数名にアンダースコア（_）をプレフィックスとして追加している点です。これは、特定の命名規則を強制したい場合に非常に有効です。\nそれでは、各セクションを詳しく見ていきましょう。\n\n\n\n%macro rawdata(raw=, sort=, out=&raw);\n\n/* データセット定義情報の DS を作成 */\nproc contents data=work.&raw. out = work.VAR noprints;\nrun;\n\n%macro rawdata(raw=, sort=, out=&raw);: %rawdataという名前のマクロを定義しています。引数は以下の3つです。\n\nraw=: 処理対象となる生データセット名を指定します。\nsort=: （このコードでは直接使用されていませんが、将来的な拡張性を示唆しています。）\nout=&raw: 処理結果の出力データセット名を指定します。デフォルトでは入力と同じ&rawになります。\n\nproc contents data=work.&raw. out = work.VAR noprints; run;: PROC CONTENTSプロシジャは、指定されたデータセット（work.&raw）の構造や変数に関する情報を取得し、その結果を新しいデータセット（work.VAR）に出力します。\n\nnoprints: 通常、PROC CONTENTSは結果をSAS出力ウィンドウに出力しますが、noprintsオプションを指定することで、この出力を抑制し、データセットへの出力のみを行います。 work.VARデータセットには、VARNUM（変数番号）、NAME（変数名）、LENGTH（変数長）、TYPE（変数型）などの情報が格納されます。このうち、今回は特にVARNUMとNAMEが重要になります。\n\n\n\n\n\n/* 変数番号でソート */\nproc sort data=work.VAR ;\nby VARNUM ;\nrun;\nPROC SORTプロシジャを使って、先ほど作成したwork.VARデータセットをVARNUM（変数番号）の昇順でソートします。これにより、後続の処理で変数を順番に扱うことが容易になります。SASは内部的に変数に番号を割り当てており、この番号順で処理することで、元のデータセットにおける変数の並び順を反映できます。\n\n\n\n/* マクロ変数(VARn_)に変数を格納 */\ndata _null_;\n  set work.VAR end = eof;\n  call symputx(\"VAR\"||strip(put(VARNUM, 8.))||\"_\", NAME, 'G');\n  /* マクロ変数(MAXV)にオブザベーション数（変数の数）を格納 */\n  if eof then call symputx('MAXV',_N_);\nrun;\nこのDATA _NULL_ステップは、このマクロの肝となる部分の一つです。work.VARデータセットの各行（つまり各変数）を読み込み、それに対応するマクロ変数を動的に生成します。\n\nset work.VAR end = eof;: work.VARデータセットを読み込みます。end=eofオプションは、データセットの最後のオブザベーションを読み込んだときに、eofという一時的な変数を1に設定します。\ncall symputx(\"VAR\"||strip(put(VARNUM, 8.))||\"_\", NAME, 'G');: CALL SYMPUTXルーチンは、データステップ内でSASマクロ変数を定義するために使用します。\n\n\"VAR\"||strip(put(VARNUM, 8.))||\"_\": ここでマクロ変数名が生成されます。\n\nVAR: プレフィックス\nstrip(put(VARNUM, 8.)): VARNUM（変数番号）を文字列に変換し、前後の空白を除去します。例えばVARNUMが1なら\"1\"になります。\n_: サフィックス 結果として、VAR1_, VAR2_, …, VARn_のようなマクロ変数が作成されます。\n\nNAME: これがマクロ変数に割り当てられる値、つまり元の変数名です。例えば、VAR1_には最初の変数名、VAR2_には2番目の変数名が格納されます。\n'G': スコープを指定します。'G'はグローバルマクロ変数として定義することを意味します。\n\nif eof then call symputx('MAXV',_N_);: データセットの最後のオブザベーションに到達したとき（eofが1のとき）に、MAXVというマクロ変数に_N_（データステップの現在のオブザベーション番号、ここでは変数番号の最大値）を格納します。これにより、データセット内の変数総数を取得できます。\n\nこのステップが完了すると、例えば元のデータセットにSUBJID, AGE, SEXという変数があった場合、以下のようなマクロ変数が生成されます。\n\n&VAR1_ = SUBJID\n&VAR2_ = AGE\n&VAR3_ = SEX\n&MAXV = 3\n\n\n\n\n/* 全変数の変数名を変更 (ex: subjid ⇒ _subjid) */\ndata work.&out. ;\n  set work.&raw. ;\n  rename %do i = 1 %to &MAXV.;\n  &&VAR&i._ = _&&VAR&i._\n  %end;\n  ;\nrun;\nいよいよ変数名の変更を行うコア部分です。DATAステップのRENAMEステートメントを、動的に生成されたマクロ変数を使って構築します。\n\ndata work.&out. ; set work.&raw. ;: 入力データセットwork.&rawを読み込み、work.&outとして新しいデータセットを作成します。\nrename %do i = 1 %to &MAXV.; ... %end;: この部分が、RENAMEステートメントをループ処理で動的に生成する肝です。\n\n%do i = 1 %to &MAXV.; ... %end;: iを1から&MAXV（変数総数）までループさせます。\n&&VAR&i._ = _&&VAR&i._:\n\n&&VAR&i._: これは二重間接参照です。\n\nまず&iが評価され、例えば1になります。\n次に&VAR1_が評価され、その値（例：SUBJID）が取得されます。 結果として、元の変数名（例：SUBJID）を指します。\n\n_&&VAR&i._: これが新しい変数名です。元の変数名の前にアンダースコア_を付けています。 結果として、SUBJID = _SUBJID、AGE = _AGE、SEX = _SEXといったRENAMEステートメントのリストがループによって生成されます。\nrename SUBJID = _SUBJID AGE = _AGE SEX = _SEX; のように展開されます。\n\n\n\nこの処理により、元のデータセットのすべての変数名が、定義されたルール（この場合は先頭に_を追加）に従って一括で変更されます。\n\n\n\nこの%rawdataマクロは、以下の点でSASプログラミングの効率と堅牢性を高めます。\n\n自動化と効率化: 手動で大量の変数名を変更する手間を省き、エラーのリスクを減らします。\n再利用性: どのようなデータセットに対しても、同じロジックで変数名を変換できます。\n命名規則の統一: プロジェクト全体で一貫した変数命名規則を強制するのに役立ちます。\n柔軟な対応: &&VAR&i._ = _&&VAR&i._ の部分を変更することで、NEWNAME = OLDNAME、OLDNAME = NEWNAMEなど、様々な変数名変換ロジックを適用できます。例えば、特定のサフィックスを追加したり、特定の文字列を置換したりすることも可能です。\n\n生データの前処理は解析の基盤です。このような自動化ツールを積極的に活用し、より質の高いデータ準備を目指しましょう。\n\n\n\n%macro rawdata(raw=, sort=, out=&raw);\n\n/* データセット定義情報の DS を作成 */\nproc contents data=work.&raw. out = work.VAR noprints;\nrun;\n\n/* 変数番号でソート */\nproc sort data=work.VAR ;\nby VARNUM ;\nrun;\n\n/* マクロ変数(VARn_)に変数を格納 */\ndata _null_;\n  set work.VAR end = eof;\n  call symputx(\"VAR\"||strip(put(VARNUM, 8.))||\"_\", NAME, 'G');\n  /* マクロ変数(MAXV)にオブザベーション数（変数の数）を格納 */\n  if eof then call symputx('MAXV',_N_);\nrun;\n\n/* 全変数の変数名を変更 (ex: subjid ⇒ _subjid) */\ndata work.&out. ;\n  set work.&raw. ;\n  rename %do i = 1 %to &MAXV.;\n  &&VAR&i._ = _&&VAR&i._\n  %end;\n  ;\nrun;\n\n%mend rawdata;"
  },
  {
    "objectID": "posts/statistics/2025/Proc_Contentsによる_nameの作成.html#sasマクロで生データをスマートに整形変数名変換の自動化テクニック",
    "href": "posts/statistics/2025/Proc_Contentsによる_nameの作成.html#sasマクロで生データをスマートに整形変数名変換の自動化テクニック",
    "title": "Proc Contentsを利用したRawデータの変数を_varにするマクロ",
    "section": "",
    "text": "SASで解析プロジェクトを進める際、最初に行う作業の一つが生データの加工です。特に、生データの変数名がSASの命名規則に厳密に準拠していなかったり、特定のプレフィックスやサフィックスを追加・削除したい場合が多くあります。\n今回ご紹介するSASマクロ%rawdataは、この「生データの変数名を自動で一括変換する」という、非常に実用的な処理を実現します。このテクニックをマスターすれば、手作業での変数名変更の手間を大幅に削減し、より効率的なデータ準備が可能になります。\n\n\nこのマクロの主な目的は、入力データセットの全変数に対して、以下のような処理を自動で適用することです。\n\nデータセットの変数情報（定義情報）を取得する\n変数情報を基に、各変数の古い名前と新しい名前のペアを作成する\nそのペアを使って、データセットの全変数名を一括で変更する\n\n特に注目すべきは、変数名の変更ロジックが_subjidのように、元の変数名にアンダースコア（_）をプレフィックスとして追加している点です。これは、特定の命名規則を強制したい場合に非常に有効です。\nそれでは、各セクションを詳しく見ていきましょう。\n\n\n\n%macro rawdata(raw=, sort=, out=&raw);\n\n/* データセット定義情報の DS を作成 */\nproc contents data=work.&raw. out = work.VAR noprints;\nrun;\n\n%macro rawdata(raw=, sort=, out=&raw);: %rawdataという名前のマクロを定義しています。引数は以下の3つです。\n\nraw=: 処理対象となる生データセット名を指定します。\nsort=: （このコードでは直接使用されていませんが、将来的な拡張性を示唆しています。）\nout=&raw: 処理結果の出力データセット名を指定します。デフォルトでは入力と同じ&rawになります。\n\nproc contents data=work.&raw. out = work.VAR noprints; run;: PROC CONTENTSプロシジャは、指定されたデータセット（work.&raw）の構造や変数に関する情報を取得し、その結果を新しいデータセット（work.VAR）に出力します。\n\nnoprints: 通常、PROC CONTENTSは結果をSAS出力ウィンドウに出力しますが、noprintsオプションを指定することで、この出力を抑制し、データセットへの出力のみを行います。 work.VARデータセットには、VARNUM（変数番号）、NAME（変数名）、LENGTH（変数長）、TYPE（変数型）などの情報が格納されます。このうち、今回は特にVARNUMとNAMEが重要になります。\n\n\n\n\n\n/* 変数番号でソート */\nproc sort data=work.VAR ;\nby VARNUM ;\nrun;\nPROC SORTプロシジャを使って、先ほど作成したwork.VARデータセットをVARNUM（変数番号）の昇順でソートします。これにより、後続の処理で変数を順番に扱うことが容易になります。SASは内部的に変数に番号を割り当てており、この番号順で処理することで、元のデータセットにおける変数の並び順を反映できます。\n\n\n\n/* マクロ変数(VARn_)に変数を格納 */\ndata _null_;\n  set work.VAR end = eof;\n  call symputx(\"VAR\"||strip(put(VARNUM, 8.))||\"_\", NAME, 'G');\n  /* マクロ変数(MAXV)にオブザベーション数（変数の数）を格納 */\n  if eof then call symputx('MAXV',_N_);\nrun;\nこのDATA _NULL_ステップは、このマクロの肝となる部分の一つです。work.VARデータセットの各行（つまり各変数）を読み込み、それに対応するマクロ変数を動的に生成します。\n\nset work.VAR end = eof;: work.VARデータセットを読み込みます。end=eofオプションは、データセットの最後のオブザベーションを読み込んだときに、eofという一時的な変数を1に設定します。\ncall symputx(\"VAR\"||strip(put(VARNUM, 8.))||\"_\", NAME, 'G');: CALL SYMPUTXルーチンは、データステップ内でSASマクロ変数を定義するために使用します。\n\n\"VAR\"||strip(put(VARNUM, 8.))||\"_\": ここでマクロ変数名が生成されます。\n\nVAR: プレフィックス\nstrip(put(VARNUM, 8.)): VARNUM（変数番号）を文字列に変換し、前後の空白を除去します。例えばVARNUMが1なら\"1\"になります。\n_: サフィックス 結果として、VAR1_, VAR2_, …, VARn_のようなマクロ変数が作成されます。\n\nNAME: これがマクロ変数に割り当てられる値、つまり元の変数名です。例えば、VAR1_には最初の変数名、VAR2_には2番目の変数名が格納されます。\n'G': スコープを指定します。'G'はグローバルマクロ変数として定義することを意味します。\n\nif eof then call symputx('MAXV',_N_);: データセットの最後のオブザベーションに到達したとき（eofが1のとき）に、MAXVというマクロ変数に_N_（データステップの現在のオブザベーション番号、ここでは変数番号の最大値）を格納します。これにより、データセット内の変数総数を取得できます。\n\nこのステップが完了すると、例えば元のデータセットにSUBJID, AGE, SEXという変数があった場合、以下のようなマクロ変数が生成されます。\n\n&VAR1_ = SUBJID\n&VAR2_ = AGE\n&VAR3_ = SEX\n&MAXV = 3\n\n\n\n\n/* 全変数の変数名を変更 (ex: subjid ⇒ _subjid) */\ndata work.&out. ;\n  set work.&raw. ;\n  rename %do i = 1 %to &MAXV.;\n  &&VAR&i._ = _&&VAR&i._\n  %end;\n  ;\nrun;\nいよいよ変数名の変更を行うコア部分です。DATAステップのRENAMEステートメントを、動的に生成されたマクロ変数を使って構築します。\n\ndata work.&out. ; set work.&raw. ;: 入力データセットwork.&rawを読み込み、work.&outとして新しいデータセットを作成します。\nrename %do i = 1 %to &MAXV.; ... %end;: この部分が、RENAMEステートメントをループ処理で動的に生成する肝です。\n\n%do i = 1 %to &MAXV.; ... %end;: iを1から&MAXV（変数総数）までループさせます。\n&&VAR&i._ = _&&VAR&i._:\n\n&&VAR&i._: これは二重間接参照です。\n\nまず&iが評価され、例えば1になります。\n次に&VAR1_が評価され、その値（例：SUBJID）が取得されます。 結果として、元の変数名（例：SUBJID）を指します。\n\n_&&VAR&i._: これが新しい変数名です。元の変数名の前にアンダースコア_を付けています。 結果として、SUBJID = _SUBJID、AGE = _AGE、SEX = _SEXといったRENAMEステートメントのリストがループによって生成されます。\nrename SUBJID = _SUBJID AGE = _AGE SEX = _SEX; のように展開されます。\n\n\n\nこの処理により、元のデータセットのすべての変数名が、定義されたルール（この場合は先頭に_を追加）に従って一括で変更されます。\n\n\n\nこの%rawdataマクロは、以下の点でSASプログラミングの効率と堅牢性を高めます。\n\n自動化と効率化: 手動で大量の変数名を変更する手間を省き、エラーのリスクを減らします。\n再利用性: どのようなデータセットに対しても、同じロジックで変数名を変換できます。\n命名規則の統一: プロジェクト全体で一貫した変数命名規則を強制するのに役立ちます。\n柔軟な対応: &&VAR&i._ = _&&VAR&i._ の部分を変更することで、NEWNAME = OLDNAME、OLDNAME = NEWNAMEなど、様々な変数名変換ロジックを適用できます。例えば、特定のサフィックスを追加したり、特定の文字列を置換したりすることも可能です。\n\n生データの前処理は解析の基盤です。このような自動化ツールを積極的に活用し、より質の高いデータ準備を目指しましょう。\n\n\n\n%macro rawdata(raw=, sort=, out=&raw);\n\n/* データセット定義情報の DS を作成 */\nproc contents data=work.&raw. out = work.VAR noprints;\nrun;\n\n/* 変数番号でソート */\nproc sort data=work.VAR ;\nby VARNUM ;\nrun;\n\n/* マクロ変数(VARn_)に変数を格納 */\ndata _null_;\n  set work.VAR end = eof;\n  call symputx(\"VAR\"||strip(put(VARNUM, 8.))||\"_\", NAME, 'G');\n  /* マクロ変数(MAXV)にオブザベーション数（変数の数）を格納 */\n  if eof then call symputx('MAXV',_N_);\nrun;\n\n/* 全変数の変数名を変更 (ex: subjid ⇒ _subjid) */\ndata work.&out. ;\n  set work.&raw. ;\n  rename %do i = 1 %to &MAXV.;\n  &&VAR&i._ = _&&VAR&i._\n  %end;\n  ;\nrun;\n\n%mend rawdata;"
  },
  {
    "objectID": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html",
    "href": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html",
    "title": "臨床試験における有害事象データの集計：PROC SQL",
    "section": "",
    "text": "臨床試験における有害事象（Adverse Event: AE）の集計は、薬剤の安全性評価において最も重要な分析の一つです。特に治療群間での比較は、薬剤の安全性プロファイルを理解する上で欠かせません。本記事では、標準的なADSL（Subject-Level Analysis Dataset）とADAE（Adverse Event Analysis Dataset）を使用して、Treatment群、Control群、Total の3つの観点からSOC（System Organ Class）/PT（Preferred Term）別の有害事象集計を実装する方法を詳しく解説します。\n標準データセットの作成 ADSLデータセット（被験者レベル）\n/* ADSL（被験者レベル分析データセット）の作成 */\ndata adsl;\n    length usubjid $20 subjid $10 arm $20 saffl $1;\n    \n    /* 50名の被験者データを作成 */\n    do i = 1 to 50;\n        usubjid = cats(\"STUDY001-\", put(i, z3.));\n        subjid = put(i, z3.);\n        \n        /* 治療群の割り当て（2:1でTreatment:Placebo） */\n        if mod(i, 3) = 0 then arm = \"Placebo\";\n        else arm = \"Treatment\";\n        \n        /* 安全性解析対象フラグ */\n        saffl = \"Y\";\n        \n        output;\n    end;\n    drop i;\nrun;\nDATA STEPの基本解説：\n\ndata adsl; - 新しいデータセット「adsl」を作成開始\nlength - 変数の型と最大長を事前定義（$は文字型、数値は文字数）\ndo i = 1 to 50; - 1から50まで繰り返し処理（50人の被験者作成）\ncats() - 複数の文字列を結合する関数（空白なしで連結）\nput(i, z3.) - 数値iを3桁のゼロパディング文字列に変換（001, 002, …）\nmod(i, 3) - iを3で割った余りを計算（0, 1, 2のサイクル）\noutput; - 現在の変数値でレコードを出力\ndrop i; - 作業用変数iを最終データセットから除外\nrun; - DATA STEPの実行\n\n\n\n/* ADAE（有害事象分析データセット）の作成 - より豊富なデータ */\ndata adae;\n    length usubjid $20 subjid $10 aesoc $50 aedecod $100 aeser $1 aerel $1 aestdy 8;\n    \n    /* 心臓障害の有害事象 */\n    usubjid = \"STUDY001-001\"; subjid = \"001\"; aesoc = \"心臓障害\"; aedecod = \"完全房室ブロック\"; aeser = \"Y\"; aerel = \"Y\"; aestdy = 15; output;\n    usubjid = \"STUDY001-001\"; subjid = \"001\"; aesoc = \"心臓障害\"; aedecod = \"徐脈\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 22; output;\n    \n    usubjid = \"STUDY001-002\"; subjid = \"002\"; aesoc = \"心臓障害\"; aedecod = \"心不全慢性\"; aeser = \"Y\"; aerel = \"Y\"; aestdy = 8; output;\n    usubjid = \"STUDY001-002\"; subjid = \"002\"; aesoc = \"心臓障害\"; aedecod = \"心不全\"; aeser = \"N\"; aerel = \"N\"; aestdy = 45; output;\n    \n    usubjid = \"STUDY001-004\"; subjid = \"004\"; aesoc = \"心臓障害\"; aedecod = \"動悸\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 3; output;\n    usubjid = \"STUDY001-007\"; subjid = \"007\"; aesoc = \"心臓障害\"; aedecod = \"洞停止\"; aeser = \"N\"; aerel = \"N\"; aestdy = 34; output;\n    usubjid = \"STUDY001-010\"; subjid = \"010\"; aesoc = \"心臓障害\"; aedecod = \"完全房室ブロック\"; aeser = \"Y\"; aerel = \"Y\"; aestdy = 25; output;\n    usubjid = \"STUDY001-013\"; subjid = \"013\"; aesoc = \"心臓障害\"; aedecod = \"徐脈\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 11; output;\n    usubjid = \"STUDY001-016\"; subjid = \"016\"; aesoc = \"心臓障害\"; aedecod = \"心房細動\"; aeser = \"Y\"; aerel = \"N\"; aestdy = 67; output;\n    usubjid = \"STUDY001-019\"; subjid = \"019\"; aesoc = \"心臓障害\"; aedecod = \"狭心症\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 44; output;\n    \n    /* 胃腸障害の有害事象 */\n    usubjid = \"STUDY001-005\"; subjid = \"005\"; aesoc = \"胃腸障害\"; aedecod = \"悪心\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 2; output;\n    usubjid = \"STUDY001-005\"; subjid = \"005\"; aesoc = \"胃腸障害\"; aedecod = \"嘔吐\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 5; output;\n    usubjid = \"STUDY001-008\"; subjid = \"008\"; aesoc = \"胃腸障害\"; aedecod = \"下痢\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 18; output;\n    usubjid = \"STUDY001-011\"; subjid = \"011\"; aesoc = \"胃腸障害\"; aedecod = \"便秘\"; aeser = \"N\"; aerel = \"N\"; aestdy = 28; output;\n    usubjid = \"STUDY001-014\"; subjid = \"014\"; aesoc = \"胃腸障害\"; aedecod = \"悪心\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 4; output;\n    usubjid = \"STUDY001-017\"; subjid = \"017\"; aesoc = \"胃腸障害\"; aedecod = \"腹痛\"; aeser = \"N\"; aerel = \"N\"; aestdy = 32; output;\n    usubjid = \"STUDY001-020\"; subjid = \"020\"; aesoc = \"胃腸障害\"; aedecod = \"消化不良\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 12; output;\n    \n    /* 神経系障害の有害事象 */\n    usubjid = \"STUDY001-006\"; subjid = \"006\"; aesoc = \"神経系障害\"; aedecod = \"頭痛\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 1; output;\n    usubjid = \"STUDY001-006\"; subjid = \"006\"; aesoc = \"神経系障害\"; aedecod = \"めまい\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 14; output;\n    usubjid = \"STUDY001-009\"; subjid = \"009\"; aesoc = \"神経系障害\"; aedecod = \"傾眠\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 7; output;\n    usubjid = \"STUDY001-012\"; subjid = \"012\"; aesoc = \"神経系障害\"; aedecod = \"頭痛\"; aeser = \"N\"; aerel = \"N\"; aestdy = 21; output;\n    usubjid = \"STUDY001-015\"; subjid = \"015\"; aesoc = \"神経系障害\"; aedecod = \"めまい\"; aeser = \"N\"; aerel = \"N\"; aestdy = 19; output;\n    usubjid = \"STUDY001-018\"; subjid = \"018\"; aesoc = \"神経系障害\"; aedecod = \"振戦\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 29; output;\n    \n    /* 皮膚および皮下組織障害 */\n    usubjid = \"STUDY001-021\"; subjid = \"021\"; aesoc = \"皮膚および皮下組織障害\"; aedecod = \"発疹\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 9; output;\n    usubjid = \"STUDY001-022\"; subjid = \"022\"; aesoc = \"皮膚および皮下組織障害\"; aedecod = \"そう痒症\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 16; output;\n    usubjid = \"STUDY001-023\"; subjid = \"023\"; aesoc = \"皮膚および皮下組織障害\"; aedecod = \"紅斑\"; aeser = \"N\"; aerel = \"N\"; aestdy = 23; output;\n    \n    /* 一般・全身障害および投与部位の状態 */\n    usubjid = \"STUDY001-024\"; subjid = \"024\"; aesoc = \"一般・全身障害および投与部位の状態\"; aedecod = \"疲労\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 5; output;\n    usubjid = \"STUDY001-025\"; subjid = \"025\"; aesoc = \"一般・全身障害および投与部位の状態\"; aedecod = \"発熱\"; aeser = \"N\"; aerel = \"N\"; aestdy = 13; output;\n    usubjid = \"STUDY001-026\"; subjid = \"026\"; aesoc = \"一般・全身障害および投与部位の状態\"; aedecod = \"無力症\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 27; output;\nrun;\nStatement解説：\n\nADAEデータの特徴：\n\naesoc: 器官別大分類（MedDRA SOC相当）\naedecod: 基本語（MedDRA PT相当）\naerel: 因果関係（Y=あり, N=なし）\naeser: 重篤性（Y=重篤, N=非重篤）\naestdy: 投与開始からの日数\n\n\n\n\n\n\n\n/* Step 3-1: 安全性解析対象データの準備と母集団サイズ取得 */\nproc sql;\n    /* 安全性解析対象のAEデータ抽出（ARM情報付き） */\n    create table ae_safety_arm as\n    select a.usubjid, a.aesoc, a.aedecod, a.aerel, a.aeser, \n           case when s.arm = \"Placebo\" then \"Control\" \n                else s.arm end as arm\n    from adae a\n    inner join adsl s on a.usubjid = s.usubjid and s.saffl = \"Y\";\n    \n    /* 各ARM別とTOTALの母集団サイズ取得 */\n    select count(*) into :total_n\n    from adsl\n    where saffl = \"Y\";\n    \n    select count(*) into :treatment_n\n    from adsl  \n    where saffl = \"Y\" and arm = \"Treatment\";\n    \n    select count(*) into :control_n\n    from adsl\n    where saffl = \"Y\" and arm = \"Placebo\";  /* 元データではPlacebo */\nquit;\nPROC SQLの基本解説：\n1. PROC SQLの開始と終了：\n\nproc sql; - SQLプロシジャの開始\nquit; - SQLプロシジャの終了（他のプロシジャはrun;だがSQLはquit;）\n\n2. CREATE TABLE文：\n\ncreate table ae_safety_arm as - 新しいテーブル「ae_safety_arm」を作成\nselect ... from ... where ...; の結果でテーブルを作成\n\n3. SELECT文の基本構造：\nselect 列名1, 列名2, 計算式 as 新しい列名 from テーブル名 where 条件;\n4. CASE文（条件分岐）：\ncase when 条件1 then 値1\n     else 値2 end as 新列名\n\nIF-THEN-ELSEのSQL版\ncase when s.arm = \"Placebo\" then \"Control\" - PlaceboをControlに表示変更\n\n5. INNER JOIN（内部結合）：\nfrom adae a\ninner join adsl s on a.usubjid = s.usubjid and s.saffl = \"Y\"\n\n2つのテーブルを結合\na, s はテーブルエイリアス（短縮名）\non の条件を満たすレコードのみ結果に含める\nand s.saffl = \"Y\" で安全性解析対象のみ抽出\n\n6. INTO句（マクロ変数への格納）：\nselect count(*) into :total_n\nfrom adsl\nwhere saffl = \"Y\";\n\ncount(*) - レコード数をカウント\ninto :total_n - 結果をマクロ変数&total_nに格納\n後で分母として使用\n\n\n\n\nproc sql;\n    create table ae_comprehensive as\n    \n    /* SOCレベル - ARM別 */\n    select aesoc, \"\" as aept, arm as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/\n               case when arm = \"Treatment\" then &treatment_n\n                    when arm = \"Control\" then &control_n\n                    else &total_n end)*100, 8.1)) || ')' as c,\n           case when arm = \"Treatment\" then 1\n                when arm = \"Control\" then 2\n                else 3 end as arm_order\n    from (select aesoc, usubjid, arm\n          from ae_safety_arm\n          group by aesoc, usubjid, arm) as subj_arm\n    group by aesoc, arm\n    \n    union all\n    \n    /* SOCレベル - Total */\n    select aesoc, \"\" as aept, \"Total\" as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/&total_n)*100, 8.1)) || ')' as c,\n           3 as arm_order\n    from (select aesoc, usubjid\n          from ae_safety_arm\n          group by aesoc, usubjid) as subj_total\n    group by aesoc\n    \n    union all\n    \n    /* PTレベル - ARM別 */\n    select aesoc, aedecod as aept, arm as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/\n               case when arm = \"Treatment\" then &treatment_n\n                    when arm = \"Control\" then &control_n\n                    else &total_n end)*100, 8.1)) || ')' as c,\n           case when arm = \"Treatment\" then 1\n                when arm = \"Control\" then 2\n                else 3 end as arm_order\n    from (select aesoc, aedecod, usubjid, arm\n          from ae_safety_arm\n          group by aesoc, aedecod, usubjid, arm) as subj_arm\n    group by aesoc, aedecod, arm\n    \n    union all\n    \n    /* PTレベル - Total */\n    select aesoc, aedecod as aept, \"Total\" as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/&total_n)*100, 8.1)) || ')' as c,\n           3 as arm_order\n    from (select aesoc, aedecod, usubjid\n          from ae_safety_arm\n          group by aesoc, aedecod, usubjid) as subj_total\n    group by aesoc, aedecod;\nquit;\n1. サブクエリ（副問い合わせ）：\nfrom (select aesoc, usubjid, arm\n      from ae_safety_arm\n      group by aesoc, usubjid, arm) as subj_arm\n\n() 内のSELECT文が先に実行される\nその結果をsubj_armという名前のテーブルとして使用\n重要な目的: 同一被験者の同一SOCで複数AEがある場合の重複除去\n\n2. GROUP BY句：\ngroup by aesoc, usubjid, arm\n\n指定した列の組み合わせでデータをグループ化\n各グループに対して集計関数（COUNT, SUMなど）を適用\n例：被験者001の心臓障害は1つのグループとして扱われる\n\n3. 集計関数：\ncount(distinct usubjid)  -- ユニークな被験者数をカウント\n4. 文字列処理：\ncompress(put(count(distinct usubjid), 8.)) || '(' || \ncompress(put((count(distinct usubjid)/&total_n)*100, 8.1)) || ')' as c\n\nput() - 数値を文字列に変換\ncompress() - 不要な空白を除去\n|| - 文字列結合演算子\n結果例：「5(10.0)」形式の文字列作成\n\n5. OUTER UNION CORRESPONDING：\nselect ... from ...\nunion all  \nselect ... from ...\n\n複数のSELECT結果を縦に結合\n\n\n\nall - 重複行も含めて全て結合\n\n\n\n\n/* Step 4-1: ソート処理（転置前に必要） */\nproc sort data=ae_comprehensive;\n    by aesoc aept arm_order arm_group;\nrun;\n\n/* Step 4-2: 転置処理 */\nproc transpose data=ae_comprehensive out=ae_arm_pivot;\n    by aesoc aept;\n    id arm_group;\n    var c;\nrun;\n重要なポイント：\n1. PROC SORTの必要性：\n\nPROC TRANSPOSEはBY変数で指定した順序でデータが並んでいることを要求\n事前にソートしないとエラーが発生\n\n2. PROC TRANSPOSEの解説：\n\ndata=ae_comprehensive - 入力データセット\nout=ae_arm_pivot - 出力データセット名\nby aesoc aept; - グループ化変数（これらの組み合わせごとに転置）\nid arm_group; - 新しい列名になる変数（Treatment, Control, Totalが列名になる）\nvar c; - 転置する値の変数\n\n転置前:\naesoc    aept  arm_group  c\n心臓障害  \"\"   Treatment  2(6.7)\n心臓障害  \"\"   Control    1(3.3)\n心臓障害  \"\"   Total      3(6.0)\n\n転置後:\naesoc    aept  Treatment  Control  Total\n心臓障害  \"\"   2(6.7)     1(3.3)   3(6.0)\n//* Step 4-3: 表示用整形 */\ndata final_display_ordered;\n    set ae_arm_pivot;\n    \n    /* 欠損値処理 */\n    if Treatment = \"\" then Treatment = \"0(0.0)\";\n    if Control = \"\" then Control = \"0(0.0)\";\n    if Total = \"\" then Total = \"0(0.0)\";\n    \n    /* 階層表示 */\n    length display_term $100;\n    if aept = \"\" then display_term = aesoc;\n    else display_term = \"  \" || aept;\n    \n    /* ソート用変数 */\n    if aept = \"\" then sort_level = 1;  /* SOCレベル */\n    else sort_level = 2;               /* PTレベル */\n    \n    drop _name_;\nrun;\nDATA STEPでの後処理解説：\n\nset ae_arm_pivot; - 入力データセットを読み込み\nif Y = \"\" then Y = \"0(0.0)\"; - 空文字列を”0(0.0)“に置換\n\"  \" || aept - PTの前に2つのスペースでインデント追加\n階層表示の仕組み:\n\n-    SOCレベル: `display_term = aesoc`（例：「心臓障害」）\n\n-    PTレベル: `display_term = \"  \" || aept`（例：「 完全房室ブロック」）\n\n\n\n\n/*======================================================================================*/\n/* プログラム名: 治療群別有害事象集計 (SOC/PT別) - シンプル版                           */\n/* 作成者: [作成者名]                                                                   */\n/* 作成日: [作成日]                                                                     */\n/* 目的: 臨床試験における有害事象データの治療群別集計                                   */\n/*      Treatment群、Control群、Total の3つの観点からSOC/PT別に集計                   */\n/*======================================================================================*/\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 1: ADSLデータセット（被験者レベル）の作成                                       */\n/*---------------------------------------------------------------------------------------*/\ndata adsl;\n    length usubjid $20 subjid $10 arm $20 saffl $1;\n    \n    /* 50名の被験者データを作成 */\n    do i = 1 to 50;\n        usubjid = cats(\"STUDY001-\", put(i, z3.));\n        subjid = put(i, z3.);\n        \n        /* 治療群の割り当て（2:1でTreatment:Placebo） */\n        if mod(i, 3) = 0 then arm = \"Placebo\";\n        else arm = \"Treatment\";\n        \n        /* 安全性解析対象フラグ */\n        saffl = \"Y\";\n        \n        output;\n    end;\n    drop i;\nrun;\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 2: ADAEデータセット（有害事象レベル）の作成                                     */\n/*---------------------------------------------------------------------------------------*/\ndata adae;\n    length usubjid $20 subjid $10 aesoc $50 aedecod $100 aeser $1 aerel $1 aestdy 8;\n    \n    /* 心臓障害の有害事象 */\n    usubjid = \"STUDY001-001\"; subjid = \"001\"; aesoc = \"心臓障害\"; aedecod = \"完全房室ブロック\"; aeser = \"Y\"; aerel = \"Y\"; aestdy = 15; output;\n    usubjid = \"STUDY001-001\"; subjid = \"001\"; aesoc = \"心臓障害\"; aedecod = \"徐脈\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 22; output;\n    \n    usubjid = \"STUDY001-002\"; subjid = \"002\"; aesoc = \"心臓障害\"; aedecod = \"心不全慢性\"; aeser = \"Y\"; aerel = \"Y\"; aestdy = 8; output;\n    usubjid = \"STUDY001-002\"; subjid = \"002\"; aesoc = \"心臓障害\"; aedecod = \"心不全\"; aeser = \"N\"; aerel = \"N\"; aestdy = 45; output;\n    \n    usubjid = \"STUDY001-004\"; subjid = \"004\"; aesoc = \"心臓障害\"; aedecod = \"動悸\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 3; output;\n    usubjid = \"STUDY001-007\"; subjid = \"007\"; aesoc = \"心臓障害\"; aedecod = \"洞停止\"; aeser = \"N\"; aerel = \"N\"; aestdy = 34; output;\n    usubjid = \"STUDY001-010\"; subjid = \"010\"; aesoc = \"心臓障害\"; aedecod = \"完全房室ブロック\"; aeser = \"Y\"; aerel = \"Y\"; aestdy = 25; output;\n    usubjid = \"STUDY001-013\"; subjid = \"013\"; aesoc = \"心臓障害\"; aedecod = \"徐脈\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 11; output;\n    usubjid = \"STUDY001-016\"; subjid = \"016\"; aesoc = \"心臓障害\"; aedecod = \"心房細動\"; aeser = \"Y\"; aerel = \"N\"; aestdy = 67; output;\n    usubjid = \"STUDY001-019\"; subjid = \"019\"; aesoc = \"心臓障害\"; aedecod = \"狭心症\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 44; output;\n    \n    /* 胃腸障害の有害事象 */\n    usubjid = \"STUDY001-005\"; subjid = \"005\"; aesoc = \"胃腸障害\"; aedecod = \"悪心\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 2; output;\n    usubjid = \"STUDY001-005\"; subjid = \"005\"; aesoc = \"胃腸障害\"; aedecod = \"嘔吐\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 5; output;\n    usubjid = \"STUDY001-008\"; subjid = \"008\"; aesoc = \"胃腸障害\"; aedecod = \"下痢\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 18; output;\n    usubjid = \"STUDY001-011\"; subjid = \"011\"; aesoc = \"胃腸障害\"; aedecod = \"便秘\"; aeser = \"N\"; aerel = \"N\"; aestdy = 28; output;\n    usubjid = \"STUDY001-014\"; subjid = \"014\"; aesoc = \"胃腸障害\"; aedecod = \"悪心\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 4; output;\n    usubjid = \"STUDY001-017\"; subjid = \"017\"; aesoc = \"胃腸障害\"; aedecod = \"腹痛\"; aeser = \"N\"; aerel = \"N\"; aestdy = 32; output;\n    usubjid = \"STUDY001-020\"; subjid = \"020\"; aesoc = \"胃腸障害\"; aedecod = \"消化不良\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 12; output;\n    \n    /* 神経系障害の有害事象 */\n    usubjid = \"STUDY001-006\"; subjid = \"006\"; aesoc = \"神経系障害\"; aedecod = \"頭痛\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 1; output;\n    usubjid = \"STUDY001-006\"; subjid = \"006\"; aesoc = \"神経系障害\"; aedecod = \"めまい\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 14; output;\n    usubjid = \"STUDY001-009\"; subjid = \"009\"; aesoc = \"神経系障害\"; aedecod = \"傾眠\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 7; output;\n    usubjid = \"STUDY001-012\"; subjid = \"012\"; aesoc = \"神経系障害\"; aedecod = \"頭痛\"; aeser = \"N\"; aerel = \"N\"; aestdy = 21; output;\n    usubjid = \"STUDY001-015\"; subjid = \"015\"; aesoc = \"神経系障害\"; aedecod = \"めまい\"; aeser = \"N\"; aerel = \"N\"; aestdy = 19; output;\n    usubjid = \"STUDY001-018\"; subjid = \"018\"; aesoc = \"神経系障害\"; aedecod = \"振戦\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 29; output;\n    \n    /* 皮膚および皮下組織障害 */\n    usubjid = \"STUDY001-021\"; subjid = \"021\"; aesoc = \"皮膚および皮下組織障害\"; aedecod = \"発疹\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 9; output;\n    usubjid = \"STUDY001-022\"; subjid = \"022\"; aesoc = \"皮膚および皮下組織障害\"; aedecod = \"そう痒症\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 16; output;\n    usubjid = \"STUDY001-023\"; subjid = \"023\"; aesoc = \"皮膚および皮下組織障害\"; aedecod = \"紅斑\"; aeser = \"N\"; aerel = \"N\"; aestdy = 23; output;\n    \n    /* 一般・全身障害および投与部位の状態 */\n    usubjid = \"STUDY001-024\"; subjid = \"024\"; aesoc = \"一般・全身障害および投与部位の状態\"; aedecod = \"疲労\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 5; output;\n    usubjid = \"STUDY001-025\"; subjid = \"025\"; aesoc = \"一般・全身障害および投与部位の状態\"; aedecod = \"発熱\"; aeser = \"N\"; aerel = \"N\"; aestdy = 13; output;\n    usubjid = \"STUDY001-026\"; subjid = \"026\"; aesoc = \"一般・全身障害および投与部位の状態\"; aedecod = \"無力症\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 27; output;\nrun;\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 3: 治療群別有害事象集計の実行                                                   */\n/*---------------------------------------------------------------------------------------*/\n\n/* Step 3-1: 安全性解析対象データの準備と母集団サイズ取得 */\nproc sql;\n    /* 安全性解析対象のAEデータ抽出（ARM情報付き） */\n    create table ae_safety_arm as\n    select a.usubjid, a.aesoc, a.aedecod, a.aerel, a.aeser, \n           case when s.arm = \"Placebo\" then \"Control\" \n                else s.arm end as arm\n    from adae a\n    inner join adsl s on a.usubjid = s.usubjid and s.saffl = \"Y\";\n    \n    /* 各ARM別とTOTALの母集団サイズ取得 */\n    select count(*) into :total_n\n    from adsl\n    where saffl = \"Y\";\n    \n    select count(*) into :treatment_n\n    from adsl  \n    where saffl = \"Y\" and arm = \"Treatment\";\n    \n    select count(*) into :control_n\n    from adsl\n    where saffl = \"Y\" and arm = \"Placebo\";  /* 元データではPlacebo */\nquit;\n\n/* Step 3-2: 治療群別SOC/PT集計の実行 */\nproc sql;\n    create table ae_comprehensive as\n    \n    /* SOCレベル - ARM別 */\n    select aesoc, \"\" as aept, arm as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/\n               case when arm = \"Treatment\" then &treatment_n\n                    when arm = \"Control\" then &control_n\n                    else &total_n end)*100, 8.1)) || ')' as c,\n           case when arm = \"Treatment\" then 1\n                when arm = \"Control\" then 2\n                else 3 end as arm_order\n    from (select aesoc, usubjid, arm\n          from ae_safety_arm\n          group by aesoc, usubjid, arm) as subj_arm\n    group by aesoc, arm\n    \n    union all\n    \n    /* SOCレベル - Total */\n    select aesoc, \"\" as aept, \"Total\" as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/&total_n)*100, 8.1)) || ')' as c,\n           3 as arm_order\n    from (select aesoc, usubjid\n          from ae_safety_arm\n          group by aesoc, usubjid) as subj_total\n    group by aesoc\n    \n    union all\n    \n    /* PTレベル - ARM別 */\n    select aesoc, aedecod as aept, arm as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/\n               case when arm = \"Treatment\" then &treatment_n\n                    when arm = \"Control\" then &control_n\n                    else &total_n end)*100, 8.1)) || ')' as c,\n           case when arm = \"Treatment\" then 1\n                when arm = \"Control\" then 2\n                else 3 end as arm_order\n    from (select aesoc, aedecod, usubjid, arm\n          from ae_safety_arm\n          group by aesoc, aedecod, usubjid, arm) as subj_arm\n    group by aesoc, aedecod, arm\n    \n    union all\n    \n    /* PTレベル - Total */\n    select aesoc, aedecod as aept, \"Total\" as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/&total_n)*100, 8.1)) || ')' as c,\n           3 as arm_order\n    from (select aesoc, aedecod, usubjid\n          from ae_safety_arm\n          group by aesoc, aedecod, usubjid) as subj_total\n    group by aesoc, aedecod;\nquit;\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 4: データの転置と整形                                                           */\n/*---------------------------------------------------------------------------------------*/\n\n/* Step 4-1: ソート処理（転置前に必要） */\nproc sort data=ae_comprehensive;\n    by aesoc aept arm_order arm_group;\nrun;\n\n/* Step 4-2: 転置処理 */\nproc transpose data=ae_comprehensive out=ae_arm_pivot;\n    by aesoc aept;\n    id arm_group;\n    var c;\nrun;\n\n/* Step 4-3: 表示用整形 */\ndata final_display_ordered;\n    retain display_term Treatment Control Total;  /* 列順序の明示的制御 */\n    set ae_arm_pivot;\n    \n    /* 欠損値処理 */\n    if Treatment = \"\" then Treatment = \"0(0.0)\";\n    if Control = \"\" then Control = \"0(0.0)\";\n    if Total = \"\" then Total = \"0(0.0)\";\n    \n    /* 階層表示 */\n    length display_term $100;\n    if aept = \"\" then display_term = aesoc;\n    else display_term = \"  \" || aept;\n    \n    /* ソート用変数 */\n    if aept = \"\" then sort_level = 1;  /* SOCレベル */\n    else sort_level = 2;               /* PTレベル */\n    \n    drop _name_;\nrun;\n\n/* Step 4-4: 最終ソート */\nproc sort data=final_display_ordered;\n    by aesoc sort_level aept;\nrun;\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 5: 結果表示                                                                     */\n/*---------------------------------------------------------------------------------------*/\n\n/* Step 5-1: 最終結果の表示 */\nproc print data=final_display_ordered noobs;\n    title1 \"有害事象集計表（治療群別）\";\n    title3 \"被験者数 (%)\";\nrun;\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 6: 検証用出力                                                                   */\n/*---------------------------------------------------------------------------------------*/\n\n/* Step 6-1: 基本統計の確認 */\nproc sql;\n    title \"データ整合性チェック\";\n    select \"元AE総件数\" as check_point, \n           count(*) as count_value\n    from adae\n    \n    union all\n    \n    select \"安全性解析対象AE件数\" as check_point,\n           count(*) as count_value\n    from ae_safety_arm\n    \n    union all\n    \n    select \"Treatment群被験者数\" as check_point,\n           &treatment_n as count_value from (select 1 as dummy)\n           \n    union all\n    \n    select \"Control群被験者数\" as check_point,\n           &control_n as count_value from (select 1 as dummy)\n           \n    union all\n    \n    select \"Total被験者数\" as check_point,\n           &total_n as count_value from (select 1 as dummy);\nquit;\n\n/* Step 6-2: SOC別詳細検証 */\nproc sql;\n   title \"SOC別被験者数検証\";\n   select aesoc,\n          count(distinct case when arm = \"Treatment\" then usubjid else null end) as treatment_subj,\n          count(distinct case when arm = \"Control\" then usubjid else null end) as control_subj,\n          count(distinct usubjid) as total_subj\n   from ae_safety_arm\n   group by aesoc\n   order by total_subj desc;\nquit;\n\n/* Step 6-3: 治療群配分の確認 */\nproc freq data=adsl;\n   tables arm / nocum;\n   title \"治療群配分\";\nrun;\n\n\n\n/*---------------------------------------------------------------------------------------*/\n/* プログラム終了                                                                       */\n/*---------------------------------------------------------------------------------------*/\n\n/* タイトルクリア */\ntitle;\n\n/* マクロ変数の確認（ログ出力） */\n%put NOTE: Treatment群被験者数 = &treatment_n;\n%put NOTE: Control群被験者数 = &control_n;\n%put NOTE: Total被験者数 = &total_n;\n\n%put NOTE: 治療群別有害事象集計プログラム実行完了;\n\n/*======================================================================================*/\n/* プログラム終了                                                                       */\n/* 出力データセット:                                                                    */\n/*   - final_display_ordered: 最終的な治療群別集計表                                   */\n/*   - ae_safety_arm: 安全性解析対象AEデータ                                           */\n/*   - ae_comprehensive: 中間集計データ                                                */\n/*======================================================================================*/\n\n\n\n/* 集計結果の検証 */\nproc sql;\n    /* 元データとの整合性チェック */\n    select \"元AE総件数\" as check_point, \n           count(*) as count_value\n    from adae\n    \n    union all\n    \n    select \"安全性解析対象AE件数\" as check_point,\n           count(*) as count_value\n    from ae_safety_arm\n    \n    union all\n    \n    select \"Treatment群被験者数\" as check_point,\n           &treatment_n as count_value\n           \n    union all\n    \n    select \"Control群被験者数\" as check_point,\n           &control_n as count_value\n           \n    union all\n    \n    select \"Total被験者数\" as check_point,\n           &total_n as count_value;\nquit;\n検証SQLの解説：\n\nunion all - 複数のSELECT結果を縦に結合（重複も含める）\n&treatment_n - 事前に計算したマクロ変数の値を表示\n検証の重要性: 各ステップで期待する件数が得られているか確認\n\n/* SOC別被験者数の詳細検証 */\nproc sql;\n    select aesoc,\n           count(distinct case when arm = \"Treatment\" then usubjid end) as treatment_subj,\n           count(distinct case when arm = \"Control\" then usubjid end) as control_subj,\n           count(distinct usubjid) as total_subj\n    from ae_safety_arm\n    where aerel = \"Y\"  /* 因果関係ありのみ */\n    group by aesoc\n    order by total_subj desc;\nquit;\nCASE文の高度な使用：\ncount(distinct case when arm = \"Treatment\" then usubjid end)\n\ncase when ... then ... end - 条件を満たす場合のみ値を返す\nTreatment群の場合のみusubjidをカウント対象にする\n1つのクエリで治療群別の集計が可能\n\n\n\n\n\n\nSELECT 何を選ぶか\nFROM どのテーブルから  \nWHERE どんな条件で\nGROUP BY どうグループ化するか\nORDER BY どう並び替えるか\n\n\n\n/* 内部結合の例 */\nfrom adae a                    -- メインテーブル\ninner join adsl s              -- 結合するテーブル  \non a.usubjid = s.usubjid       -- 結合条件\nand s.saffl = \"Y\"              -- 追加フィルタ\n\n\n\n\ncount(*) - 全行数\ncount(distinct 列名) - ユニークな値の数\nsum() - 合計\nmin(), max() - 最小値、最大値\n\n\n\n\nfrom (select ... from ... group by ...) as 別名\n\n内側のクエリから読む\n外側のクエリは内側の結果を使用\n\n\n\n\n\n本記事では、臨床試験における有害事象データの治療群別集計を、PROC SQLを用いて段階的に実装しました。初心者の方にとって重要なポイントは：\n\n\n\n小さく始める: 複雑なクエリは段階的に構築\n中間結果確認: 各ステップでPROC PRINTで結果確認\nエラー対処: エラーメッセージから問題箇所を特定\nコメント活用: 処理の目的を明記\n\nこの段階的アプローチにより、初心者でも確実に治療群別有害事象集計をマスターできます。重要なのは、各ステップの目的を理解しながら進めることです。"
  },
  {
    "objectID": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html#治療群別有害事象集計の実装",
    "href": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html#治療群別有害事象集計の実装",
    "title": "臨床試験における有害事象データの集計：PROC SQL",
    "section": "",
    "text": "/* Step 3-1: 安全性解析対象データの準備と母集団サイズ取得 */\nproc sql;\n    /* 安全性解析対象のAEデータ抽出（ARM情報付き） */\n    create table ae_safety_arm as\n    select a.usubjid, a.aesoc, a.aedecod, a.aerel, a.aeser, \n           case when s.arm = \"Placebo\" then \"Control\" \n                else s.arm end as arm\n    from adae a\n    inner join adsl s on a.usubjid = s.usubjid and s.saffl = \"Y\";\n    \n    /* 各ARM別とTOTALの母集団サイズ取得 */\n    select count(*) into :total_n\n    from adsl\n    where saffl = \"Y\";\n    \n    select count(*) into :treatment_n\n    from adsl  \n    where saffl = \"Y\" and arm = \"Treatment\";\n    \n    select count(*) into :control_n\n    from adsl\n    where saffl = \"Y\" and arm = \"Placebo\";  /* 元データではPlacebo */\nquit;\nPROC SQLの基本解説：\n1. PROC SQLの開始と終了：\n\nproc sql; - SQLプロシジャの開始\nquit; - SQLプロシジャの終了（他のプロシジャはrun;だがSQLはquit;）\n\n2. CREATE TABLE文：\n\ncreate table ae_safety_arm as - 新しいテーブル「ae_safety_arm」を作成\nselect ... from ... where ...; の結果でテーブルを作成\n\n3. SELECT文の基本構造：\nselect 列名1, 列名2, 計算式 as 新しい列名 from テーブル名 where 条件;\n4. CASE文（条件分岐）：\ncase when 条件1 then 値1\n     else 値2 end as 新列名\n\nIF-THEN-ELSEのSQL版\ncase when s.arm = \"Placebo\" then \"Control\" - PlaceboをControlに表示変更\n\n5. INNER JOIN（内部結合）：\nfrom adae a\ninner join adsl s on a.usubjid = s.usubjid and s.saffl = \"Y\"\n\n2つのテーブルを結合\na, s はテーブルエイリアス（短縮名）\non の条件を満たすレコードのみ結果に含める\nand s.saffl = \"Y\" で安全性解析対象のみ抽出\n\n6. INTO句（マクロ変数への格納）：\nselect count(*) into :total_n\nfrom adsl\nwhere saffl = \"Y\";\n\ncount(*) - レコード数をカウント\ninto :total_n - 結果をマクロ変数&total_nに格納\n後で分母として使用\n\n\n\n\nproc sql;\n    create table ae_comprehensive as\n    \n    /* SOCレベル - ARM別 */\n    select aesoc, \"\" as aept, arm as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/\n               case when arm = \"Treatment\" then &treatment_n\n                    when arm = \"Control\" then &control_n\n                    else &total_n end)*100, 8.1)) || ')' as c,\n           case when arm = \"Treatment\" then 1\n                when arm = \"Control\" then 2\n                else 3 end as arm_order\n    from (select aesoc, usubjid, arm\n          from ae_safety_arm\n          group by aesoc, usubjid, arm) as subj_arm\n    group by aesoc, arm\n    \n    union all\n    \n    /* SOCレベル - Total */\n    select aesoc, \"\" as aept, \"Total\" as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/&total_n)*100, 8.1)) || ')' as c,\n           3 as arm_order\n    from (select aesoc, usubjid\n          from ae_safety_arm\n          group by aesoc, usubjid) as subj_total\n    group by aesoc\n    \n    union all\n    \n    /* PTレベル - ARM別 */\n    select aesoc, aedecod as aept, arm as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/\n               case when arm = \"Treatment\" then &treatment_n\n                    when arm = \"Control\" then &control_n\n                    else &total_n end)*100, 8.1)) || ')' as c,\n           case when arm = \"Treatment\" then 1\n                when arm = \"Control\" then 2\n                else 3 end as arm_order\n    from (select aesoc, aedecod, usubjid, arm\n          from ae_safety_arm\n          group by aesoc, aedecod, usubjid, arm) as subj_arm\n    group by aesoc, aedecod, arm\n    \n    union all\n    \n    /* PTレベル - Total */\n    select aesoc, aedecod as aept, \"Total\" as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/&total_n)*100, 8.1)) || ')' as c,\n           3 as arm_order\n    from (select aesoc, aedecod, usubjid\n          from ae_safety_arm\n          group by aesoc, aedecod, usubjid) as subj_total\n    group by aesoc, aedecod;\nquit;\n1. サブクエリ（副問い合わせ）：\nfrom (select aesoc, usubjid, arm\n      from ae_safety_arm\n      group by aesoc, usubjid, arm) as subj_arm\n\n() 内のSELECT文が先に実行される\nその結果をsubj_armという名前のテーブルとして使用\n重要な目的: 同一被験者の同一SOCで複数AEがある場合の重複除去\n\n2. GROUP BY句：\ngroup by aesoc, usubjid, arm\n\n指定した列の組み合わせでデータをグループ化\n各グループに対して集計関数（COUNT, SUMなど）を適用\n例：被験者001の心臓障害は1つのグループとして扱われる\n\n3. 集計関数：\ncount(distinct usubjid)  -- ユニークな被験者数をカウント\n4. 文字列処理：\ncompress(put(count(distinct usubjid), 8.)) || '(' || \ncompress(put((count(distinct usubjid)/&total_n)*100, 8.1)) || ')' as c\n\nput() - 数値を文字列に変換\ncompress() - 不要な空白を除去\n|| - 文字列結合演算子\n結果例：「5(10.0)」形式の文字列作成\n\n5. OUTER UNION CORRESPONDING：\nselect ... from ...\nunion all  \nselect ... from ...\n\n複数のSELECT結果を縦に結合\n\n\n\nall - 重複行も含めて全て結合\n\n\n\n\n/* Step 4-1: ソート処理（転置前に必要） */\nproc sort data=ae_comprehensive;\n    by aesoc aept arm_order arm_group;\nrun;\n\n/* Step 4-2: 転置処理 */\nproc transpose data=ae_comprehensive out=ae_arm_pivot;\n    by aesoc aept;\n    id arm_group;\n    var c;\nrun;\n重要なポイント：\n1. PROC SORTの必要性：\n\nPROC TRANSPOSEはBY変数で指定した順序でデータが並んでいることを要求\n事前にソートしないとエラーが発生\n\n2. PROC TRANSPOSEの解説：\n\ndata=ae_comprehensive - 入力データセット\nout=ae_arm_pivot - 出力データセット名\nby aesoc aept; - グループ化変数（これらの組み合わせごとに転置）\nid arm_group; - 新しい列名になる変数（Treatment, Control, Totalが列名になる）\nvar c; - 転置する値の変数\n\n転置前:\naesoc    aept  arm_group  c\n心臓障害  \"\"   Treatment  2(6.7)\n心臓障害  \"\"   Control    1(3.3)\n心臓障害  \"\"   Total      3(6.0)\n\n転置後:\naesoc    aept  Treatment  Control  Total\n心臓障害  \"\"   2(6.7)     1(3.3)   3(6.0)\n//* Step 4-3: 表示用整形 */\ndata final_display_ordered;\n    set ae_arm_pivot;\n    \n    /* 欠損値処理 */\n    if Treatment = \"\" then Treatment = \"0(0.0)\";\n    if Control = \"\" then Control = \"0(0.0)\";\n    if Total = \"\" then Total = \"0(0.0)\";\n    \n    /* 階層表示 */\n    length display_term $100;\n    if aept = \"\" then display_term = aesoc;\n    else display_term = \"  \" || aept;\n    \n    /* ソート用変数 */\n    if aept = \"\" then sort_level = 1;  /* SOCレベル */\n    else sort_level = 2;               /* PTレベル */\n    \n    drop _name_;\nrun;\nDATA STEPでの後処理解説：\n\nset ae_arm_pivot; - 入力データセットを読み込み\nif Y = \"\" then Y = \"0(0.0)\"; - 空文字列を”0(0.0)“に置換\n\"  \" || aept - PTの前に2つのスペースでインデント追加\n階層表示の仕組み:\n\n-    SOCレベル: `display_term = aesoc`（例：「心臓障害」）\n\n-    PTレベル: `display_term = \"  \" || aept`（例：「 完全房室ブロック」）"
  },
  {
    "objectID": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html#修正されたappendixプログラム列順序修正版",
    "href": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html#修正されたappendixプログラム列順序修正版",
    "title": "臨床試験における有害事象データの集計：PROC SQL",
    "section": "",
    "text": "/*======================================================================================*/\n/* プログラム名: 治療群別有害事象集計 (SOC/PT別) - シンプル版                           */\n/* 作成者: [作成者名]                                                                   */\n/* 作成日: [作成日]                                                                     */\n/* 目的: 臨床試験における有害事象データの治療群別集計                                   */\n/*      Treatment群、Control群、Total の3つの観点からSOC/PT別に集計                   */\n/*======================================================================================*/\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 1: ADSLデータセット（被験者レベル）の作成                                       */\n/*---------------------------------------------------------------------------------------*/\ndata adsl;\n    length usubjid $20 subjid $10 arm $20 saffl $1;\n    \n    /* 50名の被験者データを作成 */\n    do i = 1 to 50;\n        usubjid = cats(\"STUDY001-\", put(i, z3.));\n        subjid = put(i, z3.);\n        \n        /* 治療群の割り当て（2:1でTreatment:Placebo） */\n        if mod(i, 3) = 0 then arm = \"Placebo\";\n        else arm = \"Treatment\";\n        \n        /* 安全性解析対象フラグ */\n        saffl = \"Y\";\n        \n        output;\n    end;\n    drop i;\nrun;\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 2: ADAEデータセット（有害事象レベル）の作成                                     */\n/*---------------------------------------------------------------------------------------*/\ndata adae;\n    length usubjid $20 subjid $10 aesoc $50 aedecod $100 aeser $1 aerel $1 aestdy 8;\n    \n    /* 心臓障害の有害事象 */\n    usubjid = \"STUDY001-001\"; subjid = \"001\"; aesoc = \"心臓障害\"; aedecod = \"完全房室ブロック\"; aeser = \"Y\"; aerel = \"Y\"; aestdy = 15; output;\n    usubjid = \"STUDY001-001\"; subjid = \"001\"; aesoc = \"心臓障害\"; aedecod = \"徐脈\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 22; output;\n    \n    usubjid = \"STUDY001-002\"; subjid = \"002\"; aesoc = \"心臓障害\"; aedecod = \"心不全慢性\"; aeser = \"Y\"; aerel = \"Y\"; aestdy = 8; output;\n    usubjid = \"STUDY001-002\"; subjid = \"002\"; aesoc = \"心臓障害\"; aedecod = \"心不全\"; aeser = \"N\"; aerel = \"N\"; aestdy = 45; output;\n    \n    usubjid = \"STUDY001-004\"; subjid = \"004\"; aesoc = \"心臓障害\"; aedecod = \"動悸\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 3; output;\n    usubjid = \"STUDY001-007\"; subjid = \"007\"; aesoc = \"心臓障害\"; aedecod = \"洞停止\"; aeser = \"N\"; aerel = \"N\"; aestdy = 34; output;\n    usubjid = \"STUDY001-010\"; subjid = \"010\"; aesoc = \"心臓障害\"; aedecod = \"完全房室ブロック\"; aeser = \"Y\"; aerel = \"Y\"; aestdy = 25; output;\n    usubjid = \"STUDY001-013\"; subjid = \"013\"; aesoc = \"心臓障害\"; aedecod = \"徐脈\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 11; output;\n    usubjid = \"STUDY001-016\"; subjid = \"016\"; aesoc = \"心臓障害\"; aedecod = \"心房細動\"; aeser = \"Y\"; aerel = \"N\"; aestdy = 67; output;\n    usubjid = \"STUDY001-019\"; subjid = \"019\"; aesoc = \"心臓障害\"; aedecod = \"狭心症\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 44; output;\n    \n    /* 胃腸障害の有害事象 */\n    usubjid = \"STUDY001-005\"; subjid = \"005\"; aesoc = \"胃腸障害\"; aedecod = \"悪心\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 2; output;\n    usubjid = \"STUDY001-005\"; subjid = \"005\"; aesoc = \"胃腸障害\"; aedecod = \"嘔吐\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 5; output;\n    usubjid = \"STUDY001-008\"; subjid = \"008\"; aesoc = \"胃腸障害\"; aedecod = \"下痢\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 18; output;\n    usubjid = \"STUDY001-011\"; subjid = \"011\"; aesoc = \"胃腸障害\"; aedecod = \"便秘\"; aeser = \"N\"; aerel = \"N\"; aestdy = 28; output;\n    usubjid = \"STUDY001-014\"; subjid = \"014\"; aesoc = \"胃腸障害\"; aedecod = \"悪心\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 4; output;\n    usubjid = \"STUDY001-017\"; subjid = \"017\"; aesoc = \"胃腸障害\"; aedecod = \"腹痛\"; aeser = \"N\"; aerel = \"N\"; aestdy = 32; output;\n    usubjid = \"STUDY001-020\"; subjid = \"020\"; aesoc = \"胃腸障害\"; aedecod = \"消化不良\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 12; output;\n    \n    /* 神経系障害の有害事象 */\n    usubjid = \"STUDY001-006\"; subjid = \"006\"; aesoc = \"神経系障害\"; aedecod = \"頭痛\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 1; output;\n    usubjid = \"STUDY001-006\"; subjid = \"006\"; aesoc = \"神経系障害\"; aedecod = \"めまい\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 14; output;\n    usubjid = \"STUDY001-009\"; subjid = \"009\"; aesoc = \"神経系障害\"; aedecod = \"傾眠\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 7; output;\n    usubjid = \"STUDY001-012\"; subjid = \"012\"; aesoc = \"神経系障害\"; aedecod = \"頭痛\"; aeser = \"N\"; aerel = \"N\"; aestdy = 21; output;\n    usubjid = \"STUDY001-015\"; subjid = \"015\"; aesoc = \"神経系障害\"; aedecod = \"めまい\"; aeser = \"N\"; aerel = \"N\"; aestdy = 19; output;\n    usubjid = \"STUDY001-018\"; subjid = \"018\"; aesoc = \"神経系障害\"; aedecod = \"振戦\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 29; output;\n    \n    /* 皮膚および皮下組織障害 */\n    usubjid = \"STUDY001-021\"; subjid = \"021\"; aesoc = \"皮膚および皮下組織障害\"; aedecod = \"発疹\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 9; output;\n    usubjid = \"STUDY001-022\"; subjid = \"022\"; aesoc = \"皮膚および皮下組織障害\"; aedecod = \"そう痒症\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 16; output;\n    usubjid = \"STUDY001-023\"; subjid = \"023\"; aesoc = \"皮膚および皮下組織障害\"; aedecod = \"紅斑\"; aeser = \"N\"; aerel = \"N\"; aestdy = 23; output;\n    \n    /* 一般・全身障害および投与部位の状態 */\n    usubjid = \"STUDY001-024\"; subjid = \"024\"; aesoc = \"一般・全身障害および投与部位の状態\"; aedecod = \"疲労\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 5; output;\n    usubjid = \"STUDY001-025\"; subjid = \"025\"; aesoc = \"一般・全身障害および投与部位の状態\"; aedecod = \"発熱\"; aeser = \"N\"; aerel = \"N\"; aestdy = 13; output;\n    usubjid = \"STUDY001-026\"; subjid = \"026\"; aesoc = \"一般・全身障害および投与部位の状態\"; aedecod = \"無力症\"; aeser = \"N\"; aerel = \"Y\"; aestdy = 27; output;\nrun;\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 3: 治療群別有害事象集計の実行                                                   */\n/*---------------------------------------------------------------------------------------*/\n\n/* Step 3-1: 安全性解析対象データの準備と母集団サイズ取得 */\nproc sql;\n    /* 安全性解析対象のAEデータ抽出（ARM情報付き） */\n    create table ae_safety_arm as\n    select a.usubjid, a.aesoc, a.aedecod, a.aerel, a.aeser, \n           case when s.arm = \"Placebo\" then \"Control\" \n                else s.arm end as arm\n    from adae a\n    inner join adsl s on a.usubjid = s.usubjid and s.saffl = \"Y\";\n    \n    /* 各ARM別とTOTALの母集団サイズ取得 */\n    select count(*) into :total_n\n    from adsl\n    where saffl = \"Y\";\n    \n    select count(*) into :treatment_n\n    from adsl  \n    where saffl = \"Y\" and arm = \"Treatment\";\n    \n    select count(*) into :control_n\n    from adsl\n    where saffl = \"Y\" and arm = \"Placebo\";  /* 元データではPlacebo */\nquit;\n\n/* Step 3-2: 治療群別SOC/PT集計の実行 */\nproc sql;\n    create table ae_comprehensive as\n    \n    /* SOCレベル - ARM別 */\n    select aesoc, \"\" as aept, arm as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/\n               case when arm = \"Treatment\" then &treatment_n\n                    when arm = \"Control\" then &control_n\n                    else &total_n end)*100, 8.1)) || ')' as c,\n           case when arm = \"Treatment\" then 1\n                when arm = \"Control\" then 2\n                else 3 end as arm_order\n    from (select aesoc, usubjid, arm\n          from ae_safety_arm\n          group by aesoc, usubjid, arm) as subj_arm\n    group by aesoc, arm\n    \n    union all\n    \n    /* SOCレベル - Total */\n    select aesoc, \"\" as aept, \"Total\" as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/&total_n)*100, 8.1)) || ')' as c,\n           3 as arm_order\n    from (select aesoc, usubjid\n          from ae_safety_arm\n          group by aesoc, usubjid) as subj_total\n    group by aesoc\n    \n    union all\n    \n    /* PTレベル - ARM別 */\n    select aesoc, aedecod as aept, arm as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/\n               case when arm = \"Treatment\" then &treatment_n\n                    when arm = \"Control\" then &control_n\n                    else &total_n end)*100, 8.1)) || ')' as c,\n           case when arm = \"Treatment\" then 1\n                when arm = \"Control\" then 2\n                else 3 end as arm_order\n    from (select aesoc, aedecod, usubjid, arm\n          from ae_safety_arm\n          group by aesoc, aedecod, usubjid, arm) as subj_arm\n    group by aesoc, aedecod, arm\n    \n    union all\n    \n    /* PTレベル - Total */\n    select aesoc, aedecod as aept, \"Total\" as arm_group,\n           compress(put(count(distinct usubjid), 8.)) || '(' || \n           compress(put((count(distinct usubjid)/&total_n)*100, 8.1)) || ')' as c,\n           3 as arm_order\n    from (select aesoc, aedecod, usubjid\n          from ae_safety_arm\n          group by aesoc, aedecod, usubjid) as subj_total\n    group by aesoc, aedecod;\nquit;\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 4: データの転置と整形                                                           */\n/*---------------------------------------------------------------------------------------*/\n\n/* Step 4-1: ソート処理（転置前に必要） */\nproc sort data=ae_comprehensive;\n    by aesoc aept arm_order arm_group;\nrun;\n\n/* Step 4-2: 転置処理 */\nproc transpose data=ae_comprehensive out=ae_arm_pivot;\n    by aesoc aept;\n    id arm_group;\n    var c;\nrun;\n\n/* Step 4-3: 表示用整形 */\ndata final_display_ordered;\n    retain display_term Treatment Control Total;  /* 列順序の明示的制御 */\n    set ae_arm_pivot;\n    \n    /* 欠損値処理 */\n    if Treatment = \"\" then Treatment = \"0(0.0)\";\n    if Control = \"\" then Control = \"0(0.0)\";\n    if Total = \"\" then Total = \"0(0.0)\";\n    \n    /* 階層表示 */\n    length display_term $100;\n    if aept = \"\" then display_term = aesoc;\n    else display_term = \"  \" || aept;\n    \n    /* ソート用変数 */\n    if aept = \"\" then sort_level = 1;  /* SOCレベル */\n    else sort_level = 2;               /* PTレベル */\n    \n    drop _name_;\nrun;\n\n/* Step 4-4: 最終ソート */\nproc sort data=final_display_ordered;\n    by aesoc sort_level aept;\nrun;\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 5: 結果表示                                                                     */\n/*---------------------------------------------------------------------------------------*/\n\n/* Step 5-1: 最終結果の表示 */\nproc print data=final_display_ordered noobs;\n    title1 \"有害事象集計表（治療群別）\";\n    title3 \"被験者数 (%)\";\nrun;\n\n/*---------------------------------------------------------------------------------------*/\n/* Step 6: 検証用出力                                                                   */\n/*---------------------------------------------------------------------------------------*/\n\n/* Step 6-1: 基本統計の確認 */\nproc sql;\n    title \"データ整合性チェック\";\n    select \"元AE総件数\" as check_point, \n           count(*) as count_value\n    from adae\n    \n    union all\n    \n    select \"安全性解析対象AE件数\" as check_point,\n           count(*) as count_value\n    from ae_safety_arm\n    \n    union all\n    \n    select \"Treatment群被験者数\" as check_point,\n           &treatment_n as count_value from (select 1 as dummy)\n           \n    union all\n    \n    select \"Control群被験者数\" as check_point,\n           &control_n as count_value from (select 1 as dummy)\n           \n    union all\n    \n    select \"Total被験者数\" as check_point,\n           &total_n as count_value from (select 1 as dummy);\nquit;\n\n/* Step 6-2: SOC別詳細検証 */\nproc sql;\n   title \"SOC別被験者数検証\";\n   select aesoc,\n          count(distinct case when arm = \"Treatment\" then usubjid else null end) as treatment_subj,\n          count(distinct case when arm = \"Control\" then usubjid else null end) as control_subj,\n          count(distinct usubjid) as total_subj\n   from ae_safety_arm\n   group by aesoc\n   order by total_subj desc;\nquit;\n\n/* Step 6-3: 治療群配分の確認 */\nproc freq data=adsl;\n   tables arm / nocum;\n   title \"治療群配分\";\nrun;\n\n\n\n/*---------------------------------------------------------------------------------------*/\n/* プログラム終了                                                                       */\n/*---------------------------------------------------------------------------------------*/\n\n/* タイトルクリア */\ntitle;\n\n/* マクロ変数の確認（ログ出力） */\n%put NOTE: Treatment群被験者数 = &treatment_n;\n%put NOTE: Control群被験者数 = &control_n;\n%put NOTE: Total被験者数 = &total_n;\n\n%put NOTE: 治療群別有害事象集計プログラム実行完了;\n\n/*======================================================================================*/\n/* プログラム終了                                                                       */\n/* 出力データセット:                                                                    */\n/*   - final_display_ordered: 最終的な治療群別集計表                                   */\n/*   - ae_safety_arm: 安全性解析対象AEデータ                                           */\n/*   - ae_comprehensive: 中間集計データ                                                */\n/*======================================================================================*/"
  },
  {
    "objectID": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html#結果の検証とデバッグ",
    "href": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html#結果の検証とデバッグ",
    "title": "臨床試験における有害事象データの集計：PROC SQL",
    "section": "",
    "text": "/* 集計結果の検証 */\nproc sql;\n    /* 元データとの整合性チェック */\n    select \"元AE総件数\" as check_point, \n           count(*) as count_value\n    from adae\n    \n    union all\n    \n    select \"安全性解析対象AE件数\" as check_point,\n           count(*) as count_value\n    from ae_safety_arm\n    \n    union all\n    \n    select \"Treatment群被験者数\" as check_point,\n           &treatment_n as count_value\n           \n    union all\n    \n    select \"Control群被験者数\" as check_point,\n           &control_n as count_value\n           \n    union all\n    \n    select \"Total被験者数\" as check_point,\n           &total_n as count_value;\nquit;\n検証SQLの解説：\n\nunion all - 複数のSELECT結果を縦に結合（重複も含める）\n&treatment_n - 事前に計算したマクロ変数の値を表示\n検証の重要性: 各ステップで期待する件数が得られているか確認\n\n/* SOC別被験者数の詳細検証 */\nproc sql;\n    select aesoc,\n           count(distinct case when arm = \"Treatment\" then usubjid end) as treatment_subj,\n           count(distinct case when arm = \"Control\" then usubjid end) as control_subj,\n           count(distinct usubjid) as total_subj\n    from ae_safety_arm\n    where aerel = \"Y\"  /* 因果関係ありのみ */\n    group by aesoc\n    order by total_subj desc;\nquit;\nCASE文の高度な使用：\ncount(distinct case when arm = \"Treatment\" then usubjid end)\n\ncase when ... then ... end - 条件を満たす場合のみ値を返す\nTreatment群の場合のみusubjidをカウント対象にする\n1つのクエリで治療群別の集計が可能"
  },
  {
    "objectID": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html#初心者向けsql学習のポイント",
    "href": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html#初心者向けsql学習のポイント",
    "title": "臨床試験における有害事象データの集計：PROC SQL",
    "section": "",
    "text": "SELECT 何を選ぶか\nFROM どのテーブルから  \nWHERE どんな条件で\nGROUP BY どうグループ化するか\nORDER BY どう並び替えるか\n\n\n\n/* 内部結合の例 */\nfrom adae a                    -- メインテーブル\ninner join adsl s              -- 結合するテーブル  \non a.usubjid = s.usubjid       -- 結合条件\nand s.saffl = \"Y\"              -- 追加フィルタ\n\n\n\n\ncount(*) - 全行数\ncount(distinct 列名) - ユニークな値の数\nsum() - 合計\nmin(), max() - 最小値、最大値\n\n\n\n\nfrom (select ... from ... group by ...) as 別名\n\n内側のクエリから読む\n外側のクエリは内側の結果を使用"
  },
  {
    "objectID": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html#まとめ",
    "href": "posts/statistics/2025/Proc_SQL_SOC_PT別の集計.html#まとめ",
    "title": "臨床試験における有害事象データの集計：PROC SQL",
    "section": "",
    "text": "本記事では、臨床試験における有害事象データの治療群別集計を、PROC SQLを用いて段階的に実装しました。初心者の方にとって重要なポイントは：\n\n\n\n小さく始める: 複雑なクエリは段階的に構築\n中間結果確認: 各ステップでPROC PRINTで結果確認\nエラー対処: エラーメッセージから問題箇所を特定\nコメント活用: 処理の目的を明記\n\nこの段階的アプローチにより、初心者でも確実に治療群別有害事象集計をマスターできます。重要なのは、各ステップの目的を理解しながら進めることです。"
  },
  {
    "objectID": "posts/statistics/2025/RWD研究におけるADS仕様書.html",
    "href": "posts/statistics/2025/RWD研究におけるADS仕様書.html",
    "title": "RWD研究における解析用データセット仕様書",
    "section": "",
    "text": "臨床試験では厳格な統計解析計画書（SAP）とそれに紐づくADS仕様書が必須ですが、観察研究やリアルワールドデータ（RWD）を用いた研究では、その柔軟性ゆえに解析データセットの管理が曖昧になりがちです。しかし、これは結果の再現性や解析効率の低下、さらには研究の信頼性に関わるリスクを孕んでいます。\n本記事では、生物統計家の視点から、観察研究・RWDにおいてもなぜ解析データセット（ADS）仕様書の作成が重要なのかを解説し、その効果的な運用戦略を提案します。\n\n\n「探索的」な要素が強い観察研究やRWD解析において、ADS仕様書は一見すると手間のように思えるかもしれません。しかし、以下の点でその作成は不可欠です。\n\n\nADS仕様書は、解析に使用する変数の定義、欠損値の処理方法、変数変換のロジックなどを明確に文書化します。これにより、誰がいつ解析を行っても同じデータセットが生成されることが保証され、結果の再現性が担保されます。これは、研究の科学的信頼性を高める上で非常に重要です。\n\n\n\n解析の基盤となるデータセットの構造が明確であれば、プログラマーは無駄なく効率的に解析プログラムを開発できます。途中で変数の追加や定義変更が発生した場合でも、ADS仕様書があれば影響範囲を素早く特定し、手戻りを最小限に抑えることが可能です。\n\n\n\n医師や研究者との間で「どのデータがどのように使われているか」という共通認識を持つことは、スムーズな研究推進に不可欠です。「このカットオフを変えてみたら？」といった要望に対しても、ADS仕様書を基に議論することで、影響や実現可能性を具体的に検討できます。\n\n\n\n研究成果の発表や薬事承認プロセスにおいては、解析の透明性とトレーサビリティが求められます。ADS仕様書は、データ処理のプロセスを客観的に示す証拠となり、将来的な監査や検証に耐えうる研究基盤を構築します。\n\n\n\n\n臨床試験のSAPのような厳密さではなく、観察研究の特性に合わせた柔軟なアプローチが求められます。\n\n\n研究の初期段階で完璧なADS仕様書を作成するのは困難です。まずは現時点での仮説に基づき、最低限の情報を盛り込んだドラフトを作成しましょう。そして、研究の進捗や医師からのフィードバックに応じて、継続的に改訂していきます。\nポイント: すべての改訂には改訂履歴（バージョン、日付、内容、担当者）を明確に記録し、変更の経緯を追跡できるようにすることが重要です。\n\n\n\n「この変数を追加してみたら？」といった要望があった際、すぐにデータセットやプログラムを変更するのではなく、ADS仕様書の変更プロセスを設けましょう。変更の必要性、影響範囲、そして最終的な合意形成（誰が承認したか）を文書化することで、無秩序な変更を防ぎ、管理を強化できます。\n\n\n\n以下のような項目を網羅することで、実用的なADS仕様書を作成できます。\n\nデータソース: 元となる生データの出所\n変数定義:\n\n元の変数名とADS上の変数名\nデータ型（例: 数値、文字列、日付）\n単位\n欠損値の扱い: どのようなルールで処理するか（例: 欠損として扱う、特定値で補完）\nカテゴリカル変数の場合は、コードとラベルの対応\n新規作成変数の定義: 計算式やロジック（例: BMI = 体重(kg) / 身長(m)2、特定のカットオフ値の定義）\n\nデータ結合・マージの定義: 複数データソースを結合する場合のキーや結合方法\n除外基準: ADS作成段階で除外する対象（例: 特定の疾患群、データ不備の症例）\n\n\n\n\n可能であれば、SAS、R、Pythonなどの統計プログラミング言語を用いて、ADSの生成プロセスを自動化しましょう。これにより、手動でのデータ操作によるヒューマンエラーを防ぎ、ADS仕様書の変更にも柔軟かつ迅速に対応できるようになります。プログラム自体もバージョン管理することで、より高い再現性を実現できます。\n\n\n\n\n初期段階での合意形成: 研究開始時に、解析の主要な目的とそれに必要な変数について医師と合意し、ADS仕様書の初稿を共有します。\n変更依頼への対応: 変更依頼があった際には、単に実行するだけでなく、ADS仕様書への反映とその影響（解析結果への影響、多重比較の問題など）を説明し、変更プロセスに乗せることを促します。これにより、科学的妥当性を保ちながら柔軟に対応できます。\n\n\n\n\n\n観察研究やRWDを用いた解析において、ADS仕様書は単なる文書作成の手間ではありません。それは、研究の質と信頼性を高め、解析効率を最大化するための強力なツールです。\n臨床試験ほど厳密である必要はありませんが、段階的な作成と改訂、明確な変更管理プロセス、そしてプログラミングによる自動化を組み合わせることで、観察研究の柔軟性を保ちつつ、再現性と信頼性の高い統計解析業務を実現できます。生物統計家として、これらの戦略を積極的に導入し、より質の高い研究成果に貢献していきましょう。"
  },
  {
    "objectID": "posts/statistics/2025/RWD研究におけるADS仕様書.html#なぜ観察研究rwdにads仕様書が必要なのか",
    "href": "posts/statistics/2025/RWD研究におけるADS仕様書.html#なぜ観察研究rwdにads仕様書が必要なのか",
    "title": "RWD研究における解析用データセット仕様書",
    "section": "",
    "text": "「探索的」な要素が強い観察研究やRWD解析において、ADS仕様書は一見すると手間のように思えるかもしれません。しかし、以下の点でその作成は不可欠です。\n\n\nADS仕様書は、解析に使用する変数の定義、欠損値の処理方法、変数変換のロジックなどを明確に文書化します。これにより、誰がいつ解析を行っても同じデータセットが生成されることが保証され、結果の再現性が担保されます。これは、研究の科学的信頼性を高める上で非常に重要です。\n\n\n\n解析の基盤となるデータセットの構造が明確であれば、プログラマーは無駄なく効率的に解析プログラムを開発できます。途中で変数の追加や定義変更が発生した場合でも、ADS仕様書があれば影響範囲を素早く特定し、手戻りを最小限に抑えることが可能です。\n\n\n\n医師や研究者との間で「どのデータがどのように使われているか」という共通認識を持つことは、スムーズな研究推進に不可欠です。「このカットオフを変えてみたら？」といった要望に対しても、ADS仕様書を基に議論することで、影響や実現可能性を具体的に検討できます。\n\n\n\n研究成果の発表や薬事承認プロセスにおいては、解析の透明性とトレーサビリティが求められます。ADS仕様書は、データ処理のプロセスを客観的に示す証拠となり、将来的な監査や検証に耐えうる研究基盤を構築します。"
  },
  {
    "objectID": "posts/statistics/2025/RWD研究におけるADS仕様書.html#観察研究rwdにおけるads仕様書の運用戦略",
    "href": "posts/statistics/2025/RWD研究におけるADS仕様書.html#観察研究rwdにおけるads仕様書の運用戦略",
    "title": "RWD研究における解析用データセット仕様書",
    "section": "",
    "text": "臨床試験のSAPのような厳密さではなく、観察研究の特性に合わせた柔軟なアプローチが求められます。\n\n\n研究の初期段階で完璧なADS仕様書を作成するのは困難です。まずは現時点での仮説に基づき、最低限の情報を盛り込んだドラフトを作成しましょう。そして、研究の進捗や医師からのフィードバックに応じて、継続的に改訂していきます。\nポイント: すべての改訂には改訂履歴（バージョン、日付、内容、担当者）を明確に記録し、変更の経緯を追跡できるようにすることが重要です。\n\n\n\n「この変数を追加してみたら？」といった要望があった際、すぐにデータセットやプログラムを変更するのではなく、ADS仕様書の変更プロセスを設けましょう。変更の必要性、影響範囲、そして最終的な合意形成（誰が承認したか）を文書化することで、無秩序な変更を防ぎ、管理を強化できます。\n\n\n\n以下のような項目を網羅することで、実用的なADS仕様書を作成できます。\n\nデータソース: 元となる生データの出所\n変数定義:\n\n元の変数名とADS上の変数名\nデータ型（例: 数値、文字列、日付）\n単位\n欠損値の扱い: どのようなルールで処理するか（例: 欠損として扱う、特定値で補完）\nカテゴリカル変数の場合は、コードとラベルの対応\n新規作成変数の定義: 計算式やロジック（例: BMI = 体重(kg) / 身長(m)2、特定のカットオフ値の定義）\n\nデータ結合・マージの定義: 複数データソースを結合する場合のキーや結合方法\n除外基準: ADS作成段階で除外する対象（例: 特定の疾患群、データ不備の症例）\n\n\n\n\n可能であれば、SAS、R、Pythonなどの統計プログラミング言語を用いて、ADSの生成プロセスを自動化しましょう。これにより、手動でのデータ操作によるヒューマンエラーを防ぎ、ADS仕様書の変更にも柔軟かつ迅速に対応できるようになります。プログラム自体もバージョン管理することで、より高い再現性を実現できます。\n\n\n\n\n初期段階での合意形成: 研究開始時に、解析の主要な目的とそれに必要な変数について医師と合意し、ADS仕様書の初稿を共有します。\n変更依頼への対応: 変更依頼があった際には、単に実行するだけでなく、ADS仕様書への反映とその影響（解析結果への影響、多重比較の問題など）を説明し、変更プロセスに乗せることを促します。これにより、科学的妥当性を保ちながら柔軟に対応できます。"
  },
  {
    "objectID": "posts/statistics/2025/RWD研究におけるADS仕様書.html#まとめ",
    "href": "posts/statistics/2025/RWD研究におけるADS仕様書.html#まとめ",
    "title": "RWD研究における解析用データセット仕様書",
    "section": "",
    "text": "観察研究やRWDを用いた解析において、ADS仕様書は単なる文書作成の手間ではありません。それは、研究の質と信頼性を高め、解析効率を最大化するための強力なツールです。\n臨床試験ほど厳密である必要はありませんが、段階的な作成と改訂、明確な変更管理プロセス、そしてプログラミングによる自動化を組み合わせることで、観察研究の柔軟性を保ちつつ、再現性と信頼性の高い統計解析業務を実現できます。生物統計家として、これらの戦略を積極的に導入し、より質の高い研究成果に貢献していきましょう。"
  },
  {
    "objectID": "posts/statistics/2025/SAS_PROC_SGPLOT.html",
    "href": "posts/statistics/2025/SAS_PROC_SGPLOT.html",
    "title": "SASのProc SGPLOTに関するTips",
    "section": "",
    "text": "参考文献\n\n2023年SAS User総会：太田さん資料：\n\n小さく始めるSGPLOT／SGPANEL ～データに語らせよう～\n\nSAS One DashのSGplotブログ\n武田薬品：舟尾先生、SAS Sgplot超入門\n\nTips\n\nvlineにおいて最終時点のみ線で結ばないⅡ\nSGPlotのカプランマイヤー図にログランク検定のp値を書き入れる\nSGPlot内に記述統計量を書き込む方法"
  },
  {
    "objectID": "posts/statistics/2025/SAS_PROC_SGPLOT.html#sgplotに関する基礎的事項と発展的な内容をまとめる",
    "href": "posts/statistics/2025/SAS_PROC_SGPLOT.html#sgplotに関する基礎的事項と発展的な内容をまとめる",
    "title": "SASのProc SGPLOTに関するTips",
    "section": "",
    "text": "参考文献\n\n2023年SAS User総会：太田さん資料：\n\n小さく始めるSGPLOT／SGPANEL ～データに語らせよう～\n\nSAS One DashのSGplotブログ\n武田薬品：舟尾先生、SAS Sgplot超入門\n\nTips\n\nvlineにおいて最終時点のみ線で結ばないⅡ\nSGPlotのカプランマイヤー図にログランク検定のp値を書き入れる\nSGPlot内に記述統計量を書き込む方法"
  },
  {
    "objectID": "posts/statistics/2025/SAS_PROC_SGPLOT.html#introduction",
    "href": "posts/statistics/2025/SAS_PROC_SGPLOT.html#introduction",
    "title": "SASのProc SGPLOTに関するTips",
    "section": "2 Introduction",
    "text": "2 Introduction\nSGPLOTプロシジャとは、ODS Graphics機能で使用できるStatistical Graphics Proceduresに分類されるプロシジャ。解析用データセットや他Procedureにて計算した統計量データを用いて、様々なグラフを生成することができる。\nODS Graphics機能\n代表的な作成できるグラフ\n\n散布図\n折れ線グラフ"
  },
  {
    "objectID": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html",
    "href": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html",
    "title": "SASによる解析業務開始時のフォルダ整理・作成",
    "section": "",
    "text": "本記事では、実務上便利なSASプログラミングのTipsを紹介する。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説",
    "href": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説",
    "title": "SASによる解析業務開始時のフォルダ整理・作成",
    "section": "2.1 プログラム解説",
    "text": "2.1 プログラム解説\nこのSASプログラムは現在の日時を取得し、異なる形式でマクロ変数に格納するコードです。\n処理の流れ：\n\n%sysfunc(datetime())で現在日時を数値形式で取得\ndate()とtime()で日付・時刻を個別に取得\nput()関数でフォーマット適用（日付：YYYY/MM/DD、時刻：HH:MM:SS）\ncompress()で区切り文字を除去（日付：YYYYMMDD、時刻：HHMMSS）\ncall symputx()で4つのマクロ変数を作成\n\n作成されるマクロ変数：\n\n&StDates：2025/06/16（スラッシュ付き日付）\n&StDate：20250616（スラッシュなし日付）\n&StTimes：14:30:25（コロン付き時刻）\n&StTime：143025（コロンなし時刻）\n\n用途： ログファイル名生成、バックアップのタイムスタンプ、処理開始時刻の記録など、バッチ処理でよく使用される汎用的なコードです。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説-1",
    "href": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説-1",
    "title": "SASによる解析業務開始時のフォルダ整理・作成",
    "section": "3.1 プログラム解説",
    "text": "3.1 プログラム解説\nこのSASプログラムは、実行中のプログラムの場所を自動判定し、プロジェクトの標準フォルダ構造に基づいて各種パスを動的に設定する汎用的なパス管理コードです。\n\n3.1.1 実行パス取得マクロ\n最初の部分では、現在実行中のSASプログラムの完全パスを取得するマクロを定義しています。このマクロは実行環境に関係なく動作するよう設計されており、バッチ実行時はGETOPTION(SYSIN)関数を、対話的実行時はSAS_EXECFILEPATH環境変数を使用します。IF文による条件分岐により、どちらの環境でも確実にプログラムパスを取得できる仕組みになっています。\n\n\n3.1.2 階層パス解析\n次に、取得したフルパスから階層構造を解析し、プロジェクト内での相対位置を把握する処理を行います。SCAN関数とQSUBSTR関数を組み合わせて、パスを階層別に分解します。PROGRAM_NAMEには実行中のプログラム名（拡張子なし）、CURRENT_DIRには現在のディレクトリの完全パス、PARENT_DIRには1つ上の階層ディレクトリのパス、PROJECT_ROOTにはプロジェクトルートディレクトリのパス（2つ上の階層）がそれぞれ格納されます。\n\n各変数の役割：\n\nPROGRAM_NAME：実行中のプログラム名（拡張子なし）\nCURRENT_DIR：現在のディレクトリの完全パス\nPARENT_DIR：1つ上の階層ディレクトリのパス\nPROJECT_ROOT：プロジェクトルートディレクトリのパス（2つ上の階層）\n\n\n\n\n3.1.3 標準パス自動生成\n最後のデータステップでは、プロジェクト標準フォルダ構造に基づいて必要なパスを自動生成します。CAT関数でPROJECT_ROOTを基準として各フォルダパスを結合し、CALL SYMPUTX文でマクロ変数として定義します。生データ格納用のINPUT_RAW、外部データ格納用のINPUT_EXT、出力ファイル格納用のOUTPUT_PATH、ログファイル格納用のLOG_PATH、マクロファイル格納用のMACRO_PATH、設定ファイル格納用のSETTING_PATH、仕様書格納用のSPEC_PATHが設定されます。\n設定されるパス：\n\nINPUT_RAW：生データ（Raw data）格納パス\nINPUT_EXT：外部データ（External data）格納パス\nOUTPUT_PATH：出力ファイル格納パス\nLOG_PATH：ログファイル格納パス\nMACRO_PATH：マクロファイル格納パス\nSETTING_PATH：設定ファイル格納パス\nSPEC_PATH：仕様書格納パス\n\n\n\n3.1.4 活用メリット\nこのコードをプロジェクトの各SASプログラム冒頭に配置することで、プロジェクトフォルダの移動や環境変更時にパス設定の修正が不要になります。チーム開発での設定統一と保守性向上を実現でき、どのサブフォルダからプログラムを実行しても、常に正しいプロジェクトルートを基準とした一貫したパス管理が可能になります。手動でのパス設定ミスを防ぎ、開発効率の向上にも寄与します。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説-2",
    "href": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説-2",
    "title": "SASによる解析業務開始時のフォルダ整理・作成",
    "section": "4.1 プログラム解説",
    "text": "4.1 プログラム解説\nこのSASマクロは、指定されたディレクトリパスが存在しない場合に、必要な階層構造を含めて自動的にフォルダを作成する汎用的なディレクトリ作成マクロです。\n\n4.1.1 マクロの動作原理\nマクロは再帰的なアルゴリズムを採用しており、深い階層のフォルダ構造でも一度の呼び出しで全ての必要なディレクトリを作成できます。まず入力されたパスを親ディレクトリ部分と最終フォルダ名に分解し、指定されたパスが存在するかをチェックします。存在しない場合、親ディレクトリの存在も確認し、親ディレクトリが存在しなければマクロが自分自身を呼び出して上位階層から順次作成していきます。\n\n\n4.1.2 パス解析のロジック\nSTRIP関数で入力パスの前後空白を除去した後、SUBSTR関数とSCAN関数を組み合わせてパスを分解します。SCAN関数でパス区切り文字（バックスラッシュ）を基準に最終フォルダ名を抽出し、SUBSTR関数で親ディレクトリ部分を切り出します。この処理により、どのような深さのパスでも正確に階層構造を解析できます。\n\n\n4.1.3 条件分岐による効率的な処理\nFILEEXIST関数による存在チェックを各段階で実行し、既に存在するディレクトリに対しては何も処理を行いません。これにより無駄な処理を避け、既存の構造を保護しながら必要な部分のみを作成します。実際のフォルダ作成はDCREATE関数で実行され、作成結果は戻り値で確認できます。\n\n\n4.1.4 プロジェクト管理での活用\nこのマクロを前回のパス設定コードと組み合わせることで、プロジェクト開始時のフォルダ構造セットアップを完全自動化できます。新しい環境でプロジェクトを開始する際や、チームメンバーが初めてプロジェクトに参加する際に、手動でフォルダを作成する手間を省き、標準的なフォルダ構造を確実に構築できます。\n\n\n4.1.5 エラー処理と保守性\nマクロはエラーハンドリングも考慮されており、作成に失敗した場合でも処理が停止することなく、次の処理に進みます。また、既存のフォルダ構造に影響を与えることなく、必要な部分のみを安全に追加できる設計になっています。プロジェクトの成長に合わせて新しいフォルダが必要になった場合も、このマクロを呼び出すだけで簡単に対応できます"
  },
  {
    "objectID": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説-3",
    "href": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説-3",
    "title": "SASによる解析業務開始時のフォルダ整理・作成",
    "section": "5.1 プログラム解説",
    "text": "5.1 プログラム解説\nこのSASプログラムは、実行日ベースのプログラム管理フォルダを自動作成する汎用的なコードです。\n\n5.1.1 基本的な仕組み\nまず現在の日付をYYYYMMDD形式で取得し、実行中のプログラムパスからプロジェクトルートを自動判定します。その後、プロジェクトルート配下のPrgフォルダ内に実行日付のサブフォルダ（例：Prg\\20250616）を作成します。\n\n\n5.1.2 日付ベースフォルダ管理の利点\nこのシステムにより、プログラムの実行履歴を日付別に整理できます。同じプログラムを異なる日に実行しても結果が混在せず、過去の実行内容を簡単に追跡できます。特に開発段階では、日々の変更内容を時系列で管理できるため、問題発生時の原因特定や以前のバージョンへの戻しが容易になります。\n\n\n5.1.3 自動ディレクトリ作成の活用\ncreate_dir_structureマクロの再帰処理により、深い階層構造でも一度の呼び出しで必要なフォルダが全て作成されます。既存フォルダの存在チェック機能により、重複実行しても安全で、チーム開発での環境差異も自動的に解決されます。\nこのコードをプログラム冒頭に配置することで、実行のたびに適切な作業フォルダが準備され、プロジェクトの標準化と履歴管理を同時に実現できます。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説-4",
    "href": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説-4",
    "title": "SASによる解析業務開始時のフォルダ整理・作成",
    "section": "6.1 プログラム解説",
    "text": "6.1 プログラム解説\nこのコードは、FILENAME文とINCLUDE文を組み合わせて、特定フォルダ内の複数のSASファイルを効率的に読み込む汎用的な手法です。\n\n6.1.1 FILENAME文による論理参照の設定\nFILENAME文で論理名「MACROLIB」を定義し、マクロ変数で指定されたフォルダパスを割り当てます。これにより、以降の処理では物理的なフォルダパスではなく、論理名を使用してファイルにアクセスできるようになります。\n\n\n6.1.2 INCLUDE文による選択的ファイル読み込み\n各INCLUDE文では、論理名に続けて括弧内にファイル名を指定することで、指定フォルダ内の特定ファイルを読み込みます。この記法により、フォルダ内の全ファイルではなく、必要なファイルのみを選択的に読み込むことが可能です。\n\n\n6.1.3 この手法の優位性\n従来の絶対パス指定と比較して、コードの保守性と可読性が大幅に向上します。フォルダパスの変更時は最初のFILENAME文のみを修正すれば良く、同一フォルダ内の複数ファイルを扱う際の記述量も削減されます。また、論理名を使用することで、プラットフォーム間でのパス記法の違いも吸収できます。\n\n\n6.1.4 応用範囲\nこの手法は、マクロライブラリの管理以外にも、設定ファイルの読み込み、データセットの一括処理、プログラムの分割実行など、様々な場面で活用できます。プロジェクトの規模が大きくなり、複数のファイルを体系的に管理する必要がある場合に特に有効な手法です。"
  },
  {
    "objectID": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説-5",
    "href": "posts/statistics/2025/SASによる解析業務開始時のフォルダ整理・作成.html#プログラム解説-5",
    "title": "SASによる解析業務開始時のフォルダ整理・作成",
    "section": "7.1 プログラム解説",
    "text": "7.1 プログラム解説\nこのSASプログラムは、メタデータを基にして解析プログラムのテンプレートを動的に生成する自動化システムです。\n\n7.1.1 プログラム生成マクロの構造\ncreate_pgマクロは、プログラム名、テーブル名、解析対象集団の3つのパラメータを受け取り、指定されたフォルダに新しいSASプログラムファイルを作成します。FILENAME文で出力先ファイルを指定し、FILE文とPUT文を使用してプログラムのヘッダー部分を標準化されたフォーマットで出力します。\n\n\n7.1.2 標準化されたヘッダー生成\n各生成プログラムには、プロジェクト情報、プログラム説明、解析対象集団、バージョン情報、履歴管理欄を含む統一フォーマットのヘッダーが自動挿入されます。これにより、手動作成時に発生しがちな記載漏れや形式の不統一を防ぎ、プロジェクト全体でのドキュメント品質を保証します。\n\n\n7.1.3 メタデータ駆動型の一括生成\n最後のデータステップでは、OUT2データセットに格納されたメタデータを読み込み、CALL EXECUTE文を使用してマクロを動的に実行します。CATS関数でマクロ呼び出し文を構築し、データセットの各レコードに対して個別のプログラムファイルを生成します。\n\n\n7.1.4 自動化の利点とメリット\nこの手法により、数十から数百の解析プログラムを一度に生成できるため、大規模プロジェクトでの開発効率が大幅に向上します。メタデータの変更時も該当部分のみを修正して再実行すれば、全プログラムに変更が反映されるため、保守性も高くなります。また、ヒューマンエラーの削減と品質の均一化も実現できます。"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "",
    "text": "本記事では、以下の2つの文献をまとめる。\n参考文献"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#はじめに",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#はじめに",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.1 はじめに",
    "text": "1.1 はじめに\n「保守しやすいコードは少ない人員で管理できる。つまり、あなたの雇用が危険にさらされる」\nこの皮肉な視点から、意図的に読みにくく、保守困難なSASコードを書く「技法」を紹介します。もちろん、これは反面教師として学ぶべき内容です。\n\n“Don’t be irreplaceable, if you can’t be replaced, you can’t be promoted.” - Dilbert’s Laws of Work"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#プログラミングスタイルやってはいけないこと",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#プログラミングスタイルやってはいけないこと",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.2 🚫 プログラミングスタイル（やってはいけないこと）",
    "text": "1.2 🚫 プログラミングスタイル（やってはいけないこと）\n\n1.2.1 論理的分離の回避\n/* 悪い例：本来分けるべき処理を無理やり1つにまとめる */\ndata result;\n   set patients;\n   if age &lt; 18 then group='pediatric'; else group='adult';\n   weight_kg = weight_lb / 2.2;\n   bmi = weight_kg / (height_m * height_m);\n   if bmi &gt; 30 then obese_flag = 1; else obese_flag = 0;\n   /* 複数の異なる処理が混在 */\nrun;\n解説: このコードは年齢分類、重量変換、BMI計算、肥満判定という4つの異なる処理を1つのデータステップに混在させています。本来なら機能ごとに分割すべきですが、すべてを混ぜることで何をしているのか分かりにくくしています。\n\n\n1.2.2 過度なネスト化（3層以上で複雑さ倍増）\n/* 悪い例：無意味に深いネスト */\ndate = mdy(month(date), day(date), year(date));\ndepth2 = input(substr(station, index(station,'-')+1), 3.);\nname = substr(name, index(name,',')+1, length(name));\n解説:\n\n1行目：既にSAS日付値の変数を、わざわざ分解して再構築する無意味な処理\n2行目：文字列から数値抽出を3つの関数で複雑にネスト\n3行目：名前の後半部分を取得する処理を複雑化 これらは全て、より単純な方法で書けるものを意図的に複雑にしています。\n\n\n\n1.2.3 関数の不適切な使用\n/* 悪い例：関数で置き換え可能な処理を冗長に記述 */\nif a &lt; 0 then b = a*-1;\nelse b = a;\n/* ABS(a)で済む処理 */\n\n/* 悪い例：非標準的な書き方 */\nab = (a*(a&gt;0) + b*(b&gt;0))/((a&gt;0)+(b&gt;0));\n/* 2つの正数の平均を求める処理 */\n解説:\n\n1つ目：ABS(a)関数で済む絶対値計算を、わざわざIF文で書いている\n2つ目：2つの正数の平均を求めるのに、論理値を数値として使う複雑な式を使用。(a+b)/2で済むところを意図的に分かりにくくしている\n\n\n\n1.2.4 マクロの悪用\n/* 悪い例：ローカル・グローバル変数の混乱 */\n%macro inside(aa);\n    %put inside &aa;\n%mend inside;\n\n%macro outside;\n    %let aa = 5;\n    %inside(3)\n    %put outside &aa;\n%mend outside;\n\n%outside\n/* 出力: inside 3, outside 5 */\n解説: マクロパラメータとマクロ変数の名前を同じにして混乱を誘発。%inside(3)では引数として3が渡されるため「inside 3」と出力されますが、%outsideマクロ内の&aaは依然として5のままです。これによりマクロ変数のスコープについて混乱を招きます。\n\n\n1.2.5 不必要な複雑化\n/* 悪い例：PROC SQLを無理やり複数ステップに分解 */\nproc sort data=sales;\n    by region;\nproc summary data=sales nway;\n    by region;\n    var saleprce;\n    output out=stats mean=meansale;\ndata report;\n    merge stats sales;\n    by region;\n    if saleprce gt meansale;\n\n/* 本来は以下の1ステップで済む */\nproc sql;\n    create table report as\n    select * from sales\n    having saleprce gt mean(saleprce)\n    group by region;\n解説: 1つのSQL文で書ける処理を、わざわざSORT→SUMMARY→DATA stepの3段階に分けています。これにより一時データセット（stats）が作成され、処理が複雑になり、エラーの可能性も増加します。"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#システムオプションの悪用",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#システムオプションの悪用",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.3 🎭 システムオプションの悪用",
    "text": "1.3 🎭 システムオプションの悪用\n\n1.3.1 デバッグ機能の無効化\n/* 悪い例：重要な情報を隠す */\noptions NOSOURCE NOSOURCE2 NONOTES NOLABEL NOMACRO;\n解説:\n\nNOSOURCE: 実行されたSASコードをログに表示しない\nNOSOURCE2: %INCLUDEで読み込まれたコードも表示しない\nNONOTES: 通常のNOTEメッセージを非表示\nNOLABEL: 変数ラベルを無効化\nNOMACRO: マクロ機能自体を無効化 これらによりデバッグが困難になります。\n\n\n\n1.3.2 観測数制御の悪用\n/* 悪い例：事前告知なしでのデータ制限 */\noptions obs=100 firstobs=50;  /* 隠して設定 */\n解説: データセットの50-100番目の観測値のみを処理対象にしていますが、これを他の人に知らせていません。全データを処理していると思い込ませる悪質な手法です。\n\n\n1.3.3 ワークエリアの操作\n/* 悪い例：一時ファイルの場所を変更 */\noptions user=sasuser;\ndata new;  /* 実際はSASUSER.NEWに保存される */\n    set project.master;\n解説: USER=オプションにより、一時的なデータセットNEWがSASUSERライブラリに保存されます。セッション終了後も残存し、ディスク容量の問題や混乱を引き起こします。\n\n\n1.3.4 危険なオプション\n/* 悪い例：エラー時即座終了 */\noptions ERRORABEND;\n\n/* 悪い例：年の解釈を混乱させる */\noptions YEARCUTOFF=1800;\ndata a;\n    date = '23mar98'd;\n    year = year(date);  /* 1897になる */\n解説:\n\nERRORABEND: エラー発生時にSASセッションが即座に終了し、ログも確認できない\nYEARCUTOFF=1800: 2桁年表記の解釈基準を1800年に設定。’98’が1998年ではなく1898年と解釈される\n\n\n\n1.3.5 S=オプションによる列数制限\n/* 悪い例：最初の10桁のみ読み取り */\noptions s=10;\ndata new;\n    set olddata    /* OLDDATになる */\n        master\n        adj;\n    profit =\n        sales + tax;  /* TAXが使われない */\n    cnt+1;  /* PROFITの計算に含まれる */\n解説: S=10により、各行の最初の10文字のみが読み取られます。olddataはolddatに、sales + taxはsales +のみが読まれ、変数taxは無視されます。また、cnt+1;が前の行に繋がってしまいます。"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#編集スタイル可読性を破壊する方法",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#編集スタイル可読性を破壊する方法",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.4 📝 編集スタイル（可読性を破壊する方法）",
    "text": "1.4 📝 編集スタイル（可読性を破壊する方法）\n\n1.4.1 インデントの無視\n/* 悪い例：インデントなし */\ndata sasclass.biomass;\ninfile rawdat missover;\ninput @1 STATION $\n@12 DATE DATE7.\n@20 BMPOLY\n@25 BMCRUS\n@31 BMMOL\n@36 BMOTHR\n@41 BMTOTL ;\n解説: インデントがないため、どの行がどのステートメントに属するかが分かりにくくなっています。特にINPUTステートメントの変数リストが見づらく、修正時にミスを誘発しやすくなります。\n\n\n1.4.2 複数ステートメントの詰め込み\n/* 悪い例：1行に複数ステートメント */\ndata new;set old;if age&gt;65 then senior=1;else senior=0;weight_kg=weight_lb/2.2;output;\n解説: 5つのステートメントを1行に詰め込んでいます。どこで1つのステートメントが終わり、次が始まるのかが分からず、デバッグや修正が困難になります。\n\n\n1.4.3 行の途中での改行\n/* 悪い例：意味のない場所での改行 */\ndata sasclass.biomass;infile rawdat\nmissover;\ninput @1 STATION $ @12 DATE DATE7.\n@20 BMPOLY @25 BMCRUS @31 BMMOL @36\nBMOTHR @41 BMTOTL ;\n解説: 文法的に意味のない場所で改行しています。infile rawdatとmissoverオプションが分離され、inputステートメントも不自然に分割されています。\n\n\n1.4.4 画面外への重要コード配置\n/* 悪い例：80桁以降に重要な変数を配置 */\n                                                                              /*80桁*/\ndata newdata;\n    set olddata (drop=name                                                   fname\n                     address city state);  /* 重要な変数が見えない */\n解説: 80桁以降に重要な変数名を配置しています。多くのエディタでは80桁以降が表示されないか、画面外に隠れるため、重要な処理内容が見えなくなります。\n\n\n1.4.5 会社ロゴ形式のコード\n/* 悪い例：見た目重視のコード配置 */\n         data\n      sasclass.biomass;\n      infile cards missover;\n      input @1 STATION $\n      @12 DATE DATE7.\n      @20 BMPOLY\n   @25 BMCRUS @31 BMMOL\n  @36 BMOTHR @41 BMTOTL\n      ; format\n      date date7.\n      ;label BMCRUS=\n   'CRUSTACEAN BIOMASS'\n         BMMOL=\n   'MOLLUSC BIOMASS'\n      ;\n      run;\n解説: 機能性を完全に無視して、見た目の形（おそらく会社ロゴや図形）を優先してコードを配置しています。読みやすさが完全に犠牲になっています。"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#コメントの悪用",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#コメントの悪用",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.5 💬 コメントの悪用",
    "text": "1.5 💬 コメントの悪用\n\n1.5.1 実行可能コメント\n/* 悪い例：コメント内に実行文を隠す */\n* The comments in this section do more ;\n* than it seems ;\n* ;\n* modify data to prep for; proc means ;\n* after adjusting the data using ;\n* the; var for weight ;\n解説: 一見コメントに見えますが、実際にはproc means; var for weight;という実行可能なコードが隠されています。3行目の* ;でコメントが終了し、4-6行目が実際に実行されます。\n\n\n1.5.2 ネストできないコメントの悪用\n/* 悪い例：未完了コメントでコード全体を隠す */\n/* *****************\n* Apply the\n* ***very ***\n* important adjustment;\ndata yearly;\n    set yearly;\n    income = income*adjust;\nrun;\n/* Plot the adjusted income */\nproc gplot data=yearly......\n解説: 最初の/*コメントが閉じられていないため、その後のコード全体がコメント扱いになります。/* Plot the adjusted income */の*/で最初のコメントが閉じられ、それ以降のコードが実行されます。\n\n\n1.5.3 埋め込みコメントによる部分実行\n/* 悪い例：コメント内の一部のみ実行 */\n/* *****************\nREMOVE FOR PRODUCTION\nproc print data=big obs=25;\n    title1 'Test print of BIG';\n    var company dept mgr /*clerk*/;\ndata big;\n    set big;\n    if name='me' then salary=salary+5;\n*END OF REMOVED SECTION;\n****************** */\n解説: 全体がコメントで囲まれているように見えますが、/*clerk*/の部分で一時的にコメントが閉じ、その後のdataステップが実行されます。給与を不正に操作するコードが隠されて実行されてしまいます。"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#命名規則の悪用",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#命名規則の悪用",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.6 🔤 命名規則の悪用",
    "text": "1.6 🔤 命名規則の悪用\n\n1.6.1 混乱を招く命名\n/* 悪い例：意味不明な変数名 */\ndata analysis;\n    set patient_data;\n    /* デバッグ時は meaningful names を使い、後でCHANGEコマンドで変更 */\n    /* ===&gt; c 'age' 'qwrtxzqr' all */\n    QWRTXZQR = age;           /* 年齢なのに意味不明な名前 */\n    QWRTZXQR = weight;        /* 似たような名前で混乱 */\n    QWRZTXQR = height;        /* 微妙な違いで判別困難 */\n    HHHIIHIH = height;        /* HとIの区別が困難 */\n    WVWVWVVW = weight;        /* VとWの区別が困難 */\n    testnuml = test_result;   /* 1（数字）とl（小文字L）の混用 */\n    test0001 = test_id;       /* 0（ゼロ）とO（オー）の混用 */\n    QWRT2XQR = group;         /* ZとNの混用 */\n解説:\n\n母音を避けた無意味な文字列を使用\n似たような文字の組み合わせで視覚的混乱を誘発\n数字と文字の見た目が似ているものを混用（1とl、0とO、2とZ）\nコメントで「デバッグ時には意味のある名前を使い、後で置換する」という悪質な手法を示唆\n\n\n\n1.6.2 SASキーワードの変数名使用\n/* 悪い例：SASキーワードを変数名に使用 */\nDATA SET; \nSET DATA;\nDO = 5+ TO -15;  /* DO loopに見えるが変数代入 */\n解説: DATA、SET、DO、TOなどのSASキーワードを変数名として使用しています。DO = 5+ TO -15;は一見DO loopに見えますが、実際は変数DOに5 + TO - 15を代入するステートメントです。\n\n\n1.6.3 誤解を招く命名\n/* 悪い例：変数名と内容が一致しない */\ndata patients;\n    set raw_data;\n    SEX = fish_count;        /* SEXという名前だが魚の数 */\n    WEIGHT = height_cm;      /* WEIGHTという名前だが身長 */\n    INCHES = height_cm;      /* INCHESだがセンチメートル */\n    \n    /* 出力先も混乱させる */\n    IF SEX = 'MALES' THEN OUTPUT FEMALES;\n    \n    /* ラベルでさらに混乱 */\n    LABEL sex = 'Sex of the Patient';  /* 実際は魚の数 */\n解説:\n\n変数名と実際の内容が完全に異なる（SEXに魚の数、WEIGHTに身長など）\nOUTPUT文で条件と出力先が逆転（MALESの条件でFEMALESデータセットに出力）\nLABELでさらに混乱を助長（魚の数に「患者の性別」というラベル）\n\n\n\n1.6.4 一貫性のない命名\n/* 悪い例：YES/NOの値が一貫しない */\ndata flags;\n    set survey;\n    /* 通常: YES=0, NO=1 */\n    response1 = (answer='YES') * 0 + (answer='NO') * 1;\n    \n    /* どこかで例外: YES=1, NO=0 */\n    response2 = (answer='YES') * 1 + (answer='NO') * 0;\n    \n    /* さらに混乱: Y=NOの意味、N=YESの意味 */\n    if answer='YES' then code='N';\n    else if answer='NO' then code='Y';\n解説:\n\n同じプログラム内でYES/NOのコーディングが一貫していない\n通常の論理（YES=1, NO=0）とは逆の設定\n最後は完全に逆転（YESなのにN、NOなのにY）"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#データステップの境界をぼかす技法",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#データステップの境界をぼかす技法",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.7 🌀 データステップの境界をぼかす技法",
    "text": "1.7 🌀 データステップの境界をぼかす技法\n\n1.7.1 コメントによる境界隠し\n/* 悪い例：セミコロンなしコメントで次ステップを隠す */\ndata new; \n    set old;\n    x = 5 * y;\n    /* この行で次のステップが始まっている\n    data second; \n    set gudstuff;\n    x = zzstuff;\n/* 結果：NEWにはzsstuffの値が入る */\n解説: 4行目のコメントにセミコロンがないため、5-7行目がコメント扱いになりません。実際には1つのDATAステップで2つのSETステートメントが実行され、最後のx = zzstuff;の値が最終的に変数xに残ります。\n\n\n1.7.2 コロンによる巧妙な隠蔽\n/* 悪い例：セミコロンの代わりにコロンを使用 */\ndata new; \n    set old;\n    x = 5 * y;\n    /* この行で次のステップが始まっている:\n    data second; \n    set gudstuff;\n    x = zzstuff;\n解説: コメント行の最後がセミコロンではなくコロンになっています。SASではコロンはコメントの終了記号として認識されないため、次のdataステップが隠されて実行されます。\n\n\n1.7.3 不完全な引用符による隠蔽\n/* 悪い例：不完全な引用符で後続コードを隠す */\ndata new;\n    y = 5;\n    frankwt = 0;\n    x = 5 * y;\n    length name $6;\n    name = 'weight;\n    data second; set gudstuff;  /* この行が隠されている */\n    *for weight use Franks';\n    x = frankwt;\nproc print; run;\n/* 結果：xは常に0（25ではない） */\n解説:\n\n6行目で開始された文字列リテラル'weight;が閉じられていない\nそのため7-8行目が文字列の一部として扱われ、実行されない\n9行目の'で文字列が閉じられ、その後のコードが実行される\n結果としてx = frankwt;（0）が実行され、期待されたx = 5 * y;（25）は無効になる"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#さらに極端な技法上級者向け悪魔術",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#さらに極端な技法上級者向け悪魔術",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.8 🔥 さらに極端な技法（上級者向け悪魔術）",
    "text": "1.8 🔥 さらに極端な技法（上級者向け悪魔術）\n\n1.8.1 ソースコードの隠蔽・削除\n/* 悪い例：コンパイル後にソースを削除・改名 */\n/* SCL source code for SAS/AF applications */\n/* Compiled DATA steps */\n/* Compiled stored macros */\n/* DATA step views */\n/* SQL views（DESCRIBE optionで復元可能だが...） */\n\n/* エディタの文字色を背景色と同じにして隠す */\n解説:\n\nコンパイル済みのSAS/AFアプリケーション、ストアドマクロなどの元ソースコードを削除\nDATA step viewやSQL viewのソースも隠蔽\nエディタで文字色を背景色と同じにして視覚的に見えなくする これらにより、動作するプログラムがあっても、どのように動作しているかが分からなくなります。\n\n\n\n1.8.2 AUTOEXEC.SASとCONFIG.SASの悪用\n/* 悪い例：AUTOEXEC.SASでセッション終了 */\nENDSAS;\n/* または */\nABORT;\n/* または */\n%MACRO DUMMY;  /* %MENDまでのすべてのコードを無効化 */\n\n/* 悪い例：CONFIG.SASで隠れたERRORABEND設定 */\n-ERRORABEND\n\n/* 悪い例：AUTOEXEC.SASの最後に未完了コメント */\n/* 以降のすべてのコードがコメント扱いに */\n解説:\n\nENDSASやABORTをAUTOEXEC.SASに入れると、ユーザーのプログラムが実行される前にセッションが終了\n%MACRO DUMMY;を入れると、%MENDが現れるまでのすべてのコードがマクロ定義として扱われ実行されない\nCONFIG.SASでのERRORABEND設定は非常に見つけにくい\nAUTOEXEC.SASの最後に/*を入れると、以降のすべての投入コードがコメント扱いになる\n\n\n\n1.8.3 危険なデータステップ技法\n/* 悪い例：BYステートメント変数の途中変更 */\ndata merged;\n    merge data1 data2;\n    by id;\n    id = id + 1;  /* BYステートメント変数を変更 */\n解説: MERGEステートメントのBY変数idを、マージ処理の途中で変更しています。これにより予期しないマージ結果が生成され、データの整合性が失われます。\n\n\n1.8.4 POINT/NOBSオプションの落とし穴\n/* 悪い例：削除された観測値との相互作用 */\ndata a;\n    do i = 1 to 5;\n        output;\n    end;\nrun;\n/* FSEDITでi=2の観測値を削除 */\n\ndata b;\n    do point=1 to nobs;\n        set a point=point nobs=nobs;\n        output;\n    end;\n    stop;\nrun;\n/* 結果：削除された観測値が読まれ、iの値が不正になる */\n解説:\n\nデータセットAから観測値2（i=2）を削除\nPOINT/NOBSオプションで順次読み取ると、削除された位置でも読み取りが行われる\n削除された観測値を読み取ると、変数iには予期しない値（この場合は1）が入る\n結果として観測値2の位置でi=1が読まれ、データの整合性が失われる\n\n\n\n1.8.5 マクロクォート関数の悪用\n/* 悪い例：マクロ変数の解決を阻害 */\n%macro doit(city);\n    %put &city;\n    %let city=%nrstr(&city);  /* 文字列として固定 */\n    %put &city;\n    %if &city = LA %then\n        %put CITY is LOS ANGELES;\n    %else \n        %put city is not LA;  /* 常にこちらが実行される */\n%mend doit;\n\n%doit(LA)\n/* 出力: LA, &city, city is not LA */\n解説:\n\n%nrstr(&city)により、マクロ変数&cityが文字列リテラル「&city」として固定される\n以降の比較&city = LAでは、「&city」と「LA」が比較されるため、常にfalseになる\n引数として「LA」を渡しても、条件判定では常に「not LA」の結果になる\n\n\n\n1.8.6 特殊システムオプションの悪用\n/* 悪い例：エラーメッセージ抑制 */\noptions DKRICOND=NOWARN;  /* DROP/KEEP/RENAME文のエラーを隠す */\n\n/* 悪い例：データセット置き換え防止 */\noptions NOREPLACE;  /* 永続データセットの置き換えを阻止 */\n\n/* 悪い例：WORK領域のクリーンアップ抑制 */\noptions NOWORKINIT NOWORKTERM;  /* セッション終了後もファイル残存 */\n解説:\n\nDKRICOND=NOWARN: DROP/KEEP/RENAMEステートメントで存在しない変数を指定してもエラーが出ない\nNOREPLACE: 既存の永続データセットを上書きしようとするとエラーになる（一見良いが、予期しない場所で設定されると混乱）\nNOWORKINIT/NOWORKTERM: SASセッション終了後もWORKライブラリのファイルが残り、ディスク容量やセキュリティの問題を引き起こす"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#正しいプログラミング実践推奨事項",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#正しいプログラミング実践推奨事項",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.9 ✅ 正しいプログラミング実践（推奨事項）",
    "text": "1.9 ✅ 正しいプログラミング実践（推奨事項）\n\n1.9.1 良いコードの原則\n/* 良い例：読みやすく保守しやすいコード */\noptions MSGLEVEL=I SOURCE FMTERR DSNFERR NOREPLACE;\n\n/* Step 1: Demographics data preparation */\ndata adam.adsl;\n    set raw.demographics;\n    \n    /* Treatment group coding */\n    if trt01a = 'Placebo' then trt01pn = 1;\n    else if trt01a = 'Active 10mg' then trt01pn = 2;\n    else if trt01a = 'Active 20mg' then trt01pn = 3;\n    \n    /* Age group classification */\n    if age &lt; 65 then agegroup = 'Under 65';\n    else agegroup = '65 and Over';\n    \n    /* Safety population flag */\n    if cmstdt ne . then saffl = 'Y';\n    else saffl = 'N';\n    \n    /* Variable labels */\n    label trt01pn = 'Treatment Group (Numeric)'\n          agegroup = 'Age Group'\n          saffl = 'Safety Population Flag';\nrun;\n\n/* Step 2: Data validation */\nproc freq data=adam.adsl;\n    tables trt01pn*trt01a / missing;\n    title1 'Treatment Group Verification';\nrun;\n\nproc means data=adam.adsl n nmiss min max;\n    var age;\n    title1 'Age Distribution Check';\nrun;\n解説:\n\n明確なコメントで各ステップの目的を説明\n意味のある変数名とラベルを使用\n処理を論理的に分離（データ準備→検証）\n品質管理オプションを適切に設定\n一貫したインデントとフォーマット\n\n\n\n1.9.2 臨床試験での品質管理\n/* Step 3: Efficacy endpoint derivation */\ndata adam.adeff;\n    set adam.adsl(keep=usubjid trt01pn saffl);\n    \n    /* Merge with vital signs */\n    merge adam.adsl(in=demo)\n          raw.vitals(in=vital \n                     keep=usubjid visitnum aval param);\n    by usubjid;\n    \n    /* Only include subjects with baseline and post-baseline values */\n    if demo and vital;\n    \n    /* Derive change from baseline */\n    retain baseline;\n    if visitnum = 1 then baseline = aval;\n    else if visitnum &gt; 1 and baseline ne . then do;\n        chg = aval - baseline;\n        pchg = (chg / baseline) * 100;\n    end;\n    \n    /* Quality checks */\n    if baseline = . then put \"WARNING: Missing baseline for \" usubjid=;\n    if aval = . and visitnum &gt; 1 then put \"WARNING: Missing post-baseline value for \" usubjid= visitnum=;\n    \n    label chg = 'Change from Baseline'\n          pchg = 'Percent Change from Baseline'\n          baseline = 'Baseline Value';\nrun;\n解説:\n\nデータセットの結合条件を明確に指定（in=demo and vital）\nベースライン値の適切な保持と計算\n品質チェックを組み込み、問題があればログに出力\n計算ロジックを段階的に記述し、理解しやすくする\n派生変数には適切なラベルを付与"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#心理的トリックと認知バイアスの悪用",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#心理的トリックと認知バイアスの悪用",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.10 🧠 心理的トリックと認知バイアスの悪用",
    "text": "1.10 🧠 心理的トリックと認知バイアスの悪用\n\n1.10.1 先入観を利用した混乱\n/* 悪い例：一般的な変数名で全く違う内容を格納 */\ndata demographics;\n    set patient_roster;\n    \n    /* 通常なら患者情報だが... */\n    age = protocol_version;      /* 年齢ではなくプロトコル番号 */\n    sex = randomization_seed;    /* 性別ではなく乱数シード */\n    height = study_duration;     /* 身長ではなく研究期間 */\n    weight = site_number;        /* 体重ではなく施設番号 */\n    \n    /* ラベルで更なる混乱を誘発 */\n    label age = 'Patient Age (Years)'\n          sex = 'Patient Gender'\n          height = 'Height (cm)'\n          weight = 'Weight (kg)';\nrun;\n解説: プログラマーの先入観を悪用した極めて悪質な手法。変数名から期待される内容と実際の内容が完全に異なり、さらにラベルでも嘘の情報を提供しています。これにより重大な解析エラーを引き起こす可能性があります。\n\n\n1.10.2 視覚的類似性を利用した混乱\n/* 悪い例：見た目が似ている文字・数字の混用 */\ndata confusion;\n    /* 数字の1と小文字のl */\n    testnum1 = score_1;\n    testnuml = score_l;  /* 実際は小文字のL */\n    \n    /* 数字の0と大文字のO */\n    patient0 = id_zero;\n    patientO = id_oh;    /* 実際は大文字のO */\n    \n    /* 数字の2とアルファベットのZ */\n    group2 = treatment_2;\n    groupZ = treatment_z; /* 実際はZ */\n    \n    /* IとlとlIの組み合わせ */\n    IlllIlIl = result_a;  /* 何がIで何がlか判別不可能 */\n    lIlIlIll = result_b;\nrun;\n解説: フォントによっては区別が困難な文字を意図的に混用しています。特にプログラミング用でないフォントでは、これらの違いを見分けることは非常に困難になります。"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#プロシージャの悪用",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#プロシージャの悪用",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.11 🔄 プロシージャの悪用",
    "text": "1.11 🔄 プロシージャの悪用\n\n1.11.1 PROC SQLの危険な使用法\n/* 悪い例：副作用のあるSQL */\nproc sql;\n    /* 一見単純な選択に見えるが... */\n    create table summary as\n    select *,\n           (select count(*) from work.temp_calc \n            where temp_calc.id = main.id) as calc_count\n    from main_data as main;\n    \n    /* work.temp_calcは実際には存在せず、\n       このクエリ実行中に副作用で作成される隠しプロセスがある */\nquit;\n\n/* 隠された前処理（別の場所に配置） */\ndata work.temp_calc / view=work.temp_calc;\n    set main_data;\n    /* 複雑な計算でmain_dataを変更 */\n    call execute('data main_data; modify main_data; id = id + 1000; run;');\nrun;\n解説:\n\n一見単純なSELECTクエリに見えるが、サブクエリが隠された副作用を持つ\nVIEWを使って実行時に元データを変更する隠れた処理を組み込む\nCALL EXECUTEにより、クエリ実行中に予期しないデータ変更が発生\n\n\n\n1.11.2 PROC SORTの落とし穴\n/* 悪い例：NODUPKEY vs NODUPLICATESの混乱 */\n/* 同じファイル名で異なる結果を生成 */\nproc sort data=patients out=clean_data NODUPKEY;\n    by patient_id visit_date;\nrun;\n/* patient_idとvisit_dateの組み合わせで重複削除 */\n\n/* どこか別の場所で... */\nproc sort data=patients out=clean_data NODUPLICATES;\n    by patient_id visit_date;\nrun;\n/* 完全に同一の行のみ重複削除（結果が異なる） */\n解説:\n\nNODUPKEYとNODUPLICATESは似ているが動作が異なる\n同じ出力データセット名を使うことで、どちらが実行されたかが分からなくなる\n結果として異なるデータセットが同じ名前で作成される"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#マクロプログラミングの悪魔術",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#マクロプログラミングの悪魔術",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.12 🎭 マクロプログラミングの悪魔術",
    "text": "1.12 🎭 マクロプログラミングの悪魔術\n\n1.12.1 動的マクロ生成の混乱\n/* 悪い例：実行時にマクロを動的生成して混乱させる */\n%macro generate_confusion(type);\n    %if &type = A %then %do;\n        %macro process_data;\n            data result; set input; value = value * 2; run;\n        %mend;\n    %end;\n    %else %if &type = B %then %do;\n        %macro process_data;\n            data result; set input; value = value / 2; run;\n        %mend;\n    %end;\n    %else %do;\n        %macro process_data;\n            data result; set input; value = .; run;\n        %mend;\n    %end;\n%mend;\n\n/* 実行時に決定される処理内容 */\n%generate_confusion(A)\n%process_data  /* 何が実行されるかは実行時まで不明 */\n解説:\n\n実行時に条件によって異なるマクロ定義を生成\n同名のマクロprocess_dataが異なる処理を行う\nプログラムを読んだだけでは実際の処理内容が予測できない\n\n\n\n1.12.2 マクロ変数の隠蔽と再定義\n/* 悪い例：グローバル変数を局所的に再定義 */\n%let important_factor = 1.5;  /* グローバル設定 */\n\n%macro sneaky_calculation(data);\n    %local important_factor;  /* 局所的に再定義 */\n    %let important_factor = 0.1;  /* 全く違う値 */\n    \n    data &data._adjusted;\n        set &data;\n        adjusted_value = original_value * &important_factor;\n    run;\n    \n    %put NOTE: Applied factor &important_factor to &data;\n%mend;\n\n/* 使用者は1.5倍されると期待するが... */\n%sneaky_calculation(patient_data)  /* 実際は0.1倍 */\n解説:\n\nグローバル変数と同名の局所変数を定義\n使用者はグローバル値（1.5）が使われると期待\n実際は局所変数の値（0.1）が使用される\nPUTステートメントで正しい値を表示するため、ログを見ても気づきにくい\n\n\n\n1.12.3 マクロクォート関数の連鎖\n/* 悪い例：複数のクォート関数を連鎖させて混乱 */\n%macro quote_chaos(input);\n    %let step1 = %nrstr(&input);\n    %let step2 = %superq(step1);\n    %let step3 = %nrbquote(&step2);\n    %let step4 = %unquote(&step3);\n    \n    %if &step4 = &input %then\n        %put SUCCESS: Values match;\n    %else\n        %put ERROR: Values do not match - &step4 vs &input;\n%mend;\n\n%quote_chaos(test_value)\n解説:\n\n複数のマクロクォート関数を意味もなく連鎖\n各ステップで文字列の扱いが微妙に変化\n最終的な比較結果が予測困難\nデバッグが非常に困難"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#データアクセスとライブラリの混乱",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#データアクセスとライブラリの混乱",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.13 🗃️ データアクセスとライブラリの混乱",
    "text": "1.13 🗃️ データアクセスとライブラリの混乱\n\n1.13.1 ライブラリ参照の動的変更\n/* 悪い例：実行中にライブラリ参照を変更 */\nlibname mydata \"/path/to/original/data\";\n\ndata important_analysis;\n    set mydata.patients;  /* 最初のデータソース */\n    \n    /* 途中で同じライブラリ名を別パスに変更 */\n    call execute('libname mydata \"/path/to/different/data\";');\n    \n    /* この後のmydata参照は別のデータを指す */\n    merge mydata.treatments mydata.outcomes;  /* 異なるデータソース */\n    by patient_id;\nrun;\n解説:\n\nCALL EXECUTEを使って実行中にライブラリ参照を変更\n同一プログラム内で同じライブラリ名が異なるデータを指すことになる\nデータの整合性が完全に失われる可能性\n\n\n\n1.13.2 隠れたデータ変更\n/* 悪い例：読み取り専用に見えるが実際は変更している */\ndata summary_report;\n    set master_data;  /* 読み取りのみに見える */\n    \n    /* 隠されたMODIFYステートメント */\n    if _n_ = 1 then do;\n        call execute('\n            data master_data;\n                modify master_data;\n                if patient_id = \"DUMMY001\" then delete;\n            run;\n        ');\n    end;\n    \n    /* 集計処理 */\n    summary_stat = mean(value);\n    output;\nrun;\n解説:\n\n一見データを読み取りのみしているように見える\n実際はCALL EXECUTEで元データを変更している\n集計レポート作成時に元データが変更されるという予期しない副作用"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#統計プロシージャの悪用",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#統計プロシージャの悪用",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.14 📊 統計プロシージャの悪用",
    "text": "1.14 📊 統計プロシージャの悪用\n\n1.14.1 PROC REGの隠れた前提条件違反\n/* 悪い例：前提条件を満たさないデータで回帰分析 */\nproc reg data=patient_data;\n    model outcome = treatment age weight;  /* 一見正常な回帰式 */\n    \n    /* 隠された問題：\n       - outcomeには欠測値が50%\n       - treatmentは完全に共線性のある3つのダミー変数\n       - ageとweightは完全相関（r=1.0）\n       - データには外れ値が意図的に挿入済み\n    */\nrun;\n解説:\n\n統計的前提条件を全く満たさないデータで分析実行\n多重共線性、欠測値、外れ値などの問題を隠蔽\n結果は統計的に無意味だが、出力は正常に見える\n\n\n\n1.14.2 PROC FREQの誤解を招く使用\n/* 悪い例：意図的に誤解を招くクロス集計 */\nproc freq data=clinical_data;\n    /* 一見、治療効果の評価に見えるが... */\n    tables treatment*outcome / chisq;\n    \n    /* 実際のデータには重大な問題：\n       - outcomeは治療開始前の状態\n       - treatmentは別の研究での割り付け\n       - 同一患者が複数回カウントされている\n    */\n    \n    title \"Treatment Effect Analysis\";  /* 誤解を招くタイトル */\nrun;\n解説:\n\n変数名から因果関係があるように見せかける\n実際は時系列が逆転していたり、関係のないデータ\nタイトルで意図的に誤解を招く"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#セキュリティと権限の悪用",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#セキュリティと権限の悪用",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.15 🔒 セキュリティと権限の悪用",
    "text": "1.15 🔒 セキュリティと権限の悪用\n\n1.15.1 パスワードとアクセス制御の隠蔽\n/* 悪い例：隠されたデータベース接続 */\n%let hidden_pw = %substr(%sysfunc(compress('pass1word2',,'kd')),1,8);\n\nlibname secret oracle user=admin password=\"&hidden_pw\" \n                   path=\"//hidden.server.com:1521/secret_db\"\n                   schema=confidential;\n\n/* 一見通常のデータ処理 */\ndata public_summary;\n    set secret.classified_data;  /* 実際は機密データにアクセス */\n    /* パスワードは暗号化されて見えない */\nrun;\n解説:\n\nパスワードを関数で暗号化・難読化\n機密データベースへの隠れたアクセス\n表面上は通常のデータ処理に見せかけ"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#オペレーティングシステム固有の悪用",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#オペレーティングシステム固有の悪用",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.16 🌐 オペレーティングシステム固有の悪用",
    "text": "1.16 🌐 オペレーティングシステム固有の悪用\n\n1.16.1 Windows環境での隠れた設定\n/* 悪い例：アイコンのプロパティで隠れたオプション設定 */\n/* SASアイコンのプロパティで以下を設定（見えない場所） */\n/*\nTarget: \"C:\\SAS\\sas.exe\" -CONFIG \"C:\\hidden\\malicious.cfg\" \n                          -AUTOEXEC \"C:\\hidden\\autoexec.sas\"\n                          -SYSIN \"C:\\decoy\\normal_program.sas\"\n                          -ARCH=BIT16\n*/\n\n/* 実際に実行されるのは hidden/autoexec.sas の内容 */\n/* ユーザーは normal_program.sas が実行されると思っている */\n解説:\n\nWindowsのアイコンプロパティで隠れた設定を行う\nユーザーには見えない設定ファイルや自動実行ファイルを指定\n16ビットモードで実行して性能を意図的に低下\nユーザーが期待するプログラムとは異なるものを実行\n\n\n\n1.16.2 外部DLLとシステムコールの悪用\n/* 悪い例：外部ライブラリで隠れた処理 */\nfilename hidden 'hidden_malicious.dll';\n\ndata _null_;\n    /* 表面上は時刻の取得 */\n    current_time = datetime();\n    \n    /* 実際は外部DLLで隠れた処理を実行 */\n    call module(hidden, 'secret_function', current_time, result);\n    \n    /* DLLの中身：\n       - ファイルシステムの操作\n       - ネットワーク通信\n       - データの外部送信\n       - ログの改竄\n    */\nrun;\n解説:\n\n外部DLLを使って隠れた処理を実行\nSASのログには通常の処理のみ記録\n実際の悪意ある処理はDLL内に隠蔽\nシステム権限を悪用した危険な操作"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#実際の被害例と教訓",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#実際の被害例と教訓",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.17 ⚠️ 実際の被害例と教訓",
    "text": "1.17 ⚠️ 実際の被害例と教訓\n\n1.17.1 臨床試験での重大インシデント\n/* 実際にあった危険な例（教育目的で再現） */\n\n/* 悪い例：効果量の計算で隠れたバイアス */\ndata efficacy_analysis;\n    set clinical_data;\n    \n    /* 表面的には標準的な効果量計算 */\n    if treatment = 'Active' then trt_num = 1;\n    else if treatment = 'Placebo' then trt_num = 0;\n    \n    /* 隠されたバイアス：特定の患者のみ除外 */\n    if patient_id in ('001', '047', '089') and trt_num = 0 then delete;\n    /* これらは偶然プラセボ群で改善した患者 */\n    \n    /* 通常の統計計算 */\n    effect_size = (active_mean - placebo_mean) / pooled_sd;\nrun;\n\n/* 結果：人為的に効果が過大評価される */\n解説:\n\n一見正常な効果量計算に見える\n実際は特定の患者（プラセボ群で改善した例）を意図的に除外\n結果として薬剤効果が過大評価される\n規制当局への提出データに重大な問題を作り出す\n\n\n\n1.17.2 データ管理での混乱事例\n/* 実際にあった混乱例 */\n\n/* 悪い例：バックアップのつもりが本番データを破壊 */\n%let backup_date = %sysfunc(today(), yymmddd6.);\n\n/* 本番データの「バックアップ」 */\ndata backup.patient_data_&backup_date;\n    set production.patient_data;\nrun;\n\n/* 隠れた問題：productionとbackupが同じ場所を指している */\n/* libname production \"//server/data/current\"; */\n/* libname backup    \"//server/data/current\"; */\n\n/* 結果：バックアップではなく上書きが発生 */\n解説:\n\nバックアップ処理のつもりで実装\n実際は両方のライブラリが同じ場所を指している\nバックアップではなく本番データの上書きが発生\nデータロストの原因となる"
  },
  {
    "objectID": "posts/statistics/2025/SASプログラミングの風刺.html#まとめ",
    "href": "posts/statistics/2025/SASプログラミングの風刺.html#まとめ",
    "title": "PROGRAMMING FOR JOB SECURITY REVISITED",
    "section": "1.18 まとめ",
    "text": "1.18 まとめ\nこの記事で紹介した「技法」は、実際のプログラムでは絶対に使用してはいけません。これらは全て、実際に遭遇する可能性のある問題パターンです。\n\n“In reality there is a constant demand for SAS programmers. Just knowing how to write good, clean, and tight SAS code provides a high level of job security.”\n\n良いSASプログラマーになるためには：\n\n可読性を重視したコード作成\n適切な命名規則の遵守\n十分なドキュメント化とコメント\n論理的な構造の維持\n品質管理オプションの適切な使用\n一貫したプログラミングスタイル\n他者が保守しやすいコード設計\n\n特に臨床試験の統計解析では、コードの品質が直接患者の安全性と規制当局への信頼性に関わるため、これらの原則を厳格に守ることが重要です。\nこの記事は、Arthur L. Carpenter氏とTony Payne氏の「Programming for Job Security」シリーズを基に作成されました。元の論文は風刺的な内容ですが、実際のプログラミングにおいて避けるべき問題を明確にする優れた教材として、SAS界隈で長く愛され続けています。\n参考文献:\n\nCarpenter, Arthur L. (1993, 1996). “Programming For Job Security: Tips and techniques to Maximize Your Indispensability”\nCarpenter, Arthur L. & Payne, Tony. “Programming For Job Security Revisited: Even More Tips and Techniques to Maximize Your Indispensability”"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html",
    "href": "posts/statistics/2025/SASマクロ入門1.html",
    "title": "SASマクロ入門1",
    "section": "",
    "text": "本記事では、2022年SASユーザー総会の森田氏の「マクロのすすめ～SASにプログラムをかいてもらおう～」の文章を写経したものである。個人の勉強記録であるため、基本的には元の資料を参考にしていただきたい。\n\n\n基礎的な事項で参考になるものはいつも通り、以下のブログである。特に実務上で重要だが知られていないデータステップで変数をマクロ化するcall syputx、データステップ外で関数を使えるようにする%sysfuncはきちんと理解したい。また、マクロ言語入門9で紹介されている&macro_variable.の.は常に記載する、もしくは記載しない等を組織/個人開発で統一しておくことが望ましい。\n\nマクロ言語入門1：マクロ変数とは【%LET】\nマクロ言語入門2：マクロの登録と実行【%MACRO、%MEND】\nマクロ言語入門3：パラメータの設定【定位置パラメータ】\nマクロ言語入門4：パラメータの設定【キーワードパラメータ】\nマクロ言語入門5：クォート処理【%STR関数】\nマクロ言語入門6：クォート処理【%BQUOTE関数】\nマクロ言語入門7：マクロ内でのループ処理【%DO】\nマクロ言語入門8：マクロ内での条件分岐処理【%IF】\nマクロ言語入門9：マクロ変数とドット\nマクロ言語入門10：マクロ変数と&&\nマクロ言語入門11：演算評価 【%EVAL、%SYSEVALF】\n値をマクロ変数に格納する「CALL SYMPUTX」その1\nデータステップ外で関数を使えるようにする「%SYSFUNC」その１\n森岡 裕, %if-%then-%doのオープンコードでの利用と9.4以降のSASマクロ拡張点について, SASユーザー総会論文集, 2021.\n本本 早紀, クォート処理及びスコープへの理解を深める, SASユーザー総会論文集, 2019, p141-150\n竹田 真, 佐藤 智美, 社内マクロライブラリの構築について, SASユーザー総会論文集, 2001, p37-44\n柳沢 健太郎, 常吉 華奈, 山本 典子, 臨床試験における集計解析用 SASプログラムの標準化, SASユーザー総会論文集, 2004, p37-44\n田村 洋介, SASマクロライブラリの開発/管理/運用, SASユーザー総会論文集, 2007, p123-134\n“How to organize your SAS projects in Git”, SAS Blogs, 2020-11-10\n“Good Programming Practice In Macro Development”, PhUSE Advance Hub, 2021-09-21\nRon Cody, Cody’s Data Cleaning Techniques Using SAS, SAS Press, 2017, 234p\n市橋 里絵, 江口 幸子, 渡邊 大丞, 月田 あづき, “Standard Template Programs”の開発, SASユーザー総会論文集, 2010, p381-383\n\nまた、他にも応用上の使い方等は以下が参考になる。 - Compareプロシジャの結果が一致か不一致か、何が不一致かをマクロ変数で取得する話 なお、私が知る限りProc Compare Procedureの解説文献は、2022年度のSASユーザー総会資料のCOMPAREプロシジャの便利な使い方がおすすめである。Proc Compare Procedureについては別記事で解説する。"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html#参考ブログ",
    "href": "posts/statistics/2025/SASマクロ入門1.html#参考ブログ",
    "title": "SASマクロ入門1",
    "section": "",
    "text": "基礎的な事項で参考になるものはいつも通り、以下のブログである。特に実務上で重要だが知られていないデータステップで変数をマクロ化するcall syputx、データステップ外で関数を使えるようにする%sysfuncはきちんと理解したい。また、マクロ言語入門9で紹介されている&macro_variable.の.は常に記載する、もしくは記載しない等を組織/個人開発で統一しておくことが望ましい。\n\nマクロ言語入門1：マクロ変数とは【%LET】\nマクロ言語入門2：マクロの登録と実行【%MACRO、%MEND】\nマクロ言語入門3：パラメータの設定【定位置パラメータ】\nマクロ言語入門4：パラメータの設定【キーワードパラメータ】\nマクロ言語入門5：クォート処理【%STR関数】\nマクロ言語入門6：クォート処理【%BQUOTE関数】\nマクロ言語入門7：マクロ内でのループ処理【%DO】\nマクロ言語入門8：マクロ内での条件分岐処理【%IF】\nマクロ言語入門9：マクロ変数とドット\nマクロ言語入門10：マクロ変数と&&\nマクロ言語入門11：演算評価 【%EVAL、%SYSEVALF】\n値をマクロ変数に格納する「CALL SYMPUTX」その1\nデータステップ外で関数を使えるようにする「%SYSFUNC」その１\n森岡 裕, %if-%then-%doのオープンコードでの利用と9.4以降のSASマクロ拡張点について, SASユーザー総会論文集, 2021.\n本本 早紀, クォート処理及びスコープへの理解を深める, SASユーザー総会論文集, 2019, p141-150\n竹田 真, 佐藤 智美, 社内マクロライブラリの構築について, SASユーザー総会論文集, 2001, p37-44\n柳沢 健太郎, 常吉 華奈, 山本 典子, 臨床試験における集計解析用 SASプログラムの標準化, SASユーザー総会論文集, 2004, p37-44\n田村 洋介, SASマクロライブラリの開発/管理/運用, SASユーザー総会論文集, 2007, p123-134\n“How to organize your SAS projects in Git”, SAS Blogs, 2020-11-10\n“Good Programming Practice In Macro Development”, PhUSE Advance Hub, 2021-09-21\nRon Cody, Cody’s Data Cleaning Techniques Using SAS, SAS Press, 2017, 234p\n市橋 里絵, 江口 幸子, 渡邊 大丞, 月田 あづき, “Standard Template Programs”の開発, SASユーザー総会論文集, 2010, p381-383\n\nまた、他にも応用上の使い方等は以下が参考になる。 - Compareプロシジャの結果が一致か不一致か、何が不一致かをマクロ変数で取得する話 なお、私が知る限りProc Compare Procedureの解説文献は、2022年度のSASユーザー総会資料のCOMPAREプロシジャの便利な使い方がおすすめである。Proc Compare Procedureについては別記事で解説する。"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html#マクロはsasにプログラムを書いてもらうための機能",
    "href": "posts/statistics/2025/SASマクロ入門1.html#マクロはsasにプログラムを書いてもらうための機能",
    "title": "SASマクロ入門1",
    "section": "2.1 マクロはSASにプログラムを書いてもらうための機能",
    "text": "2.1 マクロはSASにプログラムを書いてもらうための機能\nマクロは、簡単に言うとテキストの置換機能である。Aという文字列をBという文字列に置き換える機能である。そして、このテキスト置換機能がたいへん役に立つ。なぜなら、プログラミング業務では、似たような解析やデータハンドリングを繰り返し行っている場合が多いからである。例えば、他のプロジェクトと同じ解析を行う、対象のデータセット名だけが異なる、対象データの抽出条件だけが異なる、処理対象の変数だけが異なる、設定値やオプション指定だけが異なる、出力形式（行数や列数、ファイル形式など）だけが異なる。こういった場合、各解析のSASプログラムの大部分が重複することになり、差異が生じるのは、ほんの一部となる。つまり、基準となるプログラムをコピー&ペーストで複製して、変更が必要な箇所だけをテキスト置換すれば済む場合が多い。この基準となるプログラムの設定とテキスト置換をSASプログラムで実現するための機能がマクロである。マクロがプログラムを書いてくれるのである。"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html#マクロの仕組み",
    "href": "posts/statistics/2025/SASマクロ入門1.html#マクロの仕組み",
    "title": "SASマクロ入門1",
    "section": "2.2 マクロの仕組み",
    "text": "2.2 マクロの仕組み\n私たちの書いたSASプログラムは、SASのコンパイラによって解釈され、実行される。このとき、実はSASには二種類のコンパイラがある。\n1. マクロを解析・実行するマクロプロセッサ、\n2. DATAステップおよびPROCステップを解析・実行するコンパイラである。\nSASプログラムをサブミットすると、まず、①マクロプロセッサがマクロ部分だけを解析・実行し、DATAステップまたはPROCステップの命令だけのプログラムを作成する。その後、②のコンパイラによって、マクロ部分が解析された後のプログラムを実行する。"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html#マクロ変数を使う",
    "href": "posts/statistics/2025/SASマクロ入門1.html#マクロ変数を使う",
    "title": "SASマクロ入門1",
    "section": "2.3 マクロ変数を使う",
    "text": "2.3 マクロ変数を使う\n本章では、マクロの基本的な機能であるマクロ変数について概説する。シンプルな機能ながら応用場面は多い。マクロ変数を習得するだけでもプログラミングの効率化や品質向上が期待できる。\n\n2.3.1 マクロ変数とは\nマクロ変数はテキストを格納する容れものである。マクロ変数に格納したテキストはプログラム中で参照できる。マクロプロセッサは、SASプログラム内でマクロ変数の参照箇所を見つけると、そのマクロ変数に格納したテキストに置き換える。テキストファイルで行う一括置換をSASに実行してもらうイメージである。\n\n\n2.3.2 マクロ変数の作成と参照\nマクロ変数は%letステートメントを利用して作成する。\n%let マクロ変数名 = 格納したい値;\nマクロ変数名は最大32文字、最初の文字は英字またはアンダースコア、その後の文字は英字・数字・アンダースコアが使用可能である。また大文字と小文字は区別されない。設定値AF、DMS、SQL、SYSは該当する自動マクロ変数と名前が重複する可能性があるため、避けたほうがよい。\n%let greeting = Hello World;\n%put &greeting.;\n%put 「Hello World」と表示;\n\n%let year = 40;\n%let comment = おめでとうございます;\n%put SASユーザー総会&year.周年&comment.;\n%put 「SASユーザー総会40周年おめでとうございます」と表示;\n\n%let anavar = age; /* マクロ変数&anavarを定義し、ageという値を格納 */\n\n/* 単変量解析; */\nproc univariate data = sashelp.class;\nvar &anavar.; /* マクロプロセッサによって&age.に置換される */\nclass sex;\nrun;\n\n/* 単変量解析(bee-swarm plot)を作成; */\nproc sgplot data = sashelp.class;\nvbox &anavar. / category = sex nofill nooutliers;\nscatter y = &anavar. x = sex / jitter;\nrun;\nマクロ変数により、プログラムに一貫性を持たせることができる。例えば、追加解析や仕様変更により、年齢(age)ではなく体重(weight)の解析を行いたい場合は、%let anavarの定義部分だけを変更するだけで済む。人の手を介することで修正漏れやミスタイプのリスクがある。\nまた、CALL SYMPUTXルーチンを利用すれば、DATAステップでデータセットの変数の内容をマクロ変数に格納できる。次章で説明する制御構文と併せて、データセットの内容に応じて、プログラムを変更させることが可能になり、プログラムに柔軟性を与えられる。\ncall symputx('マクロ変数名', 格納したい値(DATAステップの変数名));\n\n/* 男女別の生徒数を数えて、それぞれをマクロ変数に格納する; */\nproc freq data = sashelp.class noprint;\ntables sex / out = out1;\nrun;\n\ndata _null_;\nset out1;\nif sex = \"男子\" then call symputx('N_Male', count);\nif sex = \"女子\" then call symputx('N_Female', count);\nrun;\n\n/* マクロ変数の値をログに出力して確認; */\n%put N_Male = &N_Male N_Female = &N_Female;/* 「N_Male : 10 N_Female : 9」と出力; */\nまた、SQLプロシジャのINTO句を利用して、データセットの変数の内容をマクロ変数に格納することも可能である。\nproc sql;\nselect 変数名1, 変数名2, ..., 変数名N\ninto :マクロ変数名1, :マクロ変数名2, ..., :マクロ変数名N\nfrom データセット名;\nquit;\n\nproc sql noprint;\ncreate table work.class as\nselect *\nfrom sashelp.class\nrun;\nquit;\n\n%put &sqlobs.; /* proc sqlで直近に処理したデータ(OBS数)を格納; */\n\n\n2.3.3 マクロ変数は文字型変数\nマクロ変数は、DATAステップと違って、すべて文字型変数として扱われる。このため、マクロプロセッサに数値演算をさせるような場面では、注意が必要である。数値の場合は%eval関数、小数を含む場合は%sysevalf関数に演算式を渡す必要がある。（ただし、%evalと%sysevalfは、演算のおよそについても調査した上で利用しないと）\n%let not5 = 1 + 4;\n%put &not5.; /* 「1 + 4」と表示; */\n\n%let equal5 = %eval(1 + 4);\n%put &equal5.; /* 「5」と表示; */\n\n%let not5 = 1.5 + 3.5;\n%put &not5.; /* 「1.5 + 3.5」と表示; */\n\n%let equal5 = %sysevalf(1.5 + 3.5);\n%put &equal5.; /* 「5」と表示; */\n\n%let not5 = %eval(1.5 + 3.5); /* %evalは整数計算だけなのでエラーとなる */\nなお、格納されるテキストによってマクロ変数の変数は自動的に調整されるため、データセットの文字型変数のように長さを気にする必要はない（ただし、SAS9.4の最大長は65,534文字である）。\n\n\n2.3.4 自動マクロ変数\nマクロプロセッサが自動的に作成する自動マクロ変数もある。自動マクロ変数は、実行環境の確認、プログラム実行時の表示などに利用できる。いくつか例を示す。\n\n\n\n\n\n\n\n自動マクロ変数名\n内容\n\n\n\n\nSYSVER\nSASのバージョンを格納（例：9.4）\n\n\nSYSDATEP\nSAS セッションの開始日をDATEフォーマットで格納（例：01SEP2022）\n\n\nSYSLAST\nSAS セッションで直近に作成したデータセットを格納（例：WORK.CLASS）\n\n\nSYSUSERID\n現在のSAS プロセスのユーザーIDを格納（例：morita.yusuke）"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html#マクロプログラムを使う",
    "href": "posts/statistics/2025/SASマクロ入門1.html#マクロプログラムを使う",
    "title": "SASマクロ入門1",
    "section": "2.4 マクロプログラムを使う",
    "text": "2.4 マクロプログラムを使う\n本章では、マクロの主要機能であるマクロプログラムについて概説する。前章のマクロ変数とマクロプログラムを組み合わせることで、より複雑なプログラムをマクロプロセッサに書いてもらうことができる。\n\n2.4.1 マクロプログラムとは\nマクロプログラムもマクロ変数と同様に、テキスト置換を行う機能である。マクロ変数は、プログラム中の変数名、データセット名、オプションまたは短いテキストの置換に用いられる場合が多い。一方、マクロプログラムは、あるまとまった単位のSAS プログラムへの置換に利用されるものであり、マクロ変数と組み合わせることで、また、制御構文を使用することで、さまざまなプログラムをマクロプロセッサに手軽に作成してもらうことができる。\n\n\n2.4.2 マクロプログラムの作成・呼び出し方法\nマクロプログラムは%macroおよび%mendステートメントを利用して作成する。\n%macro マクロプログラム名; （簡略したいテキスト） %mend マクロプログラム名;\nマクロプログラム名は最大32文字、最初の文字は英字またはアンダースコア、その後の文字は英字・数字・アンダースコアが使用可能である。\n\n\n2.4.3 マクロプログラムの特徴\n\n2.4.3.1 呼び出し時にマクロ変数を受け取ることができる\n基準となるプログラムをコピー&ペーストして、一部分を書き換えたい場合があ る。この基準となるプログラムをマクロプログラム内に格納して、書き換えたい箇所を呼び出し時に受け取れるマクロ変数として、マクロプログラムに指定できる。このマクロプログラム呼び出し時に受け取るマクロ変数をマクロパラメータという。マクロパラメータも、ユーザーが任意の名前を設定可能で、そのマクロプログラム内で参照可能なマクロ変数となる。マクロパラメータとしてデータセット名(dsn)および変数名(var)を指定する例を以下に示す。\n/* 呼び出し時にマクロパラメータを指定; */\n%macro univariate2(dsn, var); /* マクロ名の後にマクロパラメータを設定; */\n\nproc univariate data = &dsn.;\nvar &var.;\nclass sex;\nrun;\n\n%mend;\n\n/* 呼び出し時にマクロパラメータを指定して実行; */\n%univariate2(dsn=sashelp.class, var=age)\n%univariate2(dsn=sashelp.class, var=weight)\n%univariate2(dsn=sashelp.class, var=height)\nまた、マクロパラメータに予め既定値を与えることもできる。この既定値をもつマクロパラメータをキーワードパラメータという。キーワードパラメータは呼び出し時に指定しなければ、既定値が自動的に設定される。したがって、ほぼ毎回同じパラメータで実行する可能性があるパラメータはキーワードパラメータとして宣言すとよい。\n\n\n2.4.3.2 制御構文が使える\nDATAステップでは、条件分岐のIFステートメント、反復処理のDOステートメントが使用できる。マクロ言語にも同様に%ifステートメント、%doステートメントが用意されており、データセットの内容やマクロパラメータの内容に応じて、マクロプロセッサの動作を変更できる。これら制御構文はマクロ変数にはない機能で、マクロプログラムに柔軟性を与える機能の一つとなっている。\n制御構文の例として、条件分岐の%ifステートメントについて説明する。\n%if 条件文 %then %do;\n(条件文がTrueの場合の処理)\n%end;\n%else %if 条件文 %then %do;\n(条件文がTrueの場合の処理)\n%end;\n%else %do;\n(全条件文をも不満足している場合の処理)\n%end;\nなお、上記のdsn および var のように、既定値を持たないマクロパラメータを位置パラメータという。\nまた、DATAステップでは、条件分岐から反復処理の%doステートメントについて説明する。\n%macro list_by_4years(startyr, endyr);\n    title1 \"Customer List of &yr. to &endyr.\";\n    proc print data = work.customer;\n        where &yr. &lt;= year &lt;= &endyr., &by 4;\n        var year customer_revenue;\n    run;\n    title1 \"Customer List of 2000 to 2003\";\n    proc print data = work.customer;\n        where 2000 &lt;= year &lt;= 2003;\n        var year customer_revenue;\n    run;\n    title1 \"Customer List of 2004 to 2007\";\n    proc print data = work.customer;\n        where 2004 &lt;= year &lt;= 2007;\n        var year customer_revenue;\n    run;\n%mend;\n\n%list_by_4years(startyr=1996, endyr=2007)\nマクロの制御構文には、他にも%do-%while、%do-%until等がある。しかし、初心者のうちは%ifと%doの2つを事実としても余りがない。%ifと%doもマクロプロセッサに対する命令である。一方、DATAステップのコンパイラに対する命令であり、データセットの変数の内容を変更することになる。ただし、%ifと%do%sysevalfは、SAS9.4 M5からマクロプログラム内部でなくても（オープンコードで）%if%then%else等を使えるようになった。2021年のSASユーザー総会の発表資料に詳細が記載されている。\n\n\n\n2.4.4 マクロ関数\n字句SASで用意されたマクロ関数を利用できる。これらのマクロ関数は、マクロプロセッサによって実行されるマクロプログラムである。DATAステップでデータセットを編集するために利用される関数と同名・同機能であっても異なるものであることに留意されたい。以下にマクロ関数の一例を示す。\n\n\n\n\n\n\n\nマクロ関数名\n説明\n\n\n\n\n%upcase(文字列) / %lowcase(文字列)\n文字列を大文字/小文字に変換する\n\n\n%trim(文字列)\n文字列の末尾の余分なスペース文字を取り除く\n\n\n%index(文字列1, 文字列2)\n文字列1の中に文字列2が含まれていれば、最初の位置(何文字目)を返す。文字列2が含まれていなければ0を返す\n\n\n%sysfunc(関数名)\nDATAステップの関数をマクロプロセッサから使用する\n\n\n\n\n\n2.4.5 マクロ変数のスコープ\nマクロ変数にもスコープ、つまりプログラム中で参照可能な範囲がある。スコープによってglobal マクロ変数とlocal マクロ変数に区分される。global マクロ変数は、プログラムのどこからでも値を参照可能である。一方、local 変数は、その変数が宣言されたマクロプログラム内部でのみ値を参照可能である。\n/* マクロ変数のスコープ; */\n%global global_var; /* グローバルマクロ変数を宣言; */\n\n%macro global_local;\n    %let local_var = ローカル変数です; /* このマクロ内部だけで有効期間; */\n    %let global_var = グローバル変数です;\n%mend;\n\n%global_local /* マクロを実行; */\n\n%put &global_var.; /* グローバル変数なので、マクロ外でも参照可能; */\n%put &local_var.; /* マクロ外なでマイナ一ルで、変数を参照できない; */\n初心者のうちは、スコープを意識する機会はさほど無いかもしれない。しかし、チームでマクロプログラムを分担して構築する場合、マクロプログラムで構築されるシステムを利用する際は、マクロ変数の衝突を避けるため、マクロ変数のスコープに留意が必要となる。（local マクロ変数を明示的に作成するためのlocal ステートメントが用意されている。）"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html#マクロをデバッグする",
    "href": "posts/statistics/2025/SASマクロ入門1.html#マクロをデバッグする",
    "title": "SASマクロ入門1",
    "section": "2.5 5. マクロをデバッグする",
    "text": "2.5 5. マクロをデバッグする\n本節で同じ章にまとめ」で、マクロを書けばバグが生じる。したがって、マクロを効率的にデバッグする技術を重要である。以下に、マクロプロセッサが、マクロプログラムを生じさせたのかを知ることが有用である。このため、マクロのデバッグを効率的に行うためのオプションが用意されている。以下に、よく使用するオプションの例を示す。\n\n\n\n\n\n\n\nオプション名\n内容\n\n\n\n\nMPRINT\nマクロプロセッサによって生成されるSAS プログラムをSAS ログに表示する\n\n\nMLOGIC\nマクロプロセッサがマクロパラメータにどのような値を受け取ったか、%if条件分岐を True/Falseのどちらに判断したか、マクロの開始点と終了点をSASログに表示する\n\n\nSYMBOLGEN\nマクロプロセッサが、マクロ変数をどの値に置き換えたかSAS ログに表示する\n\n\n\nこれらのオプションを指定することで、SAS ログにマクロプロセッサからの情報が出力されるようになる。しかし、上記オプションを指定してもSAS ログを読み解くのが困難な場合もある。その場合、mfileオプションでマクロプロセッサが出力したプログラム全体を別ファイルとして出力することも可能である。（マクロの学習にも活用できる。）"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html#マクロをライブラリとして整理して活用する",
    "href": "posts/statistics/2025/SASマクロ入門1.html#マクロをライブラリとして整理して活用する",
    "title": "SASマクロ入門1",
    "section": "2.6 6. マクロをライブラリとして整理して活用する",
    "text": "2.6 6. マクロをライブラリとして整理して活用する\n作成したマクロは積極的に活用したほうがよい。一度制作したマクロプログラム、別の活用場面では思いがけずれなく再現全文が現在の場合がある。つまり、マクロはほとんど常設され、品質も向上していく。そこで本章では、作成したマクロプログラムをライブラリとして整理し、プログラムから呼び出し可能にする方法について説明する。\n以下にマクロライブラリを作成する手順を示す。\n\nマクロを格納するフォルダを用意する。（フォルダは複数あってもよい）\n各マクロプログラムを個別ファイルに格納していうフォルダに保存する。 このとき、ファイル名はマクロ名と同じにする必要がある。（マクロ名.sas とする）\nマクロを呼び出す側のプログラムで、以下のMAUTOSOURCE及びSASAUTOSの2つのオプションを指定する。このとき、SASAUTOSに（1）のフォルダを指定する。\n\n/* マクロをライブラリとして活用する; */\noptions mautosource sasautos=(sasautos, \"C:\\sasYearlyMacros\");\n/* （1）の sasautos内容ないこと; */\noptions mautosource sasautos=(sasautos, \"C:\\sasYearlyMacros\" \"C:\\sasYearMacros\");\nこれで、マクロライブラリ中のマクロプログラムが呼び出し可能になる。\nまた、本章では触れないが、継続として共有するライブラリを整備・活用した事例は、過去のSASユーザー総会の複数の発表がある。\nまた、SAS9.4 M6 以降、WebページのバージョンProgramming Tool であるGitHub との連携機能が搭載されている。GitHub上でマクロファイルのバージョン管理を行い、SASから直接GitHub上のマクロプログラムをinclude可能になっている。多くのオープンソースマクロがGitHub上で管理・公開されており、GitHub上での連携機能について今後の発展が注目される。"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html#マクロのコツ",
    "href": "posts/statistics/2025/SASマクロ入門1.html#マクロのコツ",
    "title": "SASマクロ入門1",
    "section": "2.7 マクロのコツ",
    "text": "2.7 マクロのコツ\nどんな項目もで正しく場合ほど拡張を発想する。本章ではマクロを作成する際に留意したいポイントを説明する。筆者にして筆者の経験に価値たけ書もあるかもしれない。PhUSE6が「Good Programming Practice In Macro Development」を公開しており、マクロ開発に関する考査ずべきポイントを学ぶことができる。なお、機械のコーディングルールやガイドラインがある場合には、それらを優先されたい。\n\n2.7.1 マクロでコードの重複を排除する\nマクロによりコードの重複部分を削減し、プログラム全体をコンパクトに保つのがポイントである。そのために、マクロ化する際には、どのようなポイントで構成するか、マクロのパラメータをどこに配置するかマクロ作成時の設計が大式である。しかし、このためには、一定の実装経験を積んで、マクロプロセッサに作成させたコードのイメージが持てるように下はないと無難い趣もある。まずはマクロを含まないプログラムを作成してから、重複部分をマクロ化していくというアプローチがある。いずれにせよコードの重複部分はマクロ化を考慮するポイントである。\n\n\n2.7.2 マクロの機能と入出力を明示する\nそのマクロプログラムが、どんな機能を提供するマクロか、出力は何か、入力は何かの3点を明示することは、便利でマクロを作成するうえで重要である。そのマクロプログラムのマクロパラメータとして明示する。そのマクロパラメータも馴染みやすいネーミングを心掛けたい。また、コメントも適切に活用して必要な情報を補記するべき利用を使用して説明する。以下に、データクリーニングの名前(Cody’s Data Cleaning Techniques Using SAS）に掲載されたもののマクロプログラムの冒頭部分である。\n\n\n2.7.3 マクロの機能と入出力を明示する\nそのマクロプログラムが、どんな機能を提供するマクロか、出力は何か、入力は何かの3点を明示することは、便利でマクロを作成するうえで重要である。そのマクロプログラムのマクロパラメータとして明示する。そのマクロパラメータも馴染みやすいネーミングを心掛けたい。また、コメントも適切に活用して必要な情報を補記するべき利用を使用して説明する。以下に、データクリーニングの名前(Cody’s Data Cleaning Techniques Using SAS）に掲載されたもののマクロプログラムの冒頭部分である。\n%macro auto_outliers(\n    Dsn=,       /* Data set name */\n    ID=,        /* Name of ID variable */ \n    Var_list=,  /* List of variables to check */\n    Trim=1,     /* Trim criterion */\n    N_sd=2      /* Number of standard deviations */\n);\nコメントの細部もあるがAuto_Outliersマクロは外れ値を検出するマクロであることが読み取れる。入力として、対象データセット名(Dsn)、オブザベーションのID変数名(ID)、外れ値を抽出したい対象変数名(Var_list)、統計的な判断基準であるTrim_N_sd)を指定すればよいことがわかる。数章では、外れ値の統計基準やプログラムの詳細より、当初マクロプログラムからできることがわかる。したがって方がユーザーにより使いやすかっただいられる（Auto,はプログラム内部からできること）。\n\n\n2.7.4 マクロの内部をブラックボックス化し、抽象化する\n細部は基本原理は「プログラムの内部アルゴリズムを知らずとも、マクロプログラムの機能を利用可能にすること」である。これはプロシジャで用いるイメージに近い。例えば、私たちは SORT プロシジャに入力かのデータセット名とソート変数を指定すれば、プロシジャ内部のソートアルゴリズムを意識することなく目的を達成できる。このように、マクロプログラムを作成する際は、ユーザーにマクロの内部を意識させない形式をイメージすべきである。\nやや哲学的な内容になるが、優れたマクロは、その出力だけを実行された課題が魅力を呈しない。マクロの内部にソート順を含めてカテゴリーゼットは変更しないようにする。また、マクロの内部だけで作成する一時的なデータセットを削除しなければ、ユーザーは、そのマクロ内部の構造する必要がないらならない。また、「文今後、新を適時」の結構で、マクロの終了時にマクロ内部で作成したworkデータセットを削除することもある。\n\n\n2.7.5 マクロプログラムはなるべくシンプルを保つ\nプログラム設計の原則に「分割して統治する」および「Keep it simple, stupid」があり、マクロプログラムもなるべくシンプルに保つことを推奨する。これは、マクロプログラムの汎用性を高めることと方針テンションスピ方向にあることも分かいている。また、新たにマクロプログラムを作成する際は、既存のマクロプログラムの入力方法を工夫すれば詳細なプロ先はせずとも済まない、あるいは既存のシンプルなマクロプログラムを利用して（呼び出して）作成できないか構想すべき場合がある。品質は既存されたシンプルのマクロプログラムを利用すれば、マクロプログラム全体のコードを予備し、かつ新たに作成するマクロプログラムの品質も維持しようといううがい対象である。\n\n\n2.7.6 コードの読みやすさや理解のしやすさにも配慮する\nマクロを書けば、いろいろな場面で活用できるようになるが、SASにプログラムを書いてもらうのが楽しくなり、%や&が使いびう複雑なプログラムになる場合がある。もちろん、そのことは体はマクロを理解している範囲であり、マクロプロセッサも文法的に問題ないよう限りは説明に動いてくれる。一方で、保存する他のプログラマや求人の自分がプログラムを読み機会もあり、コードの読みやすさや理解のしやすさを保持して、あえてマクロ化しない（マクロ化しすぎない）という判断場合もある。\nまた、マクロプログラミングでも、DATAステップ同様にネーミングが重要である。インデント（字下げ）を適切に行い、マクロの条件分岐や反復構造などを把握しやすくする、効果的にコメントを入れるなど、コードの読みやすさについても心掛けたい。\n\n\n2.7.7 小さく始める\n小さく始めて、少しずつ理解するのが習得のコツである。最初はマクロ変数を使うところからでもよい。基本プログラムによく変更する部分だけをマクロ変数化して、プラクマの電用管理を設定可能にする、先行するすプロジェクトの設定を改めてせることも多いし、実はここまででも十分な効果が着込まれる場合がある。また、簡単なマクロプログラムを作成する際給与も設ける。マクロを作成する手順として、まずマクロを含まないプログラムを作成し、少しずつマクロ化していくアプローチが推奨されている。これによりマクロプロセッサに命令があるかが明り分けすされる。"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html#マクロの実例",
    "href": "posts/statistics/2025/SASマクロ入門1.html#マクロの実例",
    "title": "SASマクロ入門1",
    "section": "2.8 マクロの実例",
    "text": "2.8 マクロの実例\n筆者らが業務で実際に作成したマクロプログラムを紹介する。筆者で簡切と練習する事例であり、なるべく小題のプログラム編集で対応できるよう意図している。\n\n2.8.1 特定のフォルダ内のSASデータセットを1つのExcelファイルに変換する\nSASデータセットをデータセット別にシートを分けてExcelファイルに変換してほしいと依頼を受ける。もちろんデータセットの教だけprocexportまたはsetステートメントを記述してもよいのだが、そのような縮り返し処理はマクロ化したときに自動転するのがよい。なお、以下でマクロプログラムが直後されたフォルダ内のSASデータセットを、同フォルダに既定のExcelファイルを指定する必要がある。\n%macro sas2xlsx;\n\n    /* データセット一覧を取得する */\n    proc sql noprint;\n        select memname into :dsname1-\n        from dictionary.tables\n        where libname = \"SAS\";\n    quit;\n\n    do i = 1 to &sqlobs.;\n\n        data work.&&&dsname&i..;\n            set sas.&&&dsname&i..;\n        run;\n\n    %end;\n\n%mend;\n上記マクロでは、現在でデータセットのあるフォルダを出力先のExcelファイルに指定する必要がある。そこで、上記マクロプログラムが配置されたフォルダ内のSASデータセットを、同フォルダに既定のExcelファイルとして出力するようにプログラムを変更すれば、条件ごとにプログラムを編集する必要がなくなる。（もちろん不測の事象が生じする能性があるため、SASログ確認および出力ファイルの確認は時う。）\n\n\n2.8.2 特定のフォルダ内のSASデータセットをXPTファイルに一括変換する\n医療機関医療品の承認申請では、原則として申請電子データ提出が求められており、既導データをXPT形式で提出する必要がある。このためSASデータセットをXPT形式に変換する機会がよくある。なお、以下ではfilel.sasという名前でマクロプログラムを保存し、指定したデータセットが配置されたフォルダに配置してくれる。細部の具い事例である。\n/* 指定フォルダ内のSASデータセットをXPTファイルに一括変換する; */\noption nofmterr nologic print symbol;\n%let xpt = %str(C:\\workXpt);  \n%let sas = %str(C:\\workXsas); \n\nlibname _sas \"&sas.\";\nlibname _xpt xport \"&sas.\";\n\n%macro sas2xpt(sptdir, sasdir);\n\n/* ～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～; */\n/* ファイル一覧を取得する; */\n/* ～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～; */\n%filelist(\n    directory=%superq(sasdir), /* Directory to read */\n    out=_sas,                  /* Output data set to create */\n    extensions=sas7bdat        /* Space delimited extensions to include. Not case\n                                sensitive. Leave blank or set other based on extension */\n)\n\n/* ファイル一覧を読み込む; */\nproc sql noprint;\n    select dsname into :_dsname1 - \n    from _sas;\nquit;\n\n/* 各ファイルがない場合は終了する; */\n%if &sqlobs. = 0 %then %return;\n\nlibname _sas %superq(sasdir)\" access=readonly;\n\n%do i = 1 to &sqlobs.;\n/* ～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～; */\n/* SAS7BDATをXPTに変換する; */\n/* ～～～～～～～～～～～～～～～～～～～～～～～～～～～～～～; */\nlibname _xpt xport \"&sptdir\\&&&_dsname&i..&_xpt\";\ndata _xpt.&&&_dsname&i..;\n    set _sas.&&&_dsname&i..;\nrun;\n%end;\n\n/* ～～～～～～終了処理～～～～～～; */\nlibname _sas clear;\nlibname _xpt clear;\nproc datasets library=work nolist;\n    delete _sas;\nquit;\nrun;\nquit;\n\n%mend;\n\n%sas2xpt(sptdir=%superq(xpt), sasdir=%superq(sas))"
  },
  {
    "objectID": "posts/statistics/2025/SASマクロ入門1.html#終わりに",
    "href": "posts/statistics/2025/SASマクロ入門1.html#終わりに",
    "title": "SASマクロ入門1",
    "section": "2.9 終わりに",
    "text": "2.9 終わりに\nマクロ言語を対象にマクロの基礎から応用、そして利用のコツを概説した。マクロプロセッサにプログラムを書いてもらうライブラリ化により大量軸な共通化に及ぶまで、マクロは様々な場面で雄用いと感じてもらえれば幸いしたい。しかし、一度にすべてを理解する必要はなく、筆者ももともくさんのトライ&エラーを経験し、必要に迫られながら時間をかけてレットたという自分求むのが実情である。やがり理解できる自然に身につくき交鎖に考え、引用文献もご参照頂ければ幸いである。マクロプロセッサは、そこと出番を待っている。"
  },
  {
    "objectID": "posts/statistics/2025/STROBE.html",
    "href": "posts/statistics/2025/STROBE.html",
    "title": "STROBE",
    "section": "",
    "text": "Much biomedical research is observational. The reporting of such research is often inadequate, which hampers the assessment of its strengths and weaknesses and of a study’s generalisability. The Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) initiative developed recommendations on what should be included in an accurate and complete report of an observational study. We defined the scope of the recommendations to cover three main study designs: cohort, case-control, and cross-sectional studies. We convened a 2-day workshop in September, 2004, with methodologists, researchers, and journal editors to draft a checklist of items. This list was subsequently revised during several meetings of the coordinating group and in e-mail discussions with the larger group of STROBE contributors, taking into account empirical evidence and methodological considerations. The workshop and the subsequent iterative process of consultation and revision resulted in a checklist of 22 items (the STROBE statement) that relate to the title, abstract, introduction, methods, results, and discussion sections of articles. 18 items are common to all three study designs and four are specifi c for cohort, case-control, or cross-sectional studies. A detailed explanation and elaboration document is published separately and is freely available on the websites of PLoS Medicine, Annals of Internal Medicine, and Epidemiology. We hope that the STROBE statement will contribute to improving the quality of reporting of observational studies."
  },
  {
    "objectID": "posts/statistics/2025/STROBE.html#abstract",
    "href": "posts/statistics/2025/STROBE.html#abstract",
    "title": "STROBE",
    "section": "",
    "text": "Much biomedical research is observational. The reporting of such research is often inadequate, which hampers the assessment of its strengths and weaknesses and of a study’s generalisability. The Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) initiative developed recommendations on what should be included in an accurate and complete report of an observational study. We defined the scope of the recommendations to cover three main study designs: cohort, case-control, and cross-sectional studies. We convened a 2-day workshop in September, 2004, with methodologists, researchers, and journal editors to draft a checklist of items. This list was subsequently revised during several meetings of the coordinating group and in e-mail discussions with the larger group of STROBE contributors, taking into account empirical evidence and methodological considerations. The workshop and the subsequent iterative process of consultation and revision resulted in a checklist of 22 items (the STROBE statement) that relate to the title, abstract, introduction, methods, results, and discussion sections of articles. 18 items are common to all three study designs and four are specifi c for cohort, case-control, or cross-sectional studies. A detailed explanation and elaboration document is published separately and is freely available on the websites of PLoS Medicine, Annals of Internal Medicine, and Epidemiology. We hope that the STROBE statement will contribute to improving the quality of reporting of observational studies."
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html",
    "href": "posts/statistics/2025/サンプルサイズ設計.html",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "",
    "text": "本書は、頻繁に利用されるサンプルサイズ設計SASプログラムをまとめます。実務において、本記事のSASプログラムをコピーして利用することのみを念頭に置いています。数学的な導出は省略します。"
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#値アウトカム",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#値アウトカム",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "3.1 2値アウトカム",
    "text": "3.1 2値アウトカム\n単群試験で2値アウトカムの場合、サンプルサイズは、P0（閾値奏効割合）、P1（期待奏効割合）、α（第一種の過誤確率）、β（第二種の過誤確率）を事前に仮定することで計算できます。閾値奏効割合とは、これ以上の奏効割合がなければ治療効果がない（開発中止をしたい）と考えられる割合のことです。期待奏効割合とは、治療効果があると考えられる割合（次相に進みたい程度）のことです。これらの割合は臨床仮説や先行研究に基づいて決定するため、試験統計家がコメントすることはあまりありません。ICHガイドラインに準じて、SASプログラムは全て両側検定として両側有意水準5%としています。これは、なお、ICH E9では以下のような記載があります。すなわち、GCP準拠の治験においては原則として以下の記載に準じて試験計画を考える必要があります。\n「規制上の観点から、本ガイドラインの施行に伴い、原則として片側仮説を検証する場合は2.5％、両側仮説の場合は5％とすることとした。」\nなお、ICH E9にはQ＆AがありQ2で有意水準について言及されている。\n\n\n\n\n\n\nノート\n\n\n\n片側検定又は両側検定のどちらを用いるか、またそこでの有意水準をいくらにすべき かを、優越性試験と非劣性試験のそれぞれで説明願いたい。\n(答) ガイドラインでは、同等性を示す場合には両側信頼区間、非劣性試験では片側信頼 区間による解析を行うことが記載されているが、一般には推測を片側と考えるか両側 と考えるかには議論があり一概に決められるものではないとされている。また、有意 水準についても、個々の試験において適切な基準を設定すべきである旨の記載がある。 しかしながら、推論を片側とするか両側とするかにより統計的な判断に大きな差異 が生じることは規制上の観点から望ましくない。また、一方で、臨床試験における有 効性の評価では、検定により有意差があるか否かを判断するだけでなく、試験治療効 果の大きさ(比較群間の差の大きさ)がどの程度であるかを推定することも重要である。 そこで、今後は、検証的試験においては、仮説の検定においてどちらの方法を用いる 場合であっても、効果の推定には95％信頼係数の両側信頼区間を用い、検定の際の有水準は、これによる判断との整合性を図るため、優越性試験、非劣性試験のいずれにおいても、片側2.5％又は両側5％とすることを原則とする。用量反応試験についても、用量反応性を示すことにより薬剤の有効性を検証するような試験においては上記と同様である。ただし、適切な説明ができるのであれば、より強固な有効性の根拠を 示すために有意水準を厳しくする、稀少疾病用医薬品にみられる例のように十分な被 験者を集めることが困難な場合は有意水準を緩くする、などの措置をとってもよい。 なお、生物学的同等性試験については、「後発医薬品の生物学的同等性試験ガイドライン(平成9年12月22日医薬審第487号)」により、90％信頼係数の両側信頼区間を用いるとされているが、臨床効果を指標に標準製剤との同等性を検証しようとする場合(臨床的同等性試験)は、上記と同様に95％信頼係数の両側信頼区間を用いることを原則とする。\n\n\n　特に重要なのは、「適切な説明ができるのであれば、より強固な有効性の根拠を示すために有意水準を厳しくする、稀少疾病用医薬品にみられる例のように十分な被験者を集めることが困難な場合は有意水準を緩くする、などの措置をとってもよい。」の記載である。すなわち、両側10%や片側5%も、きちんと説明をして納得していただければ許容されるということである。これらの【5%か2.5%か】の議論については、以下のJCOGプロトコールマニュアルにも記載がある。\n\n\n\n\n\n\nJCOGプロトコール\n\n\n\n国立がん研究センターでの第2相相当の臨床試験では、片側検定を用いることもありえます。これは、標準治療 vs 試験治療であり、少なくとも試験治療が標準治療より劣っているとは考えておらず、片側検定の方が適切と考えられるからです。 JCOGのプロトコールマニュアルのURLは以下です。 JCOGプロトコールマニュアル\n\n\n試験計画時の例：\n\np0 = 0.1（閾値奏効割合）\np1 = 0.3（期待奏効割合）\nα = 0.05（第一種の過誤、片側）\nβ = 0.2（第二種の過誤）\n\n以下は、正規近似を用いたサンプルサイズ計算の例です。 連続修正は行いません。正規近似に基づく症例数設計では、H0におけるp0を用いて分散が計算されている点に注意してください。数理統計学を学ばれた方からみるとスコア検定に基づく方法に相当します。Wald検定に基づく例数設計とするとp1を用いて分散が計算されることになります。中間解析を勉強するとWald検定に基づく方法でサンプルサイズ設計がされることもあると気付きます。中間解析では検定統計量間の相関を考えるため、漸近正規性を仮定したサンプルサイズ設計をするためWald型の統計量を利用します。\n　実務上では、抗がん剤の単群試験では中間解析を事前に規定して、2項分布に基づいてに基づいて無効中止や有効中止を考慮したくなる場面が多いです。それは患者さんにとっての倫理性や開発戦略の観点からも重要な要素です。中間解析を行う場合、無効中止や有効中止の基準を事前に規定する必要があります。無効中止や有効中止の基準は、試験の目的やデザインに応じて異なるため、試験統計家と相談して決定することが重要です。無効中止や有効中止の基準を事前に規定することで、試験の透明性が向上し、倫理的な問題を回避することができます。そのような場合は、単純な2値データの割合のサンプルサイズ設計では実施できず、適切な試験統計家がSimonの2段階デザインやFlemingの2段階デザインを用いて例数設計がなされます。また、単群試験において、ベイズ流の例数設計も提案されていることから、ベイズ流例数設計も考慮していただきたい。なお、中間解析やベイズ流例数設計は常に必要ではなく、SASでの実装はシンプルではないため、ここでは割愛します。別記事で説明します。\n通常の単群試験の例数設計は以下のように行います。以下のSASプログラムは、対照群のリスクp0 = 0.1、実験群のリスクp1 = 0.3、リスク差 = 0.2を想定しています。なお、sides = 2は両側検定を意味します。片側検定を行う場合は、sides = 1としてください。\nproc power;\nonesamplefreq test=z\n    method = normal\n    sides = 2\n    alpha = 0.05\n    nullproportion = 0.1\n    proportion = 0.3\n    ntotal = .\n    power = 0.8;\nrun;\n対象者数が多い場合（シミュレーションによる確認要です）、上記の正規分布に基づく方法でも良いと考えます。しかし、一般的に2項分布による正確な検定に基づく症例数設計を利用することが標準的な方法です。理由は正規近似による例数設計が二項分布の中心極限定理を用いているため、あくまで対象者数が十分多い状況における中心極限定理を仮定しているためです。中心極限定理を利用せず、2項分布を用いて症例数設計をするSASプログラムは以下です。\n先ほどと異なり、test = exactを指定しています。これにより、2項分布に基づく正確な検定が行われます。これにより、サンプルサイズ計算がより正確になります。また、plotオプションを使用して、サンプルサイズに対する検出力の変化を視覚化しています。これにより、必要なサンプルサイズをより直感的に理解できます。\nproc power;\n    onesamplefreq test=exact\n        sides = 2\n        alpha = 0.05\n        nullproportion = 0.1\n        proportion = 0.3\n        ntotal = 2 to 100\n        power = .;\nrun;\nExact法に基づく症例数設計では、検出力は単調増加にならず、test = normalと異なり、N = xxのように数値が出力されません。そのため、SASの出力を結果を見ながら、目標とする検出力を満たす最小の整数値 or 目標とする検出力を満たす最大の整数値とします。個人的には、より保守的であるため後者を推奨します。実際、より大きなサンプルサイズを選ぶことで、検出力の目標値をより確実に達成できる可能性が高まります。ただし、リソースの制約がある場合は、目標検出力を満たす最小の症例数を選ぶことも実用的な選択肢となり得ます。基本的にtest = exactとすることを推奨します。"
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#検定仮説について片側検定と両側検定有意水準優越性非劣性",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#検定仮説について片側検定と両側検定有意水準優越性非劣性",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "3.2 検定仮説について：片側検定と両側検定、有意水準、優越性、非劣性",
    "text": "3.2 検定仮説について：片側検定と両側検定、有意水準、優越性、非劣性\n上のプログラムでは、よく利用されるであろう両側検定（sides=2 ）を指定して、両側有意水準をalpha=0.05を指定しています。この説ではここら辺のシナリオについて、もう少し深堀したいと思います。\nsides optionについて、SAS helpでは以下のように説明されています。\n\n\n\n\n\n\nノート\n\n\n\n\nSIDES=keyword-list\n\nspecifies the number of sides (or tails) and the direction of the statistical test. For information about specifying the keyword-list, see the section Specifying Value Lists in Analysis Statements. You can specify the following keywords:\n\n1\n\nspecifies a one-sided test, with the alternative hypothesis in the same direction as the effect.\n\n2\n\nspecifies a two-sided test.\n\nU\n\nspecifies an upper one-sided test, with the alternative hypothesis indicating a proportion greater than the null value.\n\nL\n\nspecifies a lower one-sided test, with the alternative hypothesis indicating a proportion less than the null value.\n\n\nIf the effect size is zero, then SIDES=1 is not permitted; instead, specify the direction of the test explicitly in this case with either SIDES=L or SIDES=U. By default, SIDES=2.\n\n\n\n\n\n3.2.1 パラメータの正しい役割\nSASのproc powerにおけるパラメータは、以下のように解釈するのが正解です。\n\nnullproportion (または nullp): 帰無仮説 (H₀)で「真実ではないか」と疑う基準値です。これを p₀ とします。今回の例では p₀ = 0.1 です。\nproportion (または p): 対立仮説 (H₁) が正しいとした場合に「実際にはこの値だろう」と想定する具体的な値です。この値を使って検出力（Power）を計算します。これを p_alt とします。今回の例では p_alt = 0.3 です。\n\n重要なのは、仮説に登場する p は「未知の真の比率」という記号であり、proportionパラメータの値 (0.3) そのものではない、という点です。\n\n\n3.2.2 仮説の正しい立て方\n上記の役割を踏まえると、今回の例 (nullpropotion=0.1, p=0.3) における仮説は以下のようになります。\n\n帰無仮説 (H₀): 真の比率は 0.1 に等しい。\n\nH_0 : p=0.1\n\n対立仮説 (H₁):\n\n片側（上側）検定 (SIDES=U) の場合\n\n真の比率は 0.1 より大きい。\nH_1 : p&gt;0.1\n\n片側（下側）検定 (SIDES=L) の場合\n\n真の比率は 0.1 より小さい。\nH_1 : p&lt;0.1\n\n\n\n二項検定の検出力は、「もし対立仮説が真実だったとしたら、どれくらいの確率で正しく帰無仮説を棄却できるか」を計算しています。\n計算は、大きく分けて2つのステップで行われます。\n\n\n3.2.3 ## 計算の具体的なステップ\nここでは、前の例に出てきた値を使って具体的に説明します。\n\n帰無仮説 (H₀): p=0.1\n対立仮説 (H₁) で想定する真の値: palt​=0.3\nサンプルサイズ (n): 100\n有意水準 (α): 0.05\n検定の種類: 片側上側検定 (H1​:p&gt;0.1)\n\n\n3.2.3.1 ステップ1：棄却域（帰無仮説を棄却する領域）を決める\nまず、「もし帰無仮説 (p=0.1) が正しいとしたら」という世界で考えます。このとき、100人中何人以上の成功が観察されたら、「これは偶然とは考えにくい、帰無仮説は間違いだろう」と判断するか、その境界線（臨界値）を決めます。これは、その値以上になる確率が有意水準α (5%) を下回るように設定します。n=100, p=0.1 の二項分布に従うとすると、成功数が k 回以上になる確率 P(X≥k) が 0.05 以下になる最小の k を探します。\n計算すると、k=16 となります。\n\nP(X≥15∣p=0.1)≈0.06 (まだ5%より大きい)\nP(X≥16∣p=0.1)≈0.03 (5%を下回った！)\n\nつまり、「もし真の比率が0.1なら、成功数が16回以上になることは滅多にない(確率3%)。だから、もし16回以上の成功を観測したら、帰無仮説を棄却しよう」とルールを決めます。この「16回以上」が棄却域です。\n\n\n3.2.3.2 ステップ2：棄却域に入る確率（検出力）を計算する\n次に、「もし対立仮説 (p=0.3) が真実だったとしたら」という世界に切り替えます。この世界で、ステップ1で決めた棄却域（成功数が16回以上）に入る確率を計算します。これこそが検出力です。n=100, p=0.3 の二項分布に従うとして、成功数が16回以上になる確率 P(X≥16∣p=0.3) を計算します。この確率を計算すると、およそ 0.998、つまり 99.8% となります。これが検出力です。つまり、「もし真の比率が0.3であれば、サンプルサイズ100の実験で『p&gt;0.1である』と正しく結論付けられる確率は99.8%である」ということを意味します。\n臨床試験において、片側検定というと一般的にSIDES=Uを指すことが一般的である。帰無仮説のp0より大きいかどうかを検証したいためです。抗がん剤の第2相試験はまさにそれが頻繁に利用される。Simonの2段階デザインやFlemmigデザインの数理を勉強すると理解が深まるだろう。そのため、実務上で片側検定と言われた場合はSIDES=1と指定すると、指定した値に応じて、SASが指定した治療効果の方向に基づいて、対立仮説が自動的に決定される。今回の例 (nullpropotion=0.1, p=0.3) でいえば、0.1&lt;0.3であるため、SASが自動で、 H_1 : null proportion = 0.1 &lt; pの仮説を指定してくれている。結果的に、SIDES=Uを指定することと同じになる。一方、例 (nullpropotion=0.1, p=0.05) でいえば、0.1&lt;0.05であるため、SASが自動でH_1 : p &lt; 0.1$\nの仮説を指定してくれている。結果的に、SIDES=Lを指定することと同じになる。実務上は、基本的にUpperを考えることが多いし、pの対立仮説の方が多いため実務上の問題はない。ただし、統計家として、SIDES=1 で適当に指定するより、正しく理解してSIDES=U を指定することを標準にすべきである。この姿勢は特に、非劣性試験/同等性試験の検定問題を考えるときに正しく指定できる力となる。\n\n\n\n3.2.4 非劣性試験におけるmargin option\nSASのproc powerのonesamplefreqにはmargin optionがある。SAS helpでは以下のように記載されている。\n\n\n\n\n\n\nノート\n\n\n\nMARGIN=number-list\nspecifies the equivalence or noninferiority or superiority margin, depending on the analysis.\nThe MARGIN= option can be used with one-sided analyses (SIDES = 1 | U | L), in which case it specifies the margin added to the null proportion value in the hypothesis test, resulting in a noninferiority or superiority test (depending on the agreement between the effect and hypothesis directions and the sign of the margin). A test with a null proportion and a margin m is the same as a test with null proportion and no margin.\nThe MARGIN= option can also be used with equivalence analyses (TEST=EQUIV_ADJZ | EQUIV_EXACT | EQUIV_Z) when the NULLPROPORTION= option is used, in which case it specifies the lower and upper equivalence bounds as and , where is the value of the NULLPROPORTION= option and m is the value of the MARGIN= option.\nThe MARGIN= option cannot be used in conjunction with the SIDES=2 option. (Instead, specify an equivalence analysis by using TEST=EQUIV_ADJZ or TEST=EQUIV_EXACT or TEST=EQUIV_Z). Also, the MARGIN= option cannot be used with the CI= option.\nValues must be strictly between –1 and 1. In addition, the sum of NULLPROPORTION and MARGIN must be strictly between 0 and 1 for one-sided analyses, and the derived lower equivalence bound (2 * NULLPROPORTION – MARGIN) must be strictly between 0 and 1 for equivalence analyses.\nFor information about specifying the number-list, see the section Specifying Value Lists in Analysis Statements."
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#はじめに",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#はじめに",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "4.1 はじめに",
    "text": "4.1 はじめに\n臨床試験の世界では、従来の2群比較試験（RCT）が金本位とされていますが、希少疾患の治療薬開発や倫理的制約がある状況では、単群試験での評価が必要となることがあります。本記事では、歴史的対照を用いた単群非劣性試験の統計設計について、SAS PROC POWERを用いた具体的な実装方法を解説します。"
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#単群非劣性試験とは",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#単群非劣性試験とは",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "4.2 単群非劣性試験とは",
    "text": "4.2 単群非劣性試験とは\n\n4.2.1 基本概念\n単群非劣性試験は、新治療を単一群で評価し、歴史的対照（過去の研究や文献値）と比較して「劣らない」ことを示す統計手法です。これは以下のような状況で用いられます：\n\n希少疾患: 患者数の制約により対照群の設定が困難\n倫理的制約: プラセボ群や無治療群の設定が非倫理的\n歴史的対照が確立: 標準治療の効果が文献で明確\n探索的研究: Phase II試験での preliminary evaluation"
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#統計的仮説設定",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#統計的仮説設定",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "4.3 統計的仮説設定",
    "text": "4.3 統計的仮説設定\n\n4.3.1 抽象的な仮説設定\n単群非劣性試験の一般的な仮説は以下のように設定されます：\n\n4.3.1.1 パラメータ定義\n\np: 新治療の真の反応率\np₀: 歴史的対照の反応率\nδ: 非劣性マージン（δ &gt; 0）\n\n\n\n4.3.1.2 仮説設定\n帰無仮説（H₀）: p ≤ p₀ - δ\n対立仮説（H₁）: p &gt; p₀ - δ\n\n\n\n4.3.2 仮説の臨床的解釈\n\nH₀: 新治療は歴史的対照より非劣性マージン以上劣る（臨床的に許容できない）\nH₁: 新治療は歴史的対照に対し非劣性である（臨床的に許容可能）"
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#実臨床での事例希少血液疾患治療薬の開発",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#実臨床での事例希少血液疾患治療薬の開発",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "4.4 実臨床での事例：希少血液疾患治療薬の開発",
    "text": "4.4 実臨床での事例：希少血液疾患治療薬の開発\n\n4.4.1 研究背景\nある希少な血液疾患の新薬開発を想定しましょう：\n\n対象疾患: 希少な血液疾患\n評価項目: 6ヶ月後の完全寛解率\n歴史的対照: 既存標準治療での完全寛解率 10%\n新薬の期待効果: 15%の完全寛解率\n非劣性マージン: 2%（臨床的に許容できる効果の低下）\n\n\n\n4.4.2 具体的な仮説設定\nこの設定では：\n\np₀ = 0.10（歴史的対照）\nδ = 0.02（非劣性マージン）\n実効的境界値 = 0.10 - 0.02 = 0.08\n\n帰無仮説: p ≤ 0.08 対立仮説: p &gt; 0.08"
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#sas-proc-powerによる実装",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#sas-proc-powerによる実装",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "4.5 SAS PROC POWERによる実装",
    "text": "4.5 SAS PROC POWERによる実装\n\n4.5.1 基本的な検出力計算\n/* 単群非劣性試験の検出力計算 */\nproc power;\n   onesamplefreq test=exact\n      sides = U                   /* 上側片側検定 */\n      nullproportion = 0.1        /* 歴史的対照の効果 */\n      proportion = 0.15           /* 新薬の想定効果 */\n      margin = -0.02              /* 非劣性マージン（負値で指定） */\n      ntotal = 130                /* 計画症例数 */\n      power = .;                  /* 検出力を計算 */\n   title \"単群非劣性試験：検出力計算\";\nrun;\n\n\n4.5.2 SASパラメータの詳細説明\n\n\n\nパラメータ\n説明\n値\n\n\n\n\ntest=exact\n正確検定（小サンプル・低事象率向け）\n-\n\n\nsides=U\n上側片側検定\n-\n\n\nproportion\n新治療の想定反応率（p）\n0.15\n\n\nnullproportion\n歴史的対照の反応率（p₀）\n0.10\n\n\nmargin\n非劣性マージン（-δ、負値で指定）\n-0.02\n\n\nntotal\n計画症例数\n130\n\n\n\n\n\n4.5.3 なぜmarginは負の値なのか\nSASでは以下の計算を行います：\n実効的帰無仮説の境界 = nullproportion + margin = 0.10 + (-0.02) = 0.08\nこれにより、「p &gt; 0.08」という対立仮説を検定することになります。"
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#同等性検定について",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#同等性検定について",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "4.6 同等性検定について",
    "text": "4.6 同等性検定について\n他にもTest Optionにて様々な検定仮説を選択することができる。\n中々利用しないが、TOSTという二重片側検定を用いた同等性試験もありえる。TOST（Two One-Sided Tests）を用いた同等性試験は、主に後発品（ジェネリック医薬品）の生物学的同等性試験で使用される統計手法です。機会があれば、紹介します。\n\n\n\n\n\n\nノート\n\n\n\nTEST= ADJZ | EQUIV_ADJZ | EQUIV_EXACT | EQUIV_Z | EXACT | Z\nTEST\n: specifies the statistical analysis. TEST=ADJZ specifies a normal-approximate z test with continuity adjustment. TEST=EQUIV_ADJZ specifies a normal-approximate two-sided equivalence test based on the z statistic with continuity adjustment and a TOST (two one-sided tests) procedure. TEST=EQUIV_EXACT specifies the exact binomial two-sided equivalence test based on a TOST (two one-sided tests) procedure. TEST=EQUIV_Z specifies a normal-approximate two-sided equivalence test based on the z statistic without any continuity adjustment, which is the same as the chi-square statistic, and a TOST (two one-sided tests) procedure. TEST or TEST=EXACT (the default) specifies the exact binomial test. TEST=Z specifies a normal-approximate z test without any continuity adjustment, which is the same as the chi-square test when SIDES=2."
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#信頼区間の基づくサンプルサイズ設計",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#信頼区間の基づくサンプルサイズ設計",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "4.7 信頼区間の基づくサンプルサイズ設計",
    "text": "4.7 信頼区間の基づくサンプルサイズ設計\nSASでは、信頼区間に基づくサンプルサイズ設計も完備されている。The following statements performs a confidence interval precision analysis for the Wilson score-based confidence interval for a binomial proportion. The default value of the ALPHA= option specifies a confidence level of 0.95.\nproc power;\n   onesamplefreq ci=wilson\n      halfwidth = 0.1\n      proportion = 0.3\n      ntotal = 70\n      probwidth = .;\nrun;\n\n4.7.1 参考文献\n\nOneSampleFreq Statement"
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#連続量アウトカム",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#連続量アウトカム",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "5.1 連続量アウトカム",
    "text": "5.1 連続量アウトカム\n連続量アウトカムの場合、サンプルサイズは以下の式で計算できます。 以下では優越性を示す検証的試験を想定しています。 twosamplemeans test = diffを用いて、2群間の平均値の差を検定する方法を想定しています。以下では、対照群の平均値μ0 = 0、実験群の平均値μ1 = 0.5、標準偏差σ = 1を想定しています。\nproc power;\n    twosamplemeans test=diff\n        meandiff = 0.5\n        stddev = 1\n        alpha = 0.05\n        power = 0.8\n        nptotal = .;\nrun;\n複数の状況をまとめて計算する場合は、以下のようにします。\nproc power;\n    twosamplemeans test=diff\n        meandiff = 0.5 , 1 , 1.5\n        stddev = 1 , 2 , 3\n        alpha = 0.05\n        power = 0.8\n        ntotal = .;\nrun;\n　平均値の差の検定は一般的であり、連続量のアウトカムを用いた試験では最も多く利用される方法です。平均値の差の検定は、通常、正規分布に従うと仮定される連続量データに適用されます。連続量アウトカムにおける経時測定データのサンプルサイズ設計が求められることもあるかと思います。その場合、最終観察時点における平均値の差を検定する方法として上記のプログラムを利用することが保守的で望ましいと考えます。例数設計としてはt検定に基づきより保守的なサンプルサイズ設計をする。統計解析においては、共分散分析やMMRMなどの同じEstimandを推定する方法で、より検出力の高い解析方法を適用することが治験の分野では求められると考えます。\n\n5.1.1 参考文献\nTwoSampleMeans Statement"
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#値アウトカム-1",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#値アウトカム-1",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "5.2 2値アウトカム",
    "text": "5.2 2値アウトカム\n2値アウトカムの場合、サンプルサイズは以下の式で計算できます。リスク差（割合の差）に基づいて優越性を検討することを想定しています。p0を対照群のリスク、p1を実験群のリスクとします。リスク差は、対照群のリスクp0に基づいて計算されるため、対照群のp0を指定する必要があります。以下では、対照群のリスクp0 = 0.1、実験群のリスクp1 = 0.3、リスク差 = 0.2を想定しています。\n\np0 = 0.1（対照群の和依頼）\np1 = 0.3（実験群の割合）\nα = 0.05（第一種の過誤、両側）\nβ = 0.2（第二種の過誤）\n\nSASプログラムは以下となります。ここでは、対照群のリスク及びリスク差を指定している。リスク差は、対照群のp0に基づいて計算されるため、対照群のp0を指定する必要があります。\nproc power;\ntwosamplefreq test=pchi\n    sides = 2\n    alpha = 0.05\n    refproportion = 0.1\n    proportiondiff = 0.2\n    npergroup = .\n    power = 0.8;\nrun;\n\n5.2.1 参考文献\n\nTWOSAMPLEFREQ Statement"
  },
  {
    "objectID": "posts/statistics/2025/サンプルサイズ設計.html#生存時間アウトカム",
    "href": "posts/statistics/2025/サンプルサイズ設計.html#生存時間アウトカム",
    "title": "臨床試験のサンプルサイズ設計",
    "section": "5.3 生存時間アウトカム",
    "text": "5.3 生存時間アウトカム\n生存時間アウトカムの場合、サンプルサイズは以下の式で計算できます。以下では優越性を示す検証的試験を想定しています。アウトカムはハザード比として、生存時間解析のノンパラメトリック検定（ログランク検定等）を前提とした例数設計を想定する。例えば生存時間アウトカムとして生存割合（死亡、生存）を前提として例数設計を考える場合、本節に基づく方法での例数設計も考えられるが、より保守的に2値データに基づく例数設計を行うことが望ましい場合もありえます。この違いについては、別記事で説明します。生存時間の例数設計については、SAS user総会のSASプロシジャを用いた生存時間データに対する例数設計の変革が参考になります。\nSASのtwosamplesurvival statementではLakatosの方法が採用されています。これは区分指数分布を前提としており解析的にN = xxの形で表すことはできません。教育的な立場だと、解析的にN = xxの形で表すことができる方法としてFreedmanの方法やSchoenfeldの方法がありますが、上記のSASユーザー総会の資料によると、Lakatosの方法の方が性能が良いことが報告されているそうです。実務上もproc power procedureを用いて、再現可能な状態で例数設計を行うことが望ましいので、基本的にLakatosの方法を採用します。\n生存時間アウトカムに基づく症例数設計では以下のような情報が必要です。特徴的なのは、登録期間とフォローアップ期間の情報が必要な点です。登録期間は、患者さんを試験に登録するための期間であり、フォローアップ期間は、患者さんを追跡するための期間です。これらの情報は、試験のデザインや目的に応じて異なるため、試験統計家と相談して決定することが重要です。 なお、通常登録は一様に登録されると仮定され、一様分布を仮定することがデフォルトです。\n\nS_c(3) = 0.3（対照群Cの3年あたりの生存率）\nS_a(2) = 0.2（実験群Aの3年あたりの生存率）\n登録期間：A = 3年\nフォローアップ期間：F = 2年\nα = 0.05（第一種の過誤、両側）\nβ = 0.2（第二種の過誤）\n\nSASでは以下のように実行します。ここでは、対照群の生存率S_c(3) = 0.3、実験群の生存率S_a(2) = 0.2を想定しています。登録期間は3年、フォローアップ期間は2年としています。これにより、試験のデザインや目的に応じたサンプルサイズを計算することができます。\nproc power;\n    twosamplesurvival test = logrank\n    curve(\"Control\") = 3:0.3\n    curve(\"Experimental\") = 3:0.2\n    groupsurvival = \"Control\" | \"Experimental\"\n    groupweights = (1 1)\n    accrualtime = 3 \n    followuptime = 2\n    ntotal = .\n    power = 0.8\n    alpha = 0.05\n    sides = 2;\nrun;\n通常、生存時間の例数設計で3年間での生存割合20%や、30%ということを仮定できる状況は多くないと思います。例えば第2相試験では1年間の追跡しかしていない場合に3年生存割合を仮定することは難しいです。そこで、通常は生存時間Tが指数分布に従うとして以下の式を用いてハザードを基にサンプルサイズ設計は有用です。ただし、ハザード比は生存時間の分布に依存するため、注意が必要です。生存時間Tが指数分布に従う場合、ハザード比は以下のように表されます。\n\nS(t) = 1-F(t) = exp(-λt) ⇔ λ = -\\frac{log(S(t))}{t}\n\nこの関係を用いて、ある時点tとその時点のKM推定量の結果を用いて各群のハザードを指定してサンプルサイズ計算をする方法は以下となります。\nproc power;\n    twosamplesurvival test = logrank\n    groupsurvexphazards = (0.05 0.01)\n    groupweights = (1 1)\n    accrualtime = 3 \n    followuptime = 2\n    ntotal = .\n    power = 0.8\n    alpha = 0.05\n    sides = 2;\nrun;\n他にも、中央生存時間を用いて計算することもできる。これも生存時間アウトカムに指数分布を仮定すると簡単に示せます。\n\nS(1/2) = exp(-λt) ⇔ λ = -\\frac{log(S(1/2))}{t}\n\nこの関係を用いて、KM推定量の結果を用いて各群の中央生存時間からハザードを推定することもできる。いずれの方法もTに指数分布を仮定している。\n\n5.3.1 参考文献\n\nTwoSsampleSurvival Statement"
  },
  {
    "objectID": "posts/statistics/2025/医学英語論文_1.html#書かなければ何も残らない",
    "href": "posts/statistics/2025/医学英語論文_1.html#書かなければ何も残らない",
    "title": "医学論文を書くための心構え",
    "section": "2 書かなければ何も残らない。",
    "text": "2 書かなければ何も残らない。"
  },
  {
    "objectID": "posts/statistics/2025/医学英語論文_1.html#英語で論文を書こう",
    "href": "posts/statistics/2025/医学英語論文_1.html#英語で論文を書こう",
    "title": "医学論文を書くための心構え",
    "section": "3 英語で論文を書こう",
    "text": "3 英語で論文を書こう"
  },
  {
    "objectID": "posts/statistics/2025/医学英語論文_1.html#書くことを意識して読む",
    "href": "posts/statistics/2025/医学英語論文_1.html#書くことを意識して読む",
    "title": "医学論文を書くための心構え",
    "section": "4 「書く」ことを意識して「読む」",
    "text": "4 「書く」ことを意識して「読む」\n論文を書くには、様々な準備が必要がある。その第一歩が、常日頃から英語論文を読むことである。「忙しいから論文を読む暇がない」は禁句である。まともに論文を書ける医師は決してそんなセリフを吐かない。1日15分でもいいから論文を読む習慣は必要である。\n　論文を読むことは、ランニングと同じである。はじめは走ることが苦手でも、毎日走れば、そのうち苦になくなる。ただし、漠然と英語論文を読み続けても、英語論文を書けるようにはならない。AbstractとTableとFigureをConclusionだけ読めば論文の大筋はつかめる。しかし、そのような「ななめ読み」を続けては、臨床研究能力はいつまでたっても身に付かないし、英語論文を書くこともおぼつかない。\n　自ら筆を執り英語論文を書くことを強く意識して、日ごろから論文を精読することが推奨される。10本の論文のabstractを読み飛ばすよりも、1本の論文を隅から隅まで精読する方が、臨床研究能力、論文執筆力向上にとって有用である。\nIMRAD(Intoroduction,Methods,Results and Discussion）の順番に沿って、論理展開を追いながら読むことが、英語力・論理力だけでなく論文執筆力への近道である。\n自分の専門分野における主要なjournalは毎号チェックしよう。さらに、pubmedで検索し、良い論文を厳選して読むようにしよう。ただし、検索そのものに時間をかけないことが肝心である。膨大な論文を検索しそれを整理することに時間と労力を費やし、検索した論文の精読を怠っている研究者が少なくない。限られた時間はなるべく論文を読むことに費やすべきである。\n英語論文を精読する際、知らない単語を調べる必要が生じたら、英英辞典を使うことを推奨する。\n日本人が医学英語論文を書くための効率的な方法として、自分の専門領域の論文に頻出する表現のリストを作成することをお勧めする。常日頃から英語論文を読んで、よく使われている表現、使える構文やパラグラフをリストアップしておこう。論文の各節（Introduction, Methods, Results , Discussion , Conclusion ,Acknowledgement）ごとに頻出表現をまとめておくとよい。\nこれによって、自分が使える英語表現のストックを増やしていくのである。自分で英語論文を書くとき、自作の頻出表現リストから、文中にふさわしい表現を当てはめて書くようにしよう"
  },
  {
    "objectID": "posts/statistics/2025/医学英語論文_1.html#いつ論文を書き始めるか",
    "href": "posts/statistics/2025/医学英語論文_1.html#いつ論文を書き始めるか",
    "title": "医学論文を書くための心構え",
    "section": "5 いつ論文を書き始めるか？",
    "text": "5 いつ論文を書き始めるか？"
  },
  {
    "objectID": "posts/statistics/2025/医学英語論文_1.html#論文執筆時間の短縮",
    "href": "posts/statistics/2025/医学英語論文_1.html#論文執筆時間の短縮",
    "title": "医学論文を書くための心構え",
    "section": "6 論文執筆時間の短縮",
    "text": "6 論文執筆時間の短縮"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html",
    "href": "posts/statistics/2025/相対Path.html",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": "相対パスは、現在いる場所を基準にしたファイルやフォルダの位置を表す方法です。\n\n\n\n. = 現在のディレクトリ\n.. = 1つ上のディレクトリ\n../../ = 2つ上のディレクトリ\n../../../ = 3つ上のディレクトリ\n\n\n\n\n\nProject/                    ← プロジェクトルート\n├── 01_Data/\n│   ├── 00_RAW/\n│   │   └── EXT/\n│   └── 01_PROCESSED/\n├── 02_Programs/\n│   ├── ADaM/\n│   │   └── v2.0/           ← 現在ここにいる\n│   └── TLF/\n└── 03_Output/\n\n\n\n*------------------------------------------------------------------------------;\n* Get Current Directory Path ;\n*------------------------------------------------------------------------------;\nfilename *Dir* \".\" ;\n%let PgmType = %scan ( %sysfunc ( pathname ( *DIR* ) ) , -1 , %str ( \\ ) ) ;\n%put  &PgmType. ;\nfilename *Dir* \"../\" ;\n%let PgmEnv  = %scan ( %sysfunc ( pathname ( *DIR* ) ) , -1 , %str ( \\ ) ) ;\n%put  &PgmEnv. ;\nfilename *Dir* \"../../\" ;\n%let Dir     = %sysfunc ( pathname ( *DIR* ) ) ;\n%put  &Dir. ;\nfilename *Dir* clear ;\n\n\n1行目: 現在のディレクトリ（“.”）にファイル参照を設定\nfilename *Dir* \".\" ;\n2行目: 現在のディレクトリ名を取得\n%let PgmType = %scan ( %sysfunc ( pathname ( *DIR* ) ) , -1 , %str ( \\ ) ) ;\n\n%sysfunc(pathname(*DIR*)) → フルパスを取得\n%scan(..., -1, %str(\\)) → パスを\\で区切って最後の部分を取得\n結果: v2.0 （現在のフォルダ名）\n\n4行目: 1つ上のディレクトリに参照を変更\nfilename *Dir* \"../\" ;\n5行目: 1つ上のディレクトリ名を取得\n%let PgmEnv  = %scan ( %sysfunc ( pathname ( *DIR* ) ) , -1 , %str ( \\ ) ) ;\n\n結果: ADaM （1つ上のフォルダ名）\n\n7行目: 2つ上のディレクトリに参照を変更\nfilename *Dir* \"../../\" ;\n8行目: 2つ上のディレクトリのフルパスを取得\n%let Dir     = %sysfunc ( pathname ( *DIR* ) ) ;\n\n結果: C:\\Project\\02_Programs （2つ上のフルパス）\n\n10行目: ファイル参照をクリア\nfilename *Dir* clear ;\n\n\n\nPgmType: v2.0\nPgmEnv: ADaM  \nDir: C:\\Project\\02_Programs\n\n\n\n\n\n\n/* 現在地: C:\\Project\\02_Programs\\ADaM\\v2.0\\ */\n\n/* プロジェクトルートに移動 */\nX \"cd ../../../\" ;\n\n/* または、絶対パスで移動 */\n%let PROJECT_ROOT = C:\\Project ;\nX \"cd &PROJECT_ROOT.\" ;\n\n\n\n/* 現在のプログラムパスを取得 */\n%let execpath = %sysfunc(getoption(sysin)) ;\n%let CURRENT = %qsubstr(\"&execpath.\", 2, %eval(%index(\"&execpath.\", %scan(&execpath, -1, \"\\\")))-2) ;\n\n/* プロジェクトルートを算出 */\nfilename *Dir* \"../../../\" ;\n%let PROJECT_ROOT = %sysfunc(pathname(*DIR*)) ;\nfilename *Dir* clear ;\n\n/* プロジェクトルートに移動 */\nX \"cd &PROJECT_ROOT.\" ;\n%put 現在地をプロジェクトルートに変更: &PROJECT_ROOT. ;\n\n/* これで相対パスがシンプルになる */\n%let RAW_DATA = 01_Data\\00_RAW ;\n%let PROCESSED_DATA = 01_Data\\01_PROCESSED ;\n%let OUTPUT_DIR = 03_Output ;\n\n\n\n/* X commandでプロジェクトルートに移動後 */\nX \"cd &PROJECT_ROOT.\" ;\n\n/* 相対パスがシンプルに！ */\n%let external = 01_Data\\00_RAW\\EXT\\external_data.xlsx ;\n%include \"00_Settings\\init.sas\" ;\nods pdf file=\"03_Output\\ADSL_Summary.pdf\" ;\n\n/* 移動前だと... */\n%let external = ..\\..\\..\\01_Data\\00_RAW\\EXT\\external_data.xlsx ;\n%include \"..\\..\\..\\00_Settings\\init.sas\" ;\nods pdf file=\"..\\..\\..\\03_Output\\ADSL_Summary.pdf\" ;\n\n\n\n\n# 現在のディレクトリ情報を取得\nget_directory_info &lt;- function() {\n  current_path &lt;- getwd()\n  \n  # 現在のディレクトリ名\n  PgmType &lt;- basename(current_path)\n  \n  # 1つ上のディレクトリ名\n  parent_path &lt;- dirname(current_path)\n  PgmEnv &lt;- basename(parent_path)\n  \n  # 2つ上のディレクトリのフルパス\n  Dir &lt;- dirname(parent_path)\n  \n  # 結果を表示\n  cat(\"PgmType:\", PgmType, \"\\n\")\n  cat(\"PgmEnv:\", PgmEnv, \"\\n\") \n  cat(\"Dir:\", Dir, \"\\n\")\n  \n  # リストで返す\n  list(\n    PgmType = PgmType,\n    PgmEnv = PgmEnv,\n    Dir = Dir\n  )\n}\n\n# 実行\ndir_info &lt;- get_directory_info()\n\n# プロジェクトルートに移動\nPROJECT_ROOT &lt;- dirname(dirname(dirname(getwd())))\nsetwd(PROJECT_ROOT)\n\n# 相対パスがシンプルに\nexternal_file &lt;- \"01_Data/00_RAW/EXT/external_data.xlsx\"\nsource(\"00_Settings/init.R\")\n\n\n\n現在地：02_Programs/ADaM/v2.0/ にいる場合\n\n\n\n.. → 02_Programs/ADaM/ に移動\n../ → 02_Programs/ に移動\n01_Data/ → 01_Data/ フォルダに入る\n00_RAW/EXT/ → さらに奥のフォルダに入る\n\n結果: Project/01_Data/00_RAW/EXT/\n\n\n\n\n\n\n/* プログラム冒頭でプロジェクトルートに移動 */\nfilename *Dir* \"../../../\" ;\n%let PROJECT_ROOT = %sysfunc(pathname(*DIR*)) ;\nfilename *Dir* clear ;\nX \"cd &PROJECT_ROOT.\" ;\n\n/* 以降はシンプルな相対パス */\n%include \"00_Settings\\init.sas\" ;\n%let external = 01_Data\\00_RAW\\EXT\\external_data.xlsx ;\n\n\n\n/* 現在地を変更せず、長い相対パスを使用 */\n%include \"..\\..\\..\\00_Settings\\init.sas\" ;\n%let external = ..\\..\\..\\01_Data\\00_RAW\\EXT\\external_data.xlsx ;\n\n\n\nプロジェクトルート移動（推奨）\n\n✅ パスがシンプル\n✅ 可読性が良い\n✅ 他のプログラムでも同じパターン使用可能\n❌ 作業ディレクトリが変わる\n\n長い相対パス\n\n✅ 作業ディレクトリが変わらない\n❌ パスが長くて読みにくい\n❌ メンテナンスが大変\n\n\n\n\n\n\n\n/* 現在地を間違えている */\n%let external = ../01_Data/00_RAW/EXT/file.xlsx ;  /* ..が足りない */\n\n/* フォルダ名を間違えている */\n%let external = ../../Data/RAW/EXT/file.xlsx ;     /* 01_が抜けている */\n\n/* X command後にパスが混乱 */\nX \"cd &PROJECT_ROOT.\" ;\n%let external = ../../01_Data/00_RAW/EXT/file.xlsx ; /* もう..は不要 */\n\n\n\n/* 段階的に確認 */\nfilename test1 \"..\" ;\n%put %sysfunc(pathname(test1)) ;  /* 1つ上を確認 */\n\nfilename test2 \"../..\" ;\n%put %sysfunc(pathname(test2)) ;  /* 2つ上を確認 */\n\nfilename test3 \"../../01_Data\" ;\n%put %sysfunc(pathname(test3)) ;  /* 目的フォルダを確認 */\n\n\n\n\n\n\n/* SAS */\nfilename chkpath \"../../01_Data\" ;\n%put パス確認: %sysfunc(pathname(chkpath)) ;\nfilename chkpath clear ;\n# R\nfile.exists(\"../../01_Data\")  # TRUE/FALSEで確認\nnormalizePath(\"../../01_Data\")  # 絶対パス表示\n\n\n\n/* SAS - プロジェクトルート移動後 */\nX \"cd &PROJECT_ROOT.\" ;\n%let DATA_ROOT = 01_Data ;\n%let RAW_DATA = &DATA_ROOT.\\00_RAW ;\n%let EXT_DATA = &RAW_DATA.\\EXT ;\n%let external = &EXT_DATA.\\target_file.xlsx ;\n# R\nsetwd(PROJECT_ROOT)\nDATA_ROOT &lt;- \"01_Data\"\nRAW_DATA &lt;- file.path(DATA_ROOT, \"00_RAW\")\nEXT_DATA &lt;- file.path(RAW_DATA, \"EXT\")\nexternal_file &lt;- file.path(EXT_DATA, \"target_file.xlsx\")\n\n\n\n/* SAS */\n%put 現在地: %sysfunc(getoption(work)) ;\nfilename pwd \".\" ;\n%put 現在地: %sysfunc(pathname(pwd)) ;\nfilename pwd clear ;\n# R\ngetwd()  # 現在の作業ディレクトリ\n\n\n\n\n\n.. の数 = 上に戻る階層数\nX commandでプロジェクトルートに移動すると相対パスがシンプルになる\nフォルダ構造を把握してから相対パスを設計\nパス確認を習慣化する\n変数化で管理しやすくする\nSASのfilenameとRのgetwd()系関数で現在地を把握\n\n相対パスとX commandをマスターすれば、プロジェクト全体の移動や共有が格段に楽になります！"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html#相対パスとは",
    "href": "posts/statistics/2025/相対Path.html#相対パスとは",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": "相対パスは、現在いる場所を基準にしたファイルやフォルダの位置を表す方法です。\n\n\n\n. = 現在のディレクトリ\n.. = 1つ上のディレクトリ\n../../ = 2つ上のディレクトリ\n../../../ = 3つ上のディレクトリ"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html#フォルダ構造の例",
    "href": "posts/statistics/2025/相対Path.html#フォルダ構造の例",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": "Project/                    ← プロジェクトルート\n├── 01_Data/\n│   ├── 00_RAW/\n│   │   └── EXT/\n│   └── 01_PROCESSED/\n├── 02_Programs/\n│   ├── ADaM/\n│   │   └── v2.0/           ← 現在ここにいる\n│   └── TLF/\n└── 03_Output/"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html#現在のディレクトリ情報を取得するsasコード",
    "href": "posts/statistics/2025/相対Path.html#現在のディレクトリ情報を取得するsasコード",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": "*------------------------------------------------------------------------------;\n* Get Current Directory Path ;\n*------------------------------------------------------------------------------;\nfilename *Dir* \".\" ;\n%let PgmType = %scan ( %sysfunc ( pathname ( *DIR* ) ) , -1 , %str ( \\ ) ) ;\n%put  &PgmType. ;\nfilename *Dir* \"../\" ;\n%let PgmEnv  = %scan ( %sysfunc ( pathname ( *DIR* ) ) , -1 , %str ( \\ ) ) ;\n%put  &PgmEnv. ;\nfilename *Dir* \"../../\" ;\n%let Dir     = %sysfunc ( pathname ( *DIR* ) ) ;\n%put  &Dir. ;\nfilename *Dir* clear ;\n\n\n1行目: 現在のディレクトリ（“.”）にファイル参照を設定\nfilename *Dir* \".\" ;\n2行目: 現在のディレクトリ名を取得\n%let PgmType = %scan ( %sysfunc ( pathname ( *DIR* ) ) , -1 , %str ( \\ ) ) ;\n\n%sysfunc(pathname(*DIR*)) → フルパスを取得\n%scan(..., -1, %str(\\)) → パスを\\で区切って最後の部分を取得\n結果: v2.0 （現在のフォルダ名）\n\n4行目: 1つ上のディレクトリに参照を変更\nfilename *Dir* \"../\" ;\n5行目: 1つ上のディレクトリ名を取得\n%let PgmEnv  = %scan ( %sysfunc ( pathname ( *DIR* ) ) , -1 , %str ( \\ ) ) ;\n\n結果: ADaM （1つ上のフォルダ名）\n\n7行目: 2つ上のディレクトリに参照を変更\nfilename *Dir* \"../../\" ;\n8行目: 2つ上のディレクトリのフルパスを取得\n%let Dir     = %sysfunc ( pathname ( *DIR* ) ) ;\n\n結果: C:\\Project\\02_Programs （2つ上のフルパス）\n\n10行目: ファイル参照をクリア\nfilename *Dir* clear ;\n\n\n\nPgmType: v2.0\nPgmEnv: ADaM  \nDir: C:\\Project\\02_Programs"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html#x-commandでディレクトリ移動",
    "href": "posts/statistics/2025/相対Path.html#x-commandでディレクトリ移動",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": "/* 現在地: C:\\Project\\02_Programs\\ADaM\\v2.0\\ */\n\n/* プロジェクトルートに移動 */\nX \"cd ../../../\" ;\n\n/* または、絶対パスで移動 */\n%let PROJECT_ROOT = C:\\Project ;\nX \"cd &PROJECT_ROOT.\" ;\n\n\n\n/* 現在のプログラムパスを取得 */\n%let execpath = %sysfunc(getoption(sysin)) ;\n%let CURRENT = %qsubstr(\"&execpath.\", 2, %eval(%index(\"&execpath.\", %scan(&execpath, -1, \"\\\")))-2) ;\n\n/* プロジェクトルートを算出 */\nfilename *Dir* \"../../../\" ;\n%let PROJECT_ROOT = %sysfunc(pathname(*DIR*)) ;\nfilename *Dir* clear ;\n\n/* プロジェクトルートに移動 */\nX \"cd &PROJECT_ROOT.\" ;\n%put 現在地をプロジェクトルートに変更: &PROJECT_ROOT. ;\n\n/* これで相対パスがシンプルになる */\n%let RAW_DATA = 01_Data\\00_RAW ;\n%let PROCESSED_DATA = 01_Data\\01_PROCESSED ;\n%let OUTPUT_DIR = 03_Output ;\n\n\n\n/* X commandでプロジェクトルートに移動後 */\nX \"cd &PROJECT_ROOT.\" ;\n\n/* 相対パスがシンプルに！ */\n%let external = 01_Data\\00_RAW\\EXT\\external_data.xlsx ;\n%include \"00_Settings\\init.sas\" ;\nods pdf file=\"03_Output\\ADSL_Summary.pdf\" ;\n\n/* 移動前だと... */\n%let external = ..\\..\\..\\01_Data\\00_RAW\\EXT\\external_data.xlsx ;\n%include \"..\\..\\..\\00_Settings\\init.sas\" ;\nods pdf file=\"..\\..\\..\\03_Output\\ADSL_Summary.pdf\" ;"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html#r版での同等処理",
    "href": "posts/statistics/2025/相対Path.html#r版での同等処理",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": "# 現在のディレクトリ情報を取得\nget_directory_info &lt;- function() {\n  current_path &lt;- getwd()\n  \n  # 現在のディレクトリ名\n  PgmType &lt;- basename(current_path)\n  \n  # 1つ上のディレクトリ名\n  parent_path &lt;- dirname(current_path)\n  PgmEnv &lt;- basename(parent_path)\n  \n  # 2つ上のディレクトリのフルパス\n  Dir &lt;- dirname(parent_path)\n  \n  # 結果を表示\n  cat(\"PgmType:\", PgmType, \"\\n\")\n  cat(\"PgmEnv:\", PgmEnv, \"\\n\") \n  cat(\"Dir:\", Dir, \"\\n\")\n  \n  # リストで返す\n  list(\n    PgmType = PgmType,\n    PgmEnv = PgmEnv,\n    Dir = Dir\n  )\n}\n\n# 実行\ndir_info &lt;- get_directory_info()\n\n# プロジェクトルートに移動\nPROJECT_ROOT &lt;- dirname(dirname(dirname(getwd())))\nsetwd(PROJECT_ROOT)\n\n# 相対パスがシンプルに\nexternal_file &lt;- \"01_Data/00_RAW/EXT/external_data.xlsx\"\nsource(\"00_Settings/init.R\")"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html#相対パスの読み方",
    "href": "posts/statistics/2025/相対Path.html#相対パスの読み方",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": "現在地：02_Programs/ADaM/v2.0/ にいる場合\n\n\n\n.. → 02_Programs/ADaM/ に移動\n../ → 02_Programs/ に移動\n01_Data/ → 01_Data/ フォルダに入る\n00_RAW/EXT/ → さらに奥のフォルダに入る\n\n結果: Project/01_Data/00_RAW/EXT/"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html#実践的な使用パターン",
    "href": "posts/statistics/2025/相対Path.html#実践的な使用パターン",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": "/* プログラム冒頭でプロジェクトルートに移動 */\nfilename *Dir* \"../../../\" ;\n%let PROJECT_ROOT = %sysfunc(pathname(*DIR*)) ;\nfilename *Dir* clear ;\nX \"cd &PROJECT_ROOT.\" ;\n\n/* 以降はシンプルな相対パス */\n%include \"00_Settings\\init.sas\" ;\n%let external = 01_Data\\00_RAW\\EXT\\external_data.xlsx ;\n\n\n\n/* 現在地を変更せず、長い相対パスを使用 */\n%include \"..\\..\\..\\00_Settings\\init.sas\" ;\n%let external = ..\\..\\..\\01_Data\\00_RAW\\EXT\\external_data.xlsx ;\n\n\n\nプロジェクトルート移動（推奨）\n\n✅ パスがシンプル\n✅ 可読性が良い\n✅ 他のプログラムでも同じパターン使用可能\n❌ 作業ディレクトリが変わる\n\n長い相対パス\n\n✅ 作業ディレクトリが変わらない\n❌ パスが長くて読みにくい\n❌ メンテナンスが大変"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html#よくある失敗パターン",
    "href": "posts/statistics/2025/相対Path.html#よくある失敗パターン",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": "/* 現在地を間違えている */\n%let external = ../01_Data/00_RAW/EXT/file.xlsx ;  /* ..が足りない */\n\n/* フォルダ名を間違えている */\n%let external = ../../Data/RAW/EXT/file.xlsx ;     /* 01_が抜けている */\n\n/* X command後にパスが混乱 */\nX \"cd &PROJECT_ROOT.\" ;\n%let external = ../../01_Data/00_RAW/EXT/file.xlsx ; /* もう..は不要 */\n\n\n\n/* 段階的に確認 */\nfilename test1 \"..\" ;\n%put %sysfunc(pathname(test1)) ;  /* 1つ上を確認 */\n\nfilename test2 \"../..\" ;\n%put %sysfunc(pathname(test2)) ;  /* 2つ上を確認 */\n\nfilename test3 \"../../01_Data\" ;\n%put %sysfunc(pathname(test3)) ;  /* 目的フォルダを確認 */"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html#実践的なコツ",
    "href": "posts/statistics/2025/相対Path.html#実践的なコツ",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": "/* SAS */\nfilename chkpath \"../../01_Data\" ;\n%put パス確認: %sysfunc(pathname(chkpath)) ;\nfilename chkpath clear ;\n# R\nfile.exists(\"../../01_Data\")  # TRUE/FALSEで確認\nnormalizePath(\"../../01_Data\")  # 絶対パス表示\n\n\n\n/* SAS - プロジェクトルート移動後 */\nX \"cd &PROJECT_ROOT.\" ;\n%let DATA_ROOT = 01_Data ;\n%let RAW_DATA = &DATA_ROOT.\\00_RAW ;\n%let EXT_DATA = &RAW_DATA.\\EXT ;\n%let external = &EXT_DATA.\\target_file.xlsx ;\n# R\nsetwd(PROJECT_ROOT)\nDATA_ROOT &lt;- \"01_Data\"\nRAW_DATA &lt;- file.path(DATA_ROOT, \"00_RAW\")\nEXT_DATA &lt;- file.path(RAW_DATA, \"EXT\")\nexternal_file &lt;- file.path(EXT_DATA, \"target_file.xlsx\")\n\n\n\n/* SAS */\n%put 現在地: %sysfunc(getoption(work)) ;\nfilename pwd \".\" ;\n%put 現在地: %sysfunc(pathname(pwd)) ;\nfilename pwd clear ;\n# R\ngetwd()  # 現在の作業ディレクトリ"
  },
  {
    "objectID": "posts/statistics/2025/相対Path.html#まとめ",
    "href": "posts/statistics/2025/相対Path.html#まとめ",
    "title": "SAS・Rでの相対パス ../../ の使い方完全ガイド",
    "section": "",
    "text": ".. の数 = 上に戻る階層数\nX commandでプロジェクトルートに移動すると相対パスがシンプルになる\nフォルダ構造を把握してから相対パスを設計\nパス確認を習慣化する\n変数化で管理しやすくする\nSASのfilenameとRのgetwd()系関数で現在地を把握\n\n相対パスとX commandをマスターすれば、プロジェクト全体の移動や共有が格段に楽になります！"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#ベクトル",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#ベクトル",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "0.2 ベクトル",
    "text": "0.2 ベクトル\n実数を一列に並べたものをv= (v_1,..,v_d)^Tをベクトルとよぶ。d次元空間R^d上の原点から伸びる矢印だとイメージするとわかりやすいでしょう。ベクトルは向きと大きさの情報を持っていて、大きさは矢印の長さ、向きに対応しています。"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#ベクトルの内積と相関係数",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#ベクトルの内積と相関係数",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "0.3 ベクトルの内積と相関係数",
    "text": "0.3 ベクトルの内積と相関係数\nyとxの偏差ベクトルとCosθの関係を説明する。"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#内積とcos類似度",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#内積とcos類似度",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "0.4 内積とcos類似度",
    "text": "0.4 内積とcos類似度\n類似度の起点を0とするのか、相関係数のように平均値とするのかの違い。"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#行列の掛け算の解釈",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#行列の掛け算の解釈",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "0.5 行列の掛け算の解釈",
    "text": "0.5 行列の掛け算の解釈\n\n行列とベクトルの積：列ベクトルとその重みという解釈\n\n行列の掛け算は、左側の行列を列ベクトルのまとまり、右側のベクトル/行列を列ベクトルの重みとして線形和を計算したものとして解釈する。\n\nベクトルの内積をまとめたものとしての解釈\n\n行列の掛け算は、左側の行列を行ベクトルのまとまり、右側の行列を列ベクトルのまとまりとして、それぞれの内積を計算したものだと考えることができる\n\n連立一次方程式の係数としての行列\n\n一次連立方程式の係数行列をA、解きたい変数をベクトルxとすれば、一次連立方程式はAx=bとしてあらわすことができる。"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#共分散行列",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#共分散行列",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "1.1 共分散行列",
    "text": "1.1 共分散行列\nS=\\frac{1}{n} S^TS"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#多変数関数の微分から求める線形回帰分析",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#多変数関数の微分から求める線形回帰分析",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "1.2 多変数関数の微分から求める線形回帰分析",
    "text": "1.2 多変数関数の微分から求める線形回帰分析\n残差平方和（Y-X\\beta）の二乗和を考えて、これを最小にするようなβを求める方法を考えている。\n残差平方和は||y-X\\beta||^2にに対応します。この大きさの二乗は自分自身の内積に対応するので、以下のように変形できます。\n\n||y-X\\beta||^2 = (y-X\\beta)^T(y-X\\beta) = (y^T-\\beta^TX^T)(y-X\\beta)=y^Ty-\\beta^TX^Ty - y^TX\\beta+\\beta^TX^TX\\beta\n\nここで、\n\\beta^TX^Ty = (X\\beta)^Ty=y^TX\\beta\nと表すことできるので、内積に対応するので、これらの値は等しいと分かる。（スカラーである）\n\n||y-X\\beta||^2 = (y-X\\beta)^T(y-X\\beta) = (y^T-\\beta^TX^T)(y-X\\beta)=y^Ty- 2y^TX\\beta+\\beta^TX^TX\\beta\n\n残差平方和はβの一次関数と二次形式でできています。\n微分の公式は以下の通りです。\n\\frac{\\partial a^Tx}{\\partial x}=a\n\\frac{\\partial x^T Ax}{\\partial x}=2Ax\nよって、残差平方和を微分すると、以下の式となる。\n\n-2X^Ty + 2X^TX\\beta =0\n\nこれより、正規方程式が得られて、X^TXが逆行列を持てば、最小二乗推定量が得られる。"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#主成分分析と固有値分解",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#主成分分析と固有値分解",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "1.3 主成分分析と固有値分解",
    "text": "1.3 主成分分析と固有値分解\n主成分分析は||w||^2の主成分の係数ベクトルに||w||^2 = 1の制約をつけたなかで、主成分の分散s_z^2=\\frac{w^TX^TXw}{n}が最も大きくなるように求めるものである。\n\n1.3.1 固有値\n正方行列Aに対して、次のようにベクトルvと数λの組を見つける問題を固有値問題といいます。\nAv=\\lambda v\nただし、ベクトルvは0でないものとします。このようなベクトルvを固有ベクトル、λを固有値といいます。\nこの固有方程式を式変形することで、(A-\\lambda I)v=0について解いたときに、v=0以外の解がみつかるようなλを見つけ、そのときの解vを求めたい。\nここで、右辺=0というのが大事である。ここで、A-λIが一次独立であれば、v=0しか解がなくなってしまう。v=0の解を見つけるためには、A-λIが線形従属、すなわちrank落ち、すなわち行列式=0出ないとダメとなります。そこから、以下を満たすλとベクトルvを見つけることを考えます。\n|A-\\lambda I|=0"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#ridge回帰分析",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#ridge回帰分析",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "1.4 Ridge回帰分析",
    "text": "1.4 Ridge回帰分析"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#勾配ベクトルとその応用",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#勾配ベクトルとその応用",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "1.5 勾配ベクトルとその応用",
    "text": "1.5 勾配ベクトルとその応用"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#対角化",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#対角化",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "1.6 対角化",
    "text": "1.6 対角化\n正方行列Aに対して、以下のような等式を正則行列Vと対角行列Λがみつかるとします。\nA=V\\Lambda V^{-1}\nこれを行列Aの対角化といいます。対角化は行列のべき乗を計算するときに便利で、マルコフ連鎖を計算するときに便利です。"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#行列のtrace",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#行列のtrace",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "1.7 行列のtrace",
    "text": "1.7 行列のtrace\n\ntr(A+B)=tr(A)+tr(B)\ntr(A^T)=tr(A)\ntr(AB)=tr(BA)"
  },
  {
    "objectID": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#特異値分解",
    "href": "posts/statistics/2025/統計のための線形代数_微分積分_2023.html#特異値分解",
    "title": "統計のための線形代数・微分積分まとめ1",
    "section": "1.8 特異値分解",
    "text": "1.8 特異値分解\n特異値分解とは、サイズ（m,n）の行列Xをm次直交行列U、n次直交行列V、サイズ(m,n)で特に非対角成分が0の行列Σによって以下のように分解することを呼ぶ。\nX=U\\Sigma V^T"
  },
  {
    "objectID": "posts/statistics/2025/統計方法論研究におけるフォルダ構成.html",
    "href": "posts/statistics/2025/統計方法論研究におけるフォルダ構成.html",
    "title": "方法論研究におけるフォルダ構成を考える",
    "section": "",
    "text": "実データ解析に関するフォルダ構成は、別記事で作成済である。実務と同様の構成で実施したらよい。ここでは、実データを利用することを主眼とせず、特定の統計方法論に基づくシミュレーション研究/方法論研究のフォルダ構造について考える。そして、Rにて自動でフォルダを作成するような標準テンプレートを作成することを目的とする。以下はGeminiに作ってもらったもの。\n\n\nこんにちは！研究者の皆さん、日々のシミュレーション研究、お疲れ様です。\nシミュレーション研究って、実データ解析とはまた違った難しさがありますよね。特に、様々なパラメータ設定、シナリオごとの結果管理、そしていつどのスクリプトを実行したかといった記録の管理は、研究の再現性や効率に直結する重要な課題です。\n「あの時の結果、どのファイルだっけ…？」 「このスクリプト、どのシミュレーション設定で走らせたんだっけ…？」\nこんな経験、ありませんか？\n今回は、そんな悩みを解決するための「シミュレーション研究に特化した理想のフォルダ構成」と、それをRで自動生成する便利なテンプレートをご紹介します。Git管理や複雑なドキュメント、さらにはRの初期設定ファイルも一旦置いておいて、まずは研究に集中できるミニマルで効率的な環境をサッと作っちゃいましょう！\n\n\nシミュレーション研究では、特に以下の点が管理のポイントになります。\n\n再現性: どのスクリプトが、いつ、どの設定で実行され、どんな結果が出たのかが明確であること。\n効率性: 目的のファイルに素早くアクセスでき、新しいシミュレーションを追加しやすいこと。\n整理整頓: 膨大になりがちな中間ファイルや結果を体系的に保存できること。\n\nこれらを解決するために、以下のようなフォルダ構成をおすすめします。\n\n\n\n私たちが提案するフォルダ構成は、研究のワークフローを考慮し、以下のように整理されています。特に、日々の進捗を明確にするために日付(YYYYMMDD)でのフォルダ管理を取り入れています。フォルダ名の先頭に数字を付けることで、常に意図した順序で表示されるよう工夫しています。\nプロジェクトルート/\n├── 000_resources/     # 勉強した教材、参考文献、参考資料など\n├── 001_data/\n│   └── raw/           # シミュレーションの元となる外部データや真のパラメータ設定\n├── 002_R/\n│   ├── functions/     # シミュレーションの中核となるR関数群\n│   └── utils/         # 汎用的なユーティリティR関数\n├── 003_output/        # シミュレーション結果の出力ルート\n│   └── YYYYMMDD/      # 例: 20250711 (その日のシミュレーション結果を格納)\n├── 004_src/           # シミュレーション実行スクリプトのルート\n│   └── YYYYMMDD/      # 例: 20250711 (その日に実行したスクリプトを格納)\n├── 005_scripts/       # シミュレーション設定やバッチ処理用スクリプト\n├── 006_docs/          # 研究ノート、アイデアなど（論文以外のドキュメント）\n├── 007_meeting/       # 指導教官との打ち合わせ記録\n│   └── YYYYMMDD/      # 例: 20250710 (その日の打ち合わせ資料・議事録など)\n├── 008_Paper/         # 論文ドラフトや最終版の図・表、参考文献ファイル\n├── renv/              # Rパッケージ管理（renvを使用する場合）\n\n\n\n\n000_resources/ の追加: 研究の基礎となる教材や参考文献を整理して格納できる場所を設けました。\n数字プレフィックス付きフォルダ: 001_data, 002_R のように数字を付与することで、ファイルエクスプローラーやRStudioのファイルペインで常に論理的な順序で表示されます。\nYYYYMMDD 形式のフォルダ: 003_output/、004_src/、007_meeting/ の直下には、その日の作業をまとめるための日付フォルダを作成します。これにより、「あの結果、いつ出したんだっけ？」と迷うことがなくなります。\n\n例： 004_src/20250711/ には今日実行したスクリプトを、003_output/20250711/ には今日得られた結果を保存します。同じ日に複数のシミュレーションを行う場合は、ファイル名で区別すると良いでしょう。\n\n008_Paper/ フォルダ: 論文執筆のためのフォルダ。ここにQuartoやLaTeXの原稿ファイル、論文用の最終的な図表などをまとめておけます。\nrenv/ で再現性確保: Rパッケージのバージョン管理ツール renv を使うことで、異なる環境でも同じ解析を再現できるようになります。renv を使用すると、プロジェクトルートに .Rprofile と renv.lock ファイルが自動生成されますが、これは renv の機能の一部であり、パッケージの再現性確保には不可欠です。\n\n\n\n\nそれぞれのフォルダには明確な役割があります。適切に使い分けることで、プロジェクト全体が整理され、将来の自分や共同研究者が困ることがなくなります。\n\n\n\n目的: シミュレーション研究を進める上で参照する全ての資料を格納する場所です。\n内容: 勉強した教材（PDF、講義ノートなど）、論文のPDF、ウェブサイトのスクリーンショット、コードスニペットのメモ、関連する書籍の目次や章のコピーなど。\n使い分け: プロジェクトの「知識ベース」として活用します。ここにある資料は、シミュレーションの実行や結果とは直接関係ありませんが、研究の背景知識や手法の理解を深めるために不可欠なものです。\n\n\n\n\n\n目的: シミュレーションの元となる固定データを格納する場所です。\n内容:\n\nraw/: 外部から取得した生データや、シミュレーションの「真の値」として設定するパラメータのファイルなど。シミュレーション内で動的に生成されるデータはここには置きません。\n\n使い分け: シミュレーションの入力となる、変更されない静的なデータを置きます。\n\n\n\n\n\n目的: Rの関数ファイルを格納する場所です。これにより、スクリプト本体がすっきりし、関数を再利用しやすくなります。\n内容:\n\nfunctions/: シミュレーションの中核となるカスタム関数を定義したファイル。例えば、特定の分布からデータを生成する関数、開発中の推定量を計算する関数、統計モデルをフィットする関数などです。これらの関数は、004_src内の実行スクリプトから呼び出して使います。\nutils/: シミュレーション結果の集計、プロット作成、結果の整形など、汎用的に利用するユーティリティ関数を定義したファイル。特定のシミュレーションに限定されず、複数の研究で使い回せるような関数が該当します。\n\n使い分け: 長いRスクリプトを機能ごとに分割し、再利用可能な部品として管理します。実行スクリプトはこれらの関数を「呼び出す」だけに留めます。\n\n\n\n\n\n目的: シミュレーションによって生成された結果を保存する場所です。\n内容:\n\nYYYYMMDD/: シミュレーションを実行した日付ごとのフォルダ。この中に、その日の実行で得られた結果を保存します。\n\nresults/: シミュレーションによって得られた数値結果（例: 推定量、標準誤差、p値、バイアスなど）を.rdsファイルや.csvファイルとして保存します。\nfigures/: 結果を可視化したグラフや図を.pngや.pdf形式で保存します。\ntables/: 結果をまとめた表を.texや.docx形式で保存します。\n\n\n使い分け: シミュレーションの結果として「残すべきもの」は全てここに集約します。日付フォルダにより、いつの結果かが一目でわかります。\n\n\n\n\n\n目的: シミュレーションの具体的な実行ロジックと、それに伴う結果解析のコードを格納する場所です。\n内容:\n\nYYYYMMDD/: シミュレーションを実行した日付ごとのフォルダ。この中に、その日の特定のシミュレーション実行に関するスクリプトを格納します。\n\n01_run_simulation.R: シミュレーションを実際に実行し、データを生成し、003_output/YYYYMMDD/results/ に保存するスクリプト。\n02_analyze_results.R: 003_output/YYYYMMDD/results/ に保存された結果を読み込み、解析し、003_output/YYYYMMDD/figures/ や 003_output/YYYYMMDD/tables/ に保存するスクリプト。\n03_generate_report.R: シミュレーション結果をまとめた簡易レポートやQuartoドキュメントを生成するスクリプト。\n\n\n使い分け: **「このシミュレーション結果を得るために、どのコードを走らせたか？」**という問いに直接答えるコードを置きます。日付フォルダにまとめることで、再現性が飛躍的に向上します。\n\n\n\n\n\n目的: プロジェクト全体をスムーズに進めるための補助的なスクリプトやユーティリティを置く場所です。\n内容:\n\nシミュレーション実行前に必要な環境設定スクリプト（例: パス設定、共通オプション）。\n複数のシミュレーションを自動で実行するためのバッチ処理スクリプト（例: HPC環境でのジョブ投入スクリプト）。\n001_data/raw にある生データを、シミュレーションで使いやすい形に加工する一度きりの前処理スクリプト。\n複数のシミュレーションシナリオで共通して使われるパラメータ設定を定義するスクリプトなど。\n\n使い分け: **「このプロジェクトを進める上で、どんなツールや設定が必要か？」**という問いに答えるコードを置きます。特定のシミュレーション実行とは独立しており、汎用性が高いコードが該当します。\n\n\n\n\n\n目的: 研究に関するドキュメントやアイデア、メモなどを格納する場所です。\n内容: 研究ノート、ブレインストーミングのメモ、学会発表の草案、今後の研究アイデア、関連論文のまとめなど。\n使い分け: 008_Paper/ が論文原稿そのものであるのに対し、こちらは論文に直接ならないような「研究の過程」を示す資料を置きます。\n\n\n\n\n\n目的: 指導教官や共同研究者との打ち合わせ記録を保存する場所です。\n内容:\n\nYYYYMMDD/: 打ち合わせがあった日付ごとのフォルダ。この中に、議事録、議論のポイント、ToDoリスト、共有資料などを保存します。\n\n使い分け: 打ち合わせの履歴を時系列で管理することで、以前の議論内容をすぐに確認できます。\n\n\n\n\n\n目的: 論文のドラフトや、論文に最終的に含める図表のファイル、参考文献ファイルなどを格納する場所です。\n内容: Quarto（.qmd）ファイル、LaTeX（.tex）ファイル、Word（.docx）ファイルなどの論文原稿、論文で使う高品質な図（.pdf, .tiff）、表（.tex, .csv）、参考文献データベース（.bib）など。\n使い分け: シミュレーション結果 (003_output/) から論文に直接持っていく最終的な成果物をここに集約します。\n\n\n\n\n\nこの理想的なフォルダ構成を、Rのスクリプトでサクッと自動生成しちゃいましょう！以下のコードをRコンソールに貼り付けて実行するだけです。プロジェクトを作成したいディレクトリに移動してから実行してください。\n#' シミュレーション研究プロジェクトのフォルダ構造を自動生成する関数\n#'\n#' @param project_name 作成するプロジェクトのルートフォルダ名\n#' @param use_renv renvパッケージを使用するかどうか (TRUE/FALSE)\n#'\n#' @return なし。指定されたパスにフォルダ構造を作成します。\n#' @export\ncreate_sim_project_structure_minimal &lt;- function(project_name, use_renv = TRUE) {\n  \n  # プロジェクトルートのパスを設定 (現在の作業ディレクトリに作成)\n  project_path &lt;- file.path(getwd(), project_name)\n\n  # ルートフォルダの作成\n  if (dir.exists(project_path)) {\n    stop(paste0(\"フォルダ '\", project_path, \"' は既に存在します。別の名前を指定するか、既存のフォルダを削除してください。\"))\n  }\n  dir.create(project_path, recursive = TRUE)\n  message(paste0(\"プロジェクトルートフォルダ '\", project_path, \"' を作成しました。\\n\"))\n  \n  # サブフォルダの定義\n  # src, output, meetingの内部は日付管理されるため、ここでは親フォルダのみ作成\n  sub_folders &lt;- c(\n    \"000_resources\", # 新規追加\n    \"001_data/raw\",\n    \"002_R/functions\",\n    \"002_R/utils\",\n    \"003_output\",\n    \"004_src\",\n    \"005_scripts\",\n    \"006_docs\",\n    \"007_meeting\",\n    \"008_Paper\"\n  )\n  \n  # サブフォルダの作成\n  for (folder in sub_folders) {\n    dir.create(file.path(project_path, folder), recursive = TRUE)\n    message(paste0(\"  - フォルダ '\", file.path(project_path, folder), \"' を作成しました。\\n\"))\n  }\n  \n  # renvの初期化\n  if (use_renv) {\n    message(\"renvの初期化を開始します。これには少し時間がかかる場合があります...\\n\")\n    # 作業ディレクトリを一時的にプロジェクトルートに変更してrenvを初期化\n    old_wd &lt;- getwd()\n    setwd(project_path) # ここでプロジェクトパスに移動\n    tryCatch({\n      if (!requireNamespace(\"renv\", quietly = TRUE)) {\n        install.packages(\"renv\")\n      }\n      renv::init()\n      message(\"  - renvを初期化しました。\\n\")\n      message(\"    プロジェクトルートに '.Rprofile' と 'renv.lock' が生成されます。\\n\")\n    }, error = function(e) {\n      message(paste0(\"  - renvの初期化中にエラーが発生しました: \", e$message, \"\\n\"))\n      message(\"    手動で `renv::init()` を実行してください。\\n\")\n    }, finally = {\n      setwd(old_wd) # 元の作業ディレクトリに戻す\n    })\n  }\n  \n  message(paste0(\"\\nプロジェクト '\", project_name, \"' のフォルダ構造が正常に作成されました。\"))\n  if (use_renv) {\n    message(paste0(\"RStudioで '\", project_name, \".Rproj' を開いた後、`renv::restore()` を実行して必要なパッケージをインストールしてください。\"))\n  } else {\n    message(paste0(\"RStudioで '\", project_name, \".Rproj' を開いて作業を開始してください。\"))\n  }\n  message(\"\\n**補足:**\")\n  message(\"  - `004_src/`, `003_output/`, `007_meeting/` フォルダ内は、ご自身で `YYYYMMDD/` 形式のサブフォルダを作成し、ファイルを整理してください。\")\n  message(\"  - `008_Paper/` フォルダ内には、論文ドラフトや関連ファイルを自由に配置してください。\")\n}\n使い方：\n\n上記Rコードをコピーし、RStudioのコンソールに貼り付けるか、新しい .R ファイルとして保存します。\nプロジェクトを作成したい場所に移動します（例: setwd(\"C:/Users/kotas/OneDrive/デスクトップ/Project\")）。\nコンソールで create_sim_project_structure_minimal(\"任意のプロジェクト名\", use_renv = FALSE) を呼び出します。\n\n\"任意のプロジェクト名\" は、新しく作るプロジェクトのルートフォルダ名です。\nuse_renv = TRUE にすると renv が自動的に初期化されます。もし renv も不要であれば、use_renv = FALSE に設定してください。\n\n\n\n\n\n\n学習・資料収集時:\n\n新しい統計手法の論文を読んだら、そのPDFを 000_resources/ に保存。\nRの特定のパッケージの使い方を学んだら、そのメモやコード例を 000_resources/ に保存。\n\nシミュレーション実行日:\n\n今日が2025年7月11日なら、004_src/20250711/ フォルダを作り、その中に実行スクリプト (01_run_sim_A.R など) を置きます。\n実行結果は 003_output/20250711/ フォルダ内に保存します（例: results/sim_A_summary.rds, figures/plot_A.png）。\n\n打ち合わせ日:\n\n2025年7月10日に指導教官と打ち合わせをしたら、007_meeting/20250710/ フォルダを作り、議事録や資料をそこに保存します。\n\n論文執筆:\n\n008_Paper/ フォルダには、論文のQuartoファイルやLaTeXファイル、そして論文に含める最終的な図表などを格納します。\n\n\nこの自動生成テンプレートを活用して、シミュレーション研究をより効率的かつ体系的に進め、素晴らしい成果に繋げていきましょう！"
  },
  {
    "objectID": "posts/statistics/2025/統計方法論研究におけるフォルダ構成.html#シミュレーション研究の強い味方rで自動生成する理想のフォルダ構成",
    "href": "posts/statistics/2025/統計方法論研究におけるフォルダ構成.html#シミュレーション研究の強い味方rで自動生成する理想のフォルダ構成",
    "title": "方法論研究におけるフォルダ構成を考える",
    "section": "",
    "text": "こんにちは！研究者の皆さん、日々のシミュレーション研究、お疲れ様です。\nシミュレーション研究って、実データ解析とはまた違った難しさがありますよね。特に、様々なパラメータ設定、シナリオごとの結果管理、そしていつどのスクリプトを実行したかといった記録の管理は、研究の再現性や効率に直結する重要な課題です。\n「あの時の結果、どのファイルだっけ…？」 「このスクリプト、どのシミュレーション設定で走らせたんだっけ…？」\nこんな経験、ありませんか？\n今回は、そんな悩みを解決するための「シミュレーション研究に特化した理想のフォルダ構成」と、それをRで自動生成する便利なテンプレートをご紹介します。Git管理や複雑なドキュメント、さらにはRの初期設定ファイルも一旦置いておいて、まずは研究に集中できるミニマルで効率的な環境をサッと作っちゃいましょう！\n\n\nシミュレーション研究では、特に以下の点が管理のポイントになります。\n\n再現性: どのスクリプトが、いつ、どの設定で実行され、どんな結果が出たのかが明確であること。\n効率性: 目的のファイルに素早くアクセスでき、新しいシミュレーションを追加しやすいこと。\n整理整頓: 膨大になりがちな中間ファイルや結果を体系的に保存できること。\n\nこれらを解決するために、以下のようなフォルダ構成をおすすめします。\n\n\n\n私たちが提案するフォルダ構成は、研究のワークフローを考慮し、以下のように整理されています。特に、日々の進捗を明確にするために日付(YYYYMMDD)でのフォルダ管理を取り入れています。フォルダ名の先頭に数字を付けることで、常に意図した順序で表示されるよう工夫しています。\nプロジェクトルート/\n├── 000_resources/     # 勉強した教材、参考文献、参考資料など\n├── 001_data/\n│   └── raw/           # シミュレーションの元となる外部データや真のパラメータ設定\n├── 002_R/\n│   ├── functions/     # シミュレーションの中核となるR関数群\n│   └── utils/         # 汎用的なユーティリティR関数\n├── 003_output/        # シミュレーション結果の出力ルート\n│   └── YYYYMMDD/      # 例: 20250711 (その日のシミュレーション結果を格納)\n├── 004_src/           # シミュレーション実行スクリプトのルート\n│   └── YYYYMMDD/      # 例: 20250711 (その日に実行したスクリプトを格納)\n├── 005_scripts/       # シミュレーション設定やバッチ処理用スクリプト\n├── 006_docs/          # 研究ノート、アイデアなど（論文以外のドキュメント）\n├── 007_meeting/       # 指導教官との打ち合わせ記録\n│   └── YYYYMMDD/      # 例: 20250710 (その日の打ち合わせ資料・議事録など)\n├── 008_Paper/         # 論文ドラフトや最終版の図・表、参考文献ファイル\n├── renv/              # Rパッケージ管理（renvを使用する場合）\n\n\n\n\n000_resources/ の追加: 研究の基礎となる教材や参考文献を整理して格納できる場所を設けました。\n数字プレフィックス付きフォルダ: 001_data, 002_R のように数字を付与することで、ファイルエクスプローラーやRStudioのファイルペインで常に論理的な順序で表示されます。\nYYYYMMDD 形式のフォルダ: 003_output/、004_src/、007_meeting/ の直下には、その日の作業をまとめるための日付フォルダを作成します。これにより、「あの結果、いつ出したんだっけ？」と迷うことがなくなります。\n\n例： 004_src/20250711/ には今日実行したスクリプトを、003_output/20250711/ には今日得られた結果を保存します。同じ日に複数のシミュレーションを行う場合は、ファイル名で区別すると良いでしょう。\n\n008_Paper/ フォルダ: 論文執筆のためのフォルダ。ここにQuartoやLaTeXの原稿ファイル、論文用の最終的な図表などをまとめておけます。\nrenv/ で再現性確保: Rパッケージのバージョン管理ツール renv を使うことで、異なる環境でも同じ解析を再現できるようになります。renv を使用すると、プロジェクトルートに .Rprofile と renv.lock ファイルが自動生成されますが、これは renv の機能の一部であり、パッケージの再現性確保には不可欠です。\n\n\n\n\nそれぞれのフォルダには明確な役割があります。適切に使い分けることで、プロジェクト全体が整理され、将来の自分や共同研究者が困ることがなくなります。\n\n\n\n目的: シミュレーション研究を進める上で参照する全ての資料を格納する場所です。\n内容: 勉強した教材（PDF、講義ノートなど）、論文のPDF、ウェブサイトのスクリーンショット、コードスニペットのメモ、関連する書籍の目次や章のコピーなど。\n使い分け: プロジェクトの「知識ベース」として活用します。ここにある資料は、シミュレーションの実行や結果とは直接関係ありませんが、研究の背景知識や手法の理解を深めるために不可欠なものです。\n\n\n\n\n\n目的: シミュレーションの元となる固定データを格納する場所です。\n内容:\n\nraw/: 外部から取得した生データや、シミュレーションの「真の値」として設定するパラメータのファイルなど。シミュレーション内で動的に生成されるデータはここには置きません。\n\n使い分け: シミュレーションの入力となる、変更されない静的なデータを置きます。\n\n\n\n\n\n目的: Rの関数ファイルを格納する場所です。これにより、スクリプト本体がすっきりし、関数を再利用しやすくなります。\n内容:\n\nfunctions/: シミュレーションの中核となるカスタム関数を定義したファイル。例えば、特定の分布からデータを生成する関数、開発中の推定量を計算する関数、統計モデルをフィットする関数などです。これらの関数は、004_src内の実行スクリプトから呼び出して使います。\nutils/: シミュレーション結果の集計、プロット作成、結果の整形など、汎用的に利用するユーティリティ関数を定義したファイル。特定のシミュレーションに限定されず、複数の研究で使い回せるような関数が該当します。\n\n使い分け: 長いRスクリプトを機能ごとに分割し、再利用可能な部品として管理します。実行スクリプトはこれらの関数を「呼び出す」だけに留めます。\n\n\n\n\n\n目的: シミュレーションによって生成された結果を保存する場所です。\n内容:\n\nYYYYMMDD/: シミュレーションを実行した日付ごとのフォルダ。この中に、その日の実行で得られた結果を保存します。\n\nresults/: シミュレーションによって得られた数値結果（例: 推定量、標準誤差、p値、バイアスなど）を.rdsファイルや.csvファイルとして保存します。\nfigures/: 結果を可視化したグラフや図を.pngや.pdf形式で保存します。\ntables/: 結果をまとめた表を.texや.docx形式で保存します。\n\n\n使い分け: シミュレーションの結果として「残すべきもの」は全てここに集約します。日付フォルダにより、いつの結果かが一目でわかります。\n\n\n\n\n\n目的: シミュレーションの具体的な実行ロジックと、それに伴う結果解析のコードを格納する場所です。\n内容:\n\nYYYYMMDD/: シミュレーションを実行した日付ごとのフォルダ。この中に、その日の特定のシミュレーション実行に関するスクリプトを格納します。\n\n01_run_simulation.R: シミュレーションを実際に実行し、データを生成し、003_output/YYYYMMDD/results/ に保存するスクリプト。\n02_analyze_results.R: 003_output/YYYYMMDD/results/ に保存された結果を読み込み、解析し、003_output/YYYYMMDD/figures/ や 003_output/YYYYMMDD/tables/ に保存するスクリプト。\n03_generate_report.R: シミュレーション結果をまとめた簡易レポートやQuartoドキュメントを生成するスクリプト。\n\n\n使い分け: **「このシミュレーション結果を得るために、どのコードを走らせたか？」**という問いに直接答えるコードを置きます。日付フォルダにまとめることで、再現性が飛躍的に向上します。\n\n\n\n\n\n目的: プロジェクト全体をスムーズに進めるための補助的なスクリプトやユーティリティを置く場所です。\n内容:\n\nシミュレーション実行前に必要な環境設定スクリプト（例: パス設定、共通オプション）。\n複数のシミュレーションを自動で実行するためのバッチ処理スクリプト（例: HPC環境でのジョブ投入スクリプト）。\n001_data/raw にある生データを、シミュレーションで使いやすい形に加工する一度きりの前処理スクリプト。\n複数のシミュレーションシナリオで共通して使われるパラメータ設定を定義するスクリプトなど。\n\n使い分け: **「このプロジェクトを進める上で、どんなツールや設定が必要か？」**という問いに答えるコードを置きます。特定のシミュレーション実行とは独立しており、汎用性が高いコードが該当します。\n\n\n\n\n\n目的: 研究に関するドキュメントやアイデア、メモなどを格納する場所です。\n内容: 研究ノート、ブレインストーミングのメモ、学会発表の草案、今後の研究アイデア、関連論文のまとめなど。\n使い分け: 008_Paper/ が論文原稿そのものであるのに対し、こちらは論文に直接ならないような「研究の過程」を示す資料を置きます。\n\n\n\n\n\n目的: 指導教官や共同研究者との打ち合わせ記録を保存する場所です。\n内容:\n\nYYYYMMDD/: 打ち合わせがあった日付ごとのフォルダ。この中に、議事録、議論のポイント、ToDoリスト、共有資料などを保存します。\n\n使い分け: 打ち合わせの履歴を時系列で管理することで、以前の議論内容をすぐに確認できます。\n\n\n\n\n\n目的: 論文のドラフトや、論文に最終的に含める図表のファイル、参考文献ファイルなどを格納する場所です。\n内容: Quarto（.qmd）ファイル、LaTeX（.tex）ファイル、Word（.docx）ファイルなどの論文原稿、論文で使う高品質な図（.pdf, .tiff）、表（.tex, .csv）、参考文献データベース（.bib）など。\n使い分け: シミュレーション結果 (003_output/) から論文に直接持っていく最終的な成果物をここに集約します。\n\n\n\n\n\nこの理想的なフォルダ構成を、Rのスクリプトでサクッと自動生成しちゃいましょう！以下のコードをRコンソールに貼り付けて実行するだけです。プロジェクトを作成したいディレクトリに移動してから実行してください。\n#' シミュレーション研究プロジェクトのフォルダ構造を自動生成する関数\n#'\n#' @param project_name 作成するプロジェクトのルートフォルダ名\n#' @param use_renv renvパッケージを使用するかどうか (TRUE/FALSE)\n#'\n#' @return なし。指定されたパスにフォルダ構造を作成します。\n#' @export\ncreate_sim_project_structure_minimal &lt;- function(project_name, use_renv = TRUE) {\n  \n  # プロジェクトルートのパスを設定 (現在の作業ディレクトリに作成)\n  project_path &lt;- file.path(getwd(), project_name)\n\n  # ルートフォルダの作成\n  if (dir.exists(project_path)) {\n    stop(paste0(\"フォルダ '\", project_path, \"' は既に存在します。別の名前を指定するか、既存のフォルダを削除してください。\"))\n  }\n  dir.create(project_path, recursive = TRUE)\n  message(paste0(\"プロジェクトルートフォルダ '\", project_path, \"' を作成しました。\\n\"))\n  \n  # サブフォルダの定義\n  # src, output, meetingの内部は日付管理されるため、ここでは親フォルダのみ作成\n  sub_folders &lt;- c(\n    \"000_resources\", # 新規追加\n    \"001_data/raw\",\n    \"002_R/functions\",\n    \"002_R/utils\",\n    \"003_output\",\n    \"004_src\",\n    \"005_scripts\",\n    \"006_docs\",\n    \"007_meeting\",\n    \"008_Paper\"\n  )\n  \n  # サブフォルダの作成\n  for (folder in sub_folders) {\n    dir.create(file.path(project_path, folder), recursive = TRUE)\n    message(paste0(\"  - フォルダ '\", file.path(project_path, folder), \"' を作成しました。\\n\"))\n  }\n  \n  # renvの初期化\n  if (use_renv) {\n    message(\"renvの初期化を開始します。これには少し時間がかかる場合があります...\\n\")\n    # 作業ディレクトリを一時的にプロジェクトルートに変更してrenvを初期化\n    old_wd &lt;- getwd()\n    setwd(project_path) # ここでプロジェクトパスに移動\n    tryCatch({\n      if (!requireNamespace(\"renv\", quietly = TRUE)) {\n        install.packages(\"renv\")\n      }\n      renv::init()\n      message(\"  - renvを初期化しました。\\n\")\n      message(\"    プロジェクトルートに '.Rprofile' と 'renv.lock' が生成されます。\\n\")\n    }, error = function(e) {\n      message(paste0(\"  - renvの初期化中にエラーが発生しました: \", e$message, \"\\n\"))\n      message(\"    手動で `renv::init()` を実行してください。\\n\")\n    }, finally = {\n      setwd(old_wd) # 元の作業ディレクトリに戻す\n    })\n  }\n  \n  message(paste0(\"\\nプロジェクト '\", project_name, \"' のフォルダ構造が正常に作成されました。\"))\n  if (use_renv) {\n    message(paste0(\"RStudioで '\", project_name, \".Rproj' を開いた後、`renv::restore()` を実行して必要なパッケージをインストールしてください。\"))\n  } else {\n    message(paste0(\"RStudioで '\", project_name, \".Rproj' を開いて作業を開始してください。\"))\n  }\n  message(\"\\n**補足:**\")\n  message(\"  - `004_src/`, `003_output/`, `007_meeting/` フォルダ内は、ご自身で `YYYYMMDD/` 形式のサブフォルダを作成し、ファイルを整理してください。\")\n  message(\"  - `008_Paper/` フォルダ内には、論文ドラフトや関連ファイルを自由に配置してください。\")\n}\n使い方：\n\n上記Rコードをコピーし、RStudioのコンソールに貼り付けるか、新しい .R ファイルとして保存します。\nプロジェクトを作成したい場所に移動します（例: setwd(\"C:/Users/kotas/OneDrive/デスクトップ/Project\")）。\nコンソールで create_sim_project_structure_minimal(\"任意のプロジェクト名\", use_renv = FALSE) を呼び出します。\n\n\"任意のプロジェクト名\" は、新しく作るプロジェクトのルートフォルダ名です。\nuse_renv = TRUE にすると renv が自動的に初期化されます。もし renv も不要であれば、use_renv = FALSE に設定してください。\n\n\n\n\n\n\n学習・資料収集時:\n\n新しい統計手法の論文を読んだら、そのPDFを 000_resources/ に保存。\nRの特定のパッケージの使い方を学んだら、そのメモやコード例を 000_resources/ に保存。\n\nシミュレーション実行日:\n\n今日が2025年7月11日なら、004_src/20250711/ フォルダを作り、その中に実行スクリプト (01_run_sim_A.R など) を置きます。\n実行結果は 003_output/20250711/ フォルダ内に保存します（例: results/sim_A_summary.rds, figures/plot_A.png）。\n\n打ち合わせ日:\n\n2025年7月10日に指導教官と打ち合わせをしたら、007_meeting/20250710/ フォルダを作り、議事録や資料をそこに保存します。\n\n論文執筆:\n\n008_Paper/ フォルダには、論文のQuartoファイルやLaTeXファイル、そして論文に含める最終的な図表などを格納します。\n\n\nこの自動生成テンプレートを活用して、シミュレーション研究をより効率的かつ体系的に進め、素晴らしい成果に繋げていきましょう！"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験支援業務を考える.html",
    "href": "posts/statistics/2025/臨床試験支援業務を考える.html",
    "title": "業務受託時のフォルダ構造",
    "section": "",
    "text": "現在の統計解析業務について、業務受託時のフォルダ構造を特定する。\n以下のフォルダ構成は、CROにおける統計解析業務をイメージしている。実際は試験計画時の計画にて、統計家として参画することとなるので、研究計画書のフォルダにて、サンプルサイズ設計や試験デザインのシミュレーション業務が発生するのであれば、そちらに格納する。これらはPMDAの対面助言でも利用することになるため。"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験支援業務を考える.html#はじめに",
    "href": "posts/statistics/2025/臨床試験支援業務を考える.html#はじめに",
    "title": "業務受託時のフォルダ構造",
    "section": "",
    "text": "現在の統計解析業務について、業務受託時のフォルダ構造を特定する。\n以下のフォルダ構成は、CROにおける統計解析業務をイメージしている。実際は試験計画時の計画にて、統計家として参画することとなるので、研究計画書のフォルダにて、サンプルサイズ設計や試験デザインのシミュレーション業務が発生するのであれば、そちらに格納する。これらはPMDAの対面助言でも利用することになるため。"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験支援業務を考える.html#フォルダ構成",
    "href": "posts/statistics/2025/臨床試験支援業務を考える.html#フォルダ構成",
    "title": "業務受託時のフォルダ構造",
    "section": "2 フォルダ構成",
    "text": "2 フォルダ構成\n\n50_統計解析\n\n100_統計解析に関する業務手順書\n200_統計解析計画書\n300_データハンドリングルール\n400_解析図表出力計画書\n500_割付業務\n\n001_割付に関する業務手順書\n002_割付表作成仕様書\n003_割付結果\n\n600_固定データ\n\n610_症例検討会\n620_データレビュー\n\n700_解析用データセット\n\n001_解析用データセット\n002_解析用データセット仕様書\n003_解析用データセット作成プログラム\n\nDevelop\n\nyyyymmdd\n\nFixed\n\n\n800_解析プログラム\n\n001_解析プログラム\n\n解析プログラム\n\nDevelop\n\nyyyymmdd\n\nFixed\n\nSASマクロ\n\n002_解析プログラム仕様書\n\n900_解析結果\n\n001_解析結果\n002_解析結果点検記録\n\n910_追加解析業務\n\n100_追加解析計画書\n200_追加解析図表出力計画書\n700_解析用データセット\n800_解析プログラム\n900_解析結果\n\n990_業務終了報告書"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験支援業務を考える.html#中間解析業務",
    "href": "posts/statistics/2025/臨床試験支援業務を考える.html#中間解析業務",
    "title": "業務受託時のフォルダ構造",
    "section": "3 中間解析業務",
    "text": "3 中間解析業務\n\n51_中間解析業務\n\n100_中間解析に関する業務手順書\n200_中間解析計画書\n300_データハンドリングルール\n400_中間集計図表出力計画書\n600_固定データ\n620_データレビュー\n700_解析用データセット\n\n001_解析用データセット\n002_解析用データセット仕様書\n003_解析用データセット作成プログラム\n\n800_解析プログラム\n\n001_解析プログラム\n002_解析プログラム仕様書\n\n900_解析結果\n\n001_解析結果\n002_解析結果点検記録"
  },
  {
    "objectID": "posts/statistics/2025/臨床試験支援業務を考える.html#section",
    "href": "posts/statistics/2025/臨床試験支援業務を考える.html#section",
    "title": "業務受託時のフォルダ構造",
    "section": "4 ",
    "text": "4"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html",
    "title": "主語と動詞",
    "section": "",
    "text": "主語と述語は、文章の骨格です。命令文や感嘆文を除いて、英語の文章には必ず主語と述語が存在します。論文に命令文や感嘆文を用いることはほぼないので、論文中の文章には主語と動詞が必ず登場します。\n　主語と動詞の適切な用法を知ることは、fool-proof Englishの表現技法を身に付ける第一歩となります。本章では、まず文章の構造（単文・重文・複文）について説明をします。次に、読み手の思考の流れを円滑化する文章の書き方を解説します。さらに論文に頻出する動詞を列挙し、具体的な用例を示します。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#主語と動詞",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#主語と動詞",
    "title": "主語と動詞",
    "section": "",
    "text": "主語と述語は、文章の骨格です。命令文や感嘆文を除いて、英語の文章には必ず主語と述語が存在します。論文に命令文や感嘆文を用いることはほぼないので、論文中の文章には主語と動詞が必ず登場します。\n　主語と動詞の適切な用法を知ることは、fool-proof Englishの表現技法を身に付ける第一歩となります。本章では、まず文章の構造（単文・重文・複文）について説明をします。次に、読み手の思考の流れを円滑化する文章の書き方を解説します。さらに論文に頻出する動詞を列挙し、具体的な用例を示します。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#文章の構造",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#文章の構造",
    "title": "主語と動詞",
    "section": "2 文章の構造",
    "text": "2 文章の構造\n文章の構造には、単文、重文、複文がある。\n単文：1つの主語と1つの動詞を含む文章。\n重文：2つ（以上）の単文を等位接続詞で結んだ文章。\n複文：主節と従属節を従位接続詞や関係詞（関係代名詞・関係副詞）で結んだ文章。\n等位接続詞：and, but , or など\n従位接続詞：that , if ,though, while , before , after , until ,because など\n関係代名詞：that, who , which ,whose , whom , what\n関係副詞：when , where , why , how\n論文はなるべく単文をベースに書くべきです。2つの単文をつないだ重文は用いてもよいでしょう。1つの主節と1つの従属節から構成される複文も許容範囲です。\nしかし、3つ以上の単文をつなげた重文、従属節の中にさらに従属節を含む複文などはやめましょう。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#読み手の思考の流れを円滑化する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#読み手の思考の流れを円滑化する",
    "title": "主語と動詞",
    "section": "3 読み手の思考の流れを円滑化する。",
    "text": "3 読み手の思考の流れを円滑化する。\n文章の可読性（readability）を高めるためには、簡潔な文章に仕上げるとともに、主語・動詞をはじめとする単語の並び順を工夫することが重要です。読み手の論理的思考の流れがスムーズになるように、語順に気を配るべきです。思考の中断や逆転、後ろから前への読み返しを招くような語順は避けなければなりません。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#主語と動詞を文の前に配置する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#主語と動詞を文の前に配置する",
    "title": "主語と動詞",
    "section": "4 主語と動詞を文の前に配置する。",
    "text": "4 主語と動詞を文の前に配置する。\n読み手は、主語と動詞を探しながら文章を読みます。そのため書き手は、主語と動詞を文章中のなるべく前の方に、意識的に配置するとよいでしょう。そのようにすれば、読み手は主語と動詞をすぐに発見でき、安心して文章を読み進められます。\n主語にかかる修飾語が長すぎると、主語と動詞が離れてしまい、読みづらくなります。長い文章の最後の方に動詞が配置されていると、読み手はそこに至るまでに不安を抱えながら読み進めなければなりません。\n主語と動詞が離れている文章の修正例を挙げます。\n例①\n修正前：The relationship between A and B is shown in Table1.\n修正後：Table1 shows the relationship between A and B.\n修正前は、Table1が受動態になっており、by usが省略されている形である。つまり、showの主語はweである。修正後は、Table1を主語にしました。\n英語では、このように無生物主語がよく用いられます。\n例②\n修正前：The proportions of patients who entered ICU, who received emergency intubation, and who required systenmic xx were higher in those who developed severe asthma.\n修正後：Patients with severe asthma were more likely to undergo ICU admission, emergency intubation, and systemic xx.\n修正前の文は、主語の後の関係詞節が長く、主語と動詞が離れており読みにくい。修正後はpatients with severe asthmaを主語に捉えて、be more likely to やundergoという論文頻出表現を用いて、全体を短くまとめている。\n例③\n修正前：Data on several comordibities, including xxx were not available.\n修正後：No data were available on xxx.\n修正前の文章において、includingがどこまで続くのか分かりずらい。さらに、notという否定語が文章の後半に出てくるため、最後の最後で思考の逆転が必要になる。よみにくい！修正後の文章では、否定語を先頭に、主語と動詞の位置を前の方に配置しました。また、includingを含む句をかっこでくくり、冗長さ・曖昧さを削除することにしました。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#中心となる単語を先頭に配置する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#中心となる単語を先頭に配置する",
    "title": "主語と動詞",
    "section": "5 中心となる単語を先頭に配置する。",
    "text": "5 中心となる単語を先頭に配置する。\n論文には必ず中心となるテーマがあります。対象となる疾患名や治療法・薬剤名など、論文の中心テーマを示す単語は、Introductionの第一文の先頭に配置しましょう。そうすることで、書き手が注目している中心テーマが何であるかを、読み手にいち早く知ってもらうことが大事です。各文章においても、中心となる単語を先頭に配置することにより、読み手の意識をその単語に集中させることができる。\n例①\n修正前：Randomized controlled trials have shown the effect of xxx.\n修正後：xxx has been shown to be effective in randomized controlled trials.\n例②\n修正前：Among drugs for xxx, A was the most common regimen.\n修正後：A was most commonly used for xxx."
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#動詞の名詞系を避ける",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#動詞の名詞系を避ける",
    "title": "主語と動詞",
    "section": "6 動詞の名詞系を避ける。",
    "text": "6 動詞の名詞系を避ける。\n簡潔な文章とは、必要最小限の単語数からなる文章です。「動詞の名詞系＋of ～」という形の表現は、無駄に単語数を増やすので、避けるべきである。ofが長いんだな。\n例①\n修正前：Separation of xxx from the serum was conducted.\n修正後：The xxx were separated from the serum.\n例②\n修正前：Log transformation of the non-normal data was performed.\n修正後：The non-normal data were log-transformed."
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#代名詞の位置に注意する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#代名詞の位置に注意する",
    "title": "主語と動詞",
    "section": "7 代名詞の位置に注意する。",
    "text": "7 代名詞の位置に注意する。\n代名詞が指し示す名詞が先にあって、代名詞はその後に存在すること（後方指示）が原則です。\n次の文章は、代名詞（it）が先にあって、代名詞が指し示す名詞（lauric acid）が後に存在する。読み手はitが何を示すのか分からないので読みずらい！！\n例①\n修正後：XX is user for xx, although it is xxx.\n形式主語itを用いてthat節やto不定詞を受ける表現をよくみかけます。決して悪くはなく、適宜用いてもよいでしょう。しかし、文中での主語の導入が一歩遅れてしまうことは留意すべきです。できれば、形式主語を用いずに表現できる場合は、用いない方がよいでしょう\n例①\n修正前：It seemed that that patients had ..\n修正後：The patients seemed to have..\n例②\n修正前：It is mandatory for physicians to provide A with B.\n修正後：Physicians must provide A with B."
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#動作状態の主体を明示する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#動作状態の主体を明示する",
    "title": "主語と動詞",
    "section": "8 動作・状態の主体を明示する。",
    "text": "8 動作・状態の主体を明示する。\n動詞の変化形（不定詞、分詞、名詞形）を用いる場合、動詞の意味上の主語（動作の主体）が文脈上明らかであれば、それを省略できる。しかし、動作の主体が不明であるのにそれを省略すうると、曖昧な文章になります。\n修正前：It was impossible to obtain information on the volume of blood loss in the database.\nここでは主語は誰なのか？weなのか、広く一般の人々（they）なのか、判然としません。\n修正後：The database did not include information on the volume of blood loss."
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#冗長な文章の修正例",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#冗長な文章の修正例",
    "title": "主語と動詞",
    "section": "9 冗長な文章の修正例",
    "text": "9 冗長な文章の修正例\n\n9.1 無駄な単語を削除する。\n例文①\n修正前：We identified 8,561 hospitalized patients who received surgery and among them, 72 patients were identified as case patients developed surgical site infection.\n修正後：We identified 8,561 hospitalized patients who received surgery. Among them, 72 developed surgical site infection.\n例文②\n修正前：In the multivariable analysis, only treatment at high-volume hospitals was significantly associated with xxx, while the other factors were not significantly associated with in-hospital motality.\n修正後：In the multivariable analysis, treatment at high-volume hospitals was the only factor significantly associated with lower in-hospital mortality."
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#動詞を制する者",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#動詞を制する者",
    "title": "主語と動詞",
    "section": "10 動詞を制する者",
    "text": "10 動詞を制する者"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#実施する達成する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#実施する達成する",
    "title": "主語と動詞",
    "section": "11 （1）実施する、達成する",
    "text": "11 （1）実施する、達成する\n\nconduct\nperform\nachieve\ncomplete"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#調べる",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#調べる",
    "title": "主語と動詞",
    "section": "12 （2）調べる",
    "text": "12 （2）調べる\n\nexamine\ninvestigate\nexplore\nverify"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#示す表す",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#示す表す",
    "title": "主語と動詞",
    "section": "13 （3）示す、表す",
    "text": "13 （3）示す、表す\n\ndemonstrate\nreveal\nexhibit\ndenote\ndepict\ndescribe\nmanifest"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#明らかにする",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#明らかにする",
    "title": "主語と動詞",
    "section": "14 （4）明らかにする",
    "text": "14 （4）明らかにする"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#説明する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#説明する",
    "title": "主語と動詞",
    "section": "15 （5）説明する",
    "text": "15 （5）説明する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#示唆する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#示唆する",
    "title": "主語と動詞",
    "section": "16 （6）示唆する",
    "text": "16 （6）示唆する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#強調する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#強調する",
    "title": "主語と動詞",
    "section": "17 （7）強調する",
    "text": "17 （7）強調する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#仮定する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#仮定する",
    "title": "主語と動詞",
    "section": "18 （8）仮定する",
    "text": "18 （8）仮定する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#推論する推計する推測する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#推論する推計する推測する",
    "title": "主語と動詞",
    "section": "19 （9）推論する、推計する、推測する",
    "text": "19 （9）推論する、推計する、推測する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#評価する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#評価する",
    "title": "主語と動詞",
    "section": "20 （10）評価する",
    "text": "20 （10）評価する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#関連する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#関連する",
    "title": "主語と動詞",
    "section": "21 （11）関連する",
    "text": "21 （11）関連する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#起因する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#起因する",
    "title": "主語と動詞",
    "section": "22 （12）起因する",
    "text": "22 （12）起因する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#引き起こす",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#引き起こす",
    "title": "主語と動詞",
    "section": "23 （13）引き起こす",
    "text": "23 （13）引き起こす"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#受ける",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#受ける",
    "title": "主語と動詞",
    "section": "24 （14）受ける",
    "text": "24 （14）受ける"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#対処する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#対処する",
    "title": "主語と動詞",
    "section": "25 （15）対処する",
    "text": "25 （15）対処する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#影響する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#影響する",
    "title": "主語と動詞",
    "section": "26 （16）影響する",
    "text": "26 （16）影響する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#可能にする",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#可能にする",
    "title": "主語と動詞",
    "section": "27 （17）可能にする",
    "text": "27 （17）可能にする"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#含む",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#含む",
    "title": "主語と動詞",
    "section": "28 （18）含む",
    "text": "28 （18）含む"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#分割する割り当てる",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#分割する割り当てる",
    "title": "主語と動詞",
    "section": "29 （19）分割する、割り当てる、",
    "text": "29 （19）分割する、割り当てる、"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#特定する",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#特定する",
    "title": "主語と動詞",
    "section": "30 （20）特定する",
    "text": "30 （20）特定する"
  },
  {
    "objectID": "posts/statistics/2025/英文法_02_主語と述語.html#その他",
    "href": "posts/statistics/2025/英文法_02_主語と述語.html#その他",
    "title": "主語と動詞",
    "section": "31 （21）その他",
    "text": "31 （21）その他\n\n31.1"
  },
  {
    "objectID": "posts/statistics/2025/英文法_04_関係詞.html",
    "href": "posts/statistics/2025/英文法_04_関係詞.html",
    "title": "関係詞",
    "section": "",
    "text": "関係代名詞、関係副詞は、2つの文章を1つにつなげられる便利な用法であり、論文でも良く用いられます。しかし、不用意につなげると読みにくくなるので注意が必要です。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_04_関係詞.html#関係詞について",
    "href": "posts/statistics/2025/英文法_04_関係詞.html#関係詞について",
    "title": "関係詞",
    "section": "",
    "text": "関係代名詞、関係副詞は、2つの文章を1つにつなげられる便利な用法であり、論文でも良く用いられます。しかし、不用意につなげると読みにくくなるので注意が必要です。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_04_関係詞.html#制限用法と非制限用法",
    "href": "posts/statistics/2025/英文法_04_関係詞.html#制限用法と非制限用法",
    "title": "関係詞",
    "section": "2 制限用法と非制限用法",
    "text": "2 制限用法と非制限用法\n関係詞の制限用法（限定用法）とは、文中の名詞の直後に関係詞節を置いて、後方から限定修飾する用法です。\n関係詞の非制限用法（非限定用法）とは、文中の名詞に非限定的に説明を付加するために、カンマで区切った関係詞節を後方に置く用法となる。\n以下の2つの文章は全く異なります。\n\nJapanese researchers who are poor at writing English should read this book.\nJapanese researchers , who are poor at writing English , should read this book.\n\n①は関係詞節が先行詞を限定修飾する。\n②は関係詞節は先行詞に説明を付加する。\n①は「日本人研究者には英語の書き方がとくぃな人も苦手な人もいるが、そのなかでも）英語の書き方が苦手な日本人研究者はこの本を読むべきだ」という意味\n②は「日本人研究者は、（すべて）英語の書き方が下手であり、この本を読むべきだ」という意味。\n以下の2つを見比べよう\n① I met my mother who lives in Tokyo.\n② I met my mother , who lives in Tokyo."
  },
  {
    "objectID": "posts/statistics/2025/英文法_04_関係詞.html#前置詞関係代名詞",
    "href": "posts/statistics/2025/英文法_04_関係詞.html#前置詞関係代名詞",
    "title": "関係詞",
    "section": "3 前置詞＋関係代名詞",
    "text": "3 前置詞＋関係代名詞"
  },
  {
    "objectID": "posts/statistics/2025/英文法_04_関係詞.html#関係副詞",
    "href": "posts/statistics/2025/英文法_04_関係詞.html#関係副詞",
    "title": "関係詞",
    "section": "4 関係副詞",
    "text": "4 関係副詞"
  },
  {
    "objectID": "posts/statistics/2025/英文法_04_関係詞.html#論文における関係詞",
    "href": "posts/statistics/2025/英文法_04_関係詞.html#論文における関係詞",
    "title": "関係詞",
    "section": "5 論文における関係詞",
    "text": "5 論文における関係詞"
  },
  {
    "objectID": "posts/statistics/2025/英文法_04_関係詞.html#先行詞は関係詞と隣接させる",
    "href": "posts/statistics/2025/英文法_04_関係詞.html#先行詞は関係詞と隣接させる",
    "title": "関係詞",
    "section": "6 先行詞は関係詞と隣接させる",
    "text": "6 先行詞は関係詞と隣接させる"
  },
  {
    "objectID": "posts/statistics/2025/英文法_04_関係詞.html#関係詞を用いない方が簡潔に書けるケース",
    "href": "posts/statistics/2025/英文法_04_関係詞.html#関係詞を用いない方が簡潔に書けるケース",
    "title": "関係詞",
    "section": "7 関係詞を用いない方が簡潔に書けるケース",
    "text": "7 関係詞を用いない方が簡潔に書けるケース"
  },
  {
    "objectID": "posts/statistics/2025/英文法_06_接続詞.html",
    "href": "posts/statistics/2025/英文法_06_接続詞.html",
    "title": "接続詞",
    "section": "",
    "text": "接続詞は、文と文のつながりや、論理展開の方向性を決める役割があります。論文においては、適切な接続詞を用いてロジックをつなぐことがとりわけ重要となる。本章では、はじめに接続詞の基礎知識をまとめる。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_06_接続詞.html#接続詞",
    "href": "posts/statistics/2025/英文法_06_接続詞.html#接続詞",
    "title": "接続詞",
    "section": "",
    "text": "接続詞は、文と文のつながりや、論理展開の方向性を決める役割があります。論文においては、適切な接続詞を用いてロジックをつなぐことがとりわけ重要となる。本章では、はじめに接続詞の基礎知識をまとめる。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_06_接続詞.html#接続詞の基礎知識",
    "href": "posts/statistics/2025/英文法_06_接続詞.html#接続詞の基礎知識",
    "title": "接続詞",
    "section": "2 接続詞の基礎知識",
    "text": "2 接続詞の基礎知識\n\n2.1 接続詞の分類\n接続詞には、等位接続詞、従位接続詞、接続副詞がある。\n\n等位接続詞：対等の関係にある文と文、句と句、語と語をつなぐ接続詞\n\n例：and , but , or ,soなど\n\n従位接続詞：主節に従位節をつなぐ接続詞\n\n例：that , if ,whether , unless, though , although, as , while , whereas , before , after , till , until , since , once ,because\n\n接続副詞：もともと副詞であるものの接続詞的な働きをもつ語句\n\n例：however, moreover , furthermore ,also ,otherwise , yet , then\n\n\n\n\n2.2 注意すべき接続詞の用法\n\nand, so\n文頭にAndを置くことは誤りではないものの、論文にはふさわしくありあせん。文と文をカンマで区切りandでつなくこと（… , and …)はよく行われます。soは口語的なので、文頭（So,…）でも文中（…,so…)でも論文では用いない方がよいでしょう。\nbut, however\n文頭にButを置くことは誤りではないものの、論文にはふさわしくありあせん。文と文をカンマで区切りbutでつなくこと（… , but …)はよく行われます。howeverは文頭においてもよく、However, … や文中に挿入することもできます。…, however,…\nnor\n等位接続詞norには、以下の2つの用法があります。\n①neither A nor B 「AもBも～ない」\n例：Neither soy nor isoflavone intake affects make reporoductive hormones.\nBoth A and Bは複数扱い、either A or Bとneigher A nor Bは単数扱いです。\n②否定文. nor ～「否定文, また～でない」\n例：Understanding A does not reflect the xxx . nor will it improve xxx.\nnor以降の主語と動詞は倒置される。\nifとwhether\nifは、①「（間接疑問文を導いて）～かどうか」、②「もしも～ならば」\nwhetherは、①「（間接疑問文を導いて）～かどうか」、②「～であろうとなかろうと」\n①について、主語や補語となる節、前置詞の目的語となる節、同格節では、whetherのみ用いられ、ifは用いられません。\nbecauseとsince\nbecuaseは、読者がまだ把握していないと考えられる新規の情報を理由・根拠として挙げる場合に用いられる。\nsinceは、周知の事実や、読者がすでに把握していると考えられる情報を理由・根拠として挙げる場合に用いられます。\n接続詞asも理由を表す場合に用いられます。because/sinceよりもやや堅い表現です。しかし、asには「～と同様に」、「～につれて」と他の意味もあるので紛らわしい表現になりがちです。Fool-proof Englishでは、理由のasは用いず、because、sinceの使用が推奨されます。\nなお、1つ前の文章に対する理由や根拠に関する説明を、becauseを用いずに、独立した一文でthat節を用いて説明することもあります。\n・The reason for this may be that …\n・One possible explanation for this is that …"
  },
  {
    "objectID": "posts/statistics/2025/英文法_06_接続詞.html#論文における接続詞",
    "href": "posts/statistics/2025/英文法_06_接続詞.html#論文における接続詞",
    "title": "接続詞",
    "section": "3 論文における接続詞",
    "text": "3 論文における接続詞\n\n従位接続詞thatを用いる表現\nprovided that：　～である限りは（= only if ）\nin that ： ～という点では\nexcept that：～ということを除けば\ngiven that ：～であることを考えると\nin case\neven if , even though\nwhile , whereas\nalbeit\n\n\n3.1 接続副詞\n\nbesides, also, moreover,furthermore：また、そのうえ、さらに\nneverthless, nonetheless：にもかかわらず、それでもなお\ntherefore, thereby , thus , hence：それゆえ、したがって\n文頭のtherefore,…は、かなり堅い表現です。最後に重要な結論を導く場面で用います。1つの論文中に、文頭のTherefore,…を多用すべきではありません。文頭ではなく、文中に挿入すると、かなり柔らかい表現になります。\ntherebyは文頭で用いられることはありません。「完全文, thereby ~ing」の形で、完全文をカンマで区切った直後にtherebyを置いて動名詞句を導きます。\nthus\ntherefore,therebyよりもやわらかめの表現です。文頭にThus\nhence\n「そのため、それゆえ」といった意味であり、文頭に用いられる。"
  },
  {
    "objectID": "posts/statistics/2025/英文法_11_コロン_セミコロン_ハイフン_ダッシュ.html",
    "href": "posts/statistics/2025/英文法_11_コロン_セミコロン_ハイフン_ダッシュ.html",
    "title": "コロン_セミコロン_ハイフン_ダッシュ",
    "section": "",
    "text": "本章では、文章や語句をつなぐ記号である、コロン、セミコロン、ハイフン、ダッシュについて解説する。\n\n\nコロン（：）は前後の内容がイコールの関係を示す。コロンはthe followingやas followsの後に用いられることもあります。\n例：The inclusion criteria were as follows:\nセミコロン（;）の後に続く文は、セミコロンの前の文の補足説明となる。セミコロンの直後にthusやthereforeを挿入することもある。\n\n\n\n\nHe ate beef, and she ate chicken.\nHe ate beef: she ate chicken.\nHe ate beef. She ate chicken.\n\n上の3つの文章において、2つの単文は、①が最も弱く区切られ、②がそれに続き、③は完全に区切られている。区切る強さは、カンマ（,）&lt; セミコロン（:）&lt;ピリオド(.）です。\nカンマの直後にandを置く必要がありますが、セミコロンの直後にはandを置く必要はありません。\nなお、3つ以上の単語やフレーズを並列で表記する場合、カンマで区切り、最後にandを挿入します。\nandの直後にカンマを入れるかどうかは流儀によるので、どちらでもよいでしょう。\n\n×：A and B and C\n〇： A , B and C\n〇：A , B , and C"
  },
  {
    "objectID": "posts/statistics/2025/英文法_11_コロン_セミコロン_ハイフン_ダッシュ.html#コロンとセミコロン",
    "href": "posts/statistics/2025/英文法_11_コロン_セミコロン_ハイフン_ダッシュ.html#コロンとセミコロン",
    "title": "コロン_セミコロン_ハイフン_ダッシュ",
    "section": "",
    "text": "本章では、文章や語句をつなぐ記号である、コロン、セミコロン、ハイフン、ダッシュについて解説する。\n\n\nコロン（：）は前後の内容がイコールの関係を示す。コロンはthe followingやas followsの後に用いられることもあります。\n例：The inclusion criteria were as follows:\nセミコロン（;）の後に続く文は、セミコロンの前の文の補足説明となる。セミコロンの直後にthusやthereforeを挿入することもある。\n\n\n\n\nHe ate beef, and she ate chicken.\nHe ate beef: she ate chicken.\nHe ate beef. She ate chicken.\n\n上の3つの文章において、2つの単文は、①が最も弱く区切られ、②がそれに続き、③は完全に区切られている。区切る強さは、カンマ（,）&lt; セミコロン（:）&lt;ピリオド(.）です。\nカンマの直後にandを置く必要がありますが、セミコロンの直後にはandを置く必要はありません。\nなお、3つ以上の単語やフレーズを並列で表記する場合、カンマで区切り、最後にandを挿入します。\nandの直後にカンマを入れるかどうかは流儀によるので、どちらでもよいでしょう。\n\n×：A and B and C\n〇： A , B and C\n〇：A , B , and C"
  },
  {
    "objectID": "posts/statistics/2025/英文法_11_コロン_セミコロン_ハイフン_ダッシュ.html#ハイフンとダッシュ",
    "href": "posts/statistics/2025/英文法_11_コロン_セミコロン_ハイフン_ダッシュ.html#ハイフンとダッシュ",
    "title": "コロン_セミコロン_ハイフン_ダッシュ",
    "section": "2 ハイフンとダッシュ",
    "text": "2 ハイフンとダッシュ\n\n2.1 2つ以上の単語をつなぐハイフン\n\n\n2.2 接頭辞をつなぐハイフン"
  },
  {
    "objectID": "posts/statistics/2025/解析プロジェクトの最初に作成するSAS_PGM.html",
    "href": "posts/statistics/2025/解析プロジェクトの最初に作成するSAS_PGM.html",
    "title": "解析プロジェクトの最初に作成するSASプログラム",
    "section": "",
    "text": "SASを使った解析プロジェクトを始める際、毎回同じような初期設定のコードを書いている方も多いのではないでしょうか。この技術メモでは、解析プロジェクトの最初に作成する**Run_ADAM-Prg.sas**というメインプログラムについて、その構成と各セクションの意図を詳しく解説します。この初期設定をしっかり行うことで、プロジェクト全体の効率性と再現性が大幅に向上します。\n\n\nproc datasets kill nolist ; run ; quit ;\ndm 'out ; clear ; log ; clear ;' ;\n\n/***********************************************************************\n* Project         : 臨床研究の統計解析プログラミング\n* Program name    : Run_ADAM-Prg.sas\n* Author          : Kota Sakamoto\n* Date created    : 2025/07/02\n* Purpose         : ADaM作成プログラム\n* Revision History :\n***********************************************************************/\nプログラムの冒頭にこれらを記述することで、SASセッションを完全にクリーンな状態から始めることができます。\n\nproc datasets kill nolist ; run ; quit ;: 現在のSASセッションに存在するすべてのライブラリ（特にWORKライブラリ）内のデータセットとカタログを削除します。これにより、以前の実行で残った一時ファイルによる予期せぬエラーを防ぎます。\ndm 'out ; clear ; log ; clear ;' ;: SASログウィンドウと出力ウィンドウの内容をクリアします。これにより、現在の実行に関するログのみが表示され、デバッグがしやすくなります。\n\nその下には、ご指定のプログラムヘッダを記述します。プロジェクト名、ファイル名、作成者、作成日、目的、変更履歴などの情報を含めることで、プログラムの目的が明確になり、後から見返したり他の人と共有したりする際に非常に役立ちます。特に変更履歴は、問題発生時の原因特定や、新しいメンバーへの引き継ぎにおいて不可欠です。\n\n\n\nSAS\n*--- オプション設定 ---*;\noptions noxwait;    * XコマンドでSASシステムから制御した後、自動的に再度SASシステムに戻る ;\noptions noxsync;  * Xコマンドで開始した処理の終了を待たずにSASシステムに戻る ;\noptions noquotelenmax;  * 引用符で囲んだ文字が長すぎる場合のNOTEを出さないようにする ;\noptions source2;    * 2次ソースステートメントをSASログに書き込む ;\nSASの**OPTIONSステートメント**は、SASセッションの動作を制御するためのものです。プロジェクトの性質や個人の好みに応じて設定しますが、特に重要なものをいくつかご紹介します。\n\nnoxwait / noxsync: 外部コマンド（Xコマンドなど）の実行に関するオプションです。これらを指定することで、外部プロセスの完了を待たずにSASに戻る、または外部プロセスの実行中にSASがブロックされないようにすることができます。これは、外部スクリプトの呼び出しや、大量のファイル操作を行う際に役立ちます。\nnoquotelenmax: 長い文字列を引用符で囲んだ際に表示されるNOTEメッセージを抑制します。ログをきれいに保ちたい場合に便利です。\nsource2: INCLUDEステートメントなどで読み込まれた二次ソースコードもSASログに出力するようになります。デバッグ時にどのコードが実行されているかを確認するのに非常に役立ちます。\n追加の推奨オプション:\n\nls=MAX / ps=MAX: ログの行長とページ長を最大に設定し、ログが見やすくなります。\nfmterr: フォーマットが見つからない場合にエラーを発生させます。予期せぬフォーマットエラーを防ぐために重要です。\nerrors=0: SASセッション中のエラー表示数を無制限にします。これにより、すべてのエラーを確認できます。\n\n\n\n\n\nSAS\n*--- マクロ変数のクリア ---*;\n%put _user_;\ndata vars;\n    set sashelp.vmacro;\nrun;\n\ndata symdel;\n  set sashelp.vmacro;\n  where scope = 'GLOBAL' and NAME not in ('SYS_SQL_IP_ALL', 'SYS_SQL_IP_STMT');\nrun;\n\ndata _null_;\n  set symdel;\n  call symdel(name);\nrun; \n前のセッションやテスト実行で残ったグローバルマクロ変数が、現在の実行に影響を与えることを防ぐために、不要なマクロ変数をクリアします。sashelp.vmacroは現在定義されているマクロ変数の情報を持ち、CALL SYMDEL関数を使って特定の変数を削除できます。これにより、常にクリーンな状態でプログラムを実行できます。\n\n\n\nSAS\n*--- データセットの一括クリア ---*;\nproc delete data=work._all_;\nrun;\nSASの**WORKライブラリ**は一時的なデータセットが格納される場所です。過去の実行で作成されたデータセットが残っていると、意図しない結果を招いたり、ディスク容量を圧迫したりする可能性があります。PROC DELETE DATA=WORK._ALL_を使用することで、WORKライブラリ内のすべてのデータセットを効率的に削除し、常にゼロベースで解析を開始できます。\n\n\n\nSAS\n*--- ディレクトリの設定 ---*;\n* フルパスの取得 ;\n%let path = %sysget(SAS_EXECFILEPATH);\n%put &path.;\n\n* ファイル名の取得 ;\n%let filename = %SCAN(&path., -1, \"\\\");\n%put &filename.;\n\n* ディレクトリの取得 ;\n%let drct = %substr(&path., 1, %length(&path.)-%length(%SCAN(&path.,-1,\"\\\"))-1);\n%put &drct.;\n\n* ディレクトリ設定 ;\ndata _null_;\n    call system(\"cd &drct.\");\nrun;\nプロジェクトのパスを動的に設定することは非常に重要です。これにより、プログラムを異なる環境やPCに移動しても、パスを手動で変更する必要がなくなります。\n\n%sysget(SAS_EXECFILEPATH): 現在実行しているSASプログラムのフルパスを取得します。\n%SCAN / %SUBSTR / %LENGTH: マクロ関数を組み合わせて、フルパスからファイル名やディレクトリパスを抽出します。\nCALL SYSTEM(\"cd &drct.\"): SASセッションのカレントディレクトリを実行プログラムのディレクトリに設定します。これにより、相対パスでのファイル指定が容易になります。\n\n\n\n\nSAS\n*--- マクロ変数の定義 ---*;\n%let InputPath = &drct.\\Input;\n%let OutputPath = &drct.\\Output;\n%let LogPath = &drct.\\Log;\n%let PrgPath = &drct.\\Prg;\n%let SettingsPath = &drct.\\Settings;\n%let SpecPath = &drct.\\Spec;\n\n%let SettingsFile = ADAM設定ファイル.xlsx;\n%let SettingsSheet = 設定;\n\n%put _user_;\nプロジェクト内で頻繁に使用するパスやファイル名をマクロ変数として定義します。これにより、コードの可読性が向上し、将来的にパスが変更になった場合でも、このセクションを修正するだけで済みます。\n\nInputPath: 生データや外部ファイルなど、入力データが置かれるディレクトリ\nOutputPath: 出力される解析結果やレポートが保存されるディレクトリ\nLogPath: SASログファイルが保存されるディレクトリ（ログの自動保存を設定する場合）\nPrgPath: サブプログラムが置かれるディレクトリ\nSettingsPath: 設定ファイルが置かれるディレクトリ\nSpecPath: 仕様書が置かれるディレクトリ\n\nまた、設定ファイル名やシート名などもマクロ変数として定義しておくと、変更があった際に対応が容易になります。\n\n\n\nSAS\n*--- 実行Prgのパス設定 ---*;\nfilename saspgm \"&PrgPath.\";\nFILENAMEステートメントを使用して、サブプログラムが格納されているディレクトリに**fileref（ファイル参照名）**を割り当てます。これにより、後続の%INCLUDEステートメントで、相対パスでサブプログラムを指定できるようになります。\n\n\n\n/*--- 基本設定の読み込み ---*/\n%include saspgm( \"01_LoadBaseSettings.sas\" );\n\n\n/*--- プロジェクト設定の読み込み ---*/\n%include saspgm( \"02_ImportProjectConfig.sas\" );\n\n\n/*--- データソース定義のロード ---*/\n%include saspgm( \"03_DefineDataSources.sas\" );\n\n\n/*--- 外部ライブラリ接続 ---*/\n%include saspgm( \"04_LinkExternalLibs.sas\" );\n\n\n/*--- フォーマット定義の生成 ---*/\n%include saspgm( \"05_GenerateFormatDefs.sas\" );\n\n\n/*--- 全体設定の統合 ---*/\n%include saspgm( \"06_IntegrateGlobalSettings.sas\");\n\n\n/*--- 共通マクロのロード ---*/\n%include saspgm( \"11_McrBuildMasterDataset.sas\" );\n%include saspgm( \"12_McrStandardizeDates.sas\" );\n%include saspgm( \"13_McrCreateCoreADaMs.sas\" );\n%include saspgm( \"14_McrAppendSuppData.sas\" );\n%include saspgm( \"15_McrRecodeVariables_V2.sas\" );\n\n\n/*--- ADaMデータセットの作成 ---*/\n%include saspgm( \"21_DeriveADSL.sas\" );\n%include saspgm( \"22_DeriveADLB.sas\" );\n%include saspgm( \"23_DeriveADVS.sas\" );\n%include saspgm( \"24_DeriveADRS.sas\" );\n%include saspgm( \"25_DeriveADAE.sas\" );\n\n\n/*--- 後処理とクリーンアップ ---*/\n%include saspgm( \"99_PostProcessingAndCleanup.sas\" );\nメインプログラムの役割は、各機能ごとのサブプログラムを**%INCLUDEステートメント**で呼び出すことです。これにより、プログラム全体がモジュール化され、以下の利点が得られます。\n\n可読性の向上: 各ファイルが特定の機能に集中するため、コードが読みやすくなります。\n保守性の向上: 特定の機能を変更したい場合、該当するサブプログラムのみを修正すればよいため、影響範囲が限定されます。\n再利用性の向上: 各サブプログラムは独立した機能を持つため、他のプロジェクトでも再利用しやすくなります。\nデバッグの容易性: エラーが発生した場合、どのモジュールで問題が起きているかを特定しやすくなります。\n\n特に、以下のような処理をモジュール化すると良いでしょう。\n\n基本設定の読み込み（01_LoadBaseSettings.sas）: 環境変数や基本的なオプション設定の読み込み。\nプロジェクト設定の読み込み（02_ImportProjectConfig.sas）: 解析に必要な各種設定を外部ファイルから読み込む処理。\nデータソース定義のロード（03_DefineDataSources.sas）: 使用するデータのソースや構造を定義する処理。\n外部ライブラリ接続（04_LinkExternalLibs.sas）: データベースや他のSASライブラリへの接続設定。\nフォーマット定義の生成（05_GenerateFormatDefs.sas）: 値のラベル付けなど、解析に必要なSASフォーマットの作成。\n全体設定の統合（06_IntegrateGlobalSettings.sas）: 読み込んだ設定や定義を統合する処理。\n共通マクロのロード（11_McrBuildMasterDataset.sasなど）: プロジェクト全体で利用する汎用的なSASマクロの定義。\nADaMデータセットの作成（21_DeriveADSL.sasなど）: 各ADaMデータセットの具体的な作成ロジック。\n後処理とクリーンアップ（99_PostProcessingAndCleanup.sas）: ログファイルの保存、一時的なリソースの解放など、プログラム終了時に行うべき処理。\n\n以下のプログラムも便利です。\n*-----------------------------------------------------------------------------*;\n* Initial ;\n*-----------------------------------------------------------------------------*;\n%let execpath = \"\" ;\n\n/* 現在実行しているプログラムのパスを取得 (ファイル名を含む) */\n%macro setexecpath ;\n  %let execpath = %sysfunc(getoption(sysin)) ;\n  %if %length(&execpath.) = 0 %then %let execpath = %sysget(sas_execfilepath) ;\n%mend setexecpath ;\n%setexecpath ;\n\n/* ファイル名からドメイン名の切出し */\n%let DOMAIN = %scan(%scan(&execpath, -1, \"\\\"), 1, \".\") ;\n/* パスのみの切り出し (プログラムパスからファイル名を削除) */\n%let CURRENT = %qsubstr(\"&execpath.\", 2, %eval(%index(\"&execpath.\", %scan(&execpath, -1, \"\\\")))-2) ;\n\n%put &DOMAIN. &CURRENT. ;\n\n/* 接続先をカレントパスに変更 */\nX \"cd &CURRENT.\" ;\n\n/* init.sas を実行 */\n%inc \"../../05_Macro\\init.sas\" ;\n\n/* 定数の定義 */\n%let FNAME = %upcase(&DOMAIN.) ;\n%let LABEL = Laboratory Tests Analysis Dataset ;\n%let key = STUDYID USUBJID APERIOD PARAMN VISITNUM ;\n\n\n\nこのRun_ADAM-Prg.sasのような初期設定プログラムは、SAS解析プロジェクトの基盤となります。プログラムの冒頭で環境を整え、パスを動的に管理し、処理をモジュール化することで、コードの品質、保守性、そして何よりも解析の信頼性を向上させることができます。ぜひ、あなたの解析プロジェクトでもこのベストプラクティスを取り入れてみてください。\n\n\n\n*--- データセットとログの一括クリア ---*;\nproc datasets kill nolist ; run ; quit ;\ndm 'out ; clear ; log ; clear ;' ;\n\n/***********************************************************************\n* Project         : 臨床研究の統計解析プログラミング\n* Program name    : Run_ADAM-Prg.sas\n* Author          : Kota Sakamoto\n* Date created    : 2025/07/02\n* Purpose         : ADaM作成プログラム\n* Revision History :\n***********************************************************************/\n\n*--- オプション設定 ---*;\noptions noxwait;    * XコマンドでSASシステムから制御した後、自動的に再度SASシステムに戻る ;\noptions noxsync;  * Xコマンドで開始した処理の終了を待たずにSASシステムに戻る ;\noptions noquotelenmax;  * 引用符で囲んだ文字が長すぎる場合のNOTEを出さないようにする ;\noptions source2;    * 2次ソースステートメントをSASログに書き込む ;\n\n*--- マクロ変数のクリア ---*;\n%put _user_;\ndata vars;\n    set sashelp.vmacro;\nrun;\n\ndata symdel;\n  set sashelp.vmacro;\n  where scope = 'GLOBAL' and NAME not in ('SYS_SQL_IP_ALL', 'SYS_SQL_IP_STMT');\nrun;\n\ndata _null_;\n  set symdel;\n  call symdel(name);\nrun;\n\n*--- ディレクトリの設定 ---*;\n* フルパスの取得 ;\n%let path = %sysget(SAS_EXECFILEPATH);\n%put &path.;\n\n* ファイル名の取得 ;\n%let filename = %SCAN(&path., -1, \"\\\");\n%put &filename.;\n\n* ディレクトリの取得 ;\n%let drct = %substr(&path., 1, %length(&path.)-%length(%SCAN(&path.,-1,\"\\\"))-1);\n%put &drct.;\n\n* ディレクトリ設定 ;\ndata _null_;\n    call system(\"cd &drct.\");\nrun;\n\n\n*--- マクロ変数の定義 ---*;\n%let InputPath = &drct.\\Input;\n%let OutputPath = &drct.\\Output;\n%let LogPath = &drct.\\Log;\n%let PrgPath = &drct.\\Prg;\n%let SettingsPath = &drct.\\Settings;\n%let SpecPath = &drct.\\Spec;\n\n%let SettingsFile = ADAM設定ファイル.xlsx;\n%let SettingsSheet = 設定;\n\n%put _user_;\n\n\n*--- 実行Prgのパス設定 ---*;\nfilename saspgm \"&PrgPath.\";\n\n\n/*--- 基本設定の読み込み ---*/\n%include saspgm( \"01_LoadBaseSettings.sas\" );\n\n\n/*--- プロジェクト設定の読み込み ---*/\n%include saspgm( \"02_ImportProjectConfig.sas\" );\n\n\n/*--- データソース定義のロード ---*/\n%include saspgm( \"03_DefineDataSources.sas\" );\n\n\n/*--- 外部ライブラリ接続 ---*/\n%include saspgm( \"04_LinkExternalLibs.sas\" );\n\n\n/*--- フォーマット定義の生成 ---*/\n%include saspgm( \"05_GenerateFormatDefs.sas\" );\n\n\n/*--- 全体設定の統合 ---*/\n%include saspgm( \"06_IntegrateGlobalSettings.sas\");\n\n\n/*--- 共通マクロのロード ---*/\n%include saspgm( \"11_McrBuildMasterDataset.sas\" );\n%include saspgm( \"12_McrStandardizeDates.sas\" );\n%include saspgm( \"13_McrCreateCoreADaMs.sas\" );\n%include saspgm( \"14_McrAppendSuppData.sas\" );\n%include saspgm( \"15_McrRecodeVariables_V2.sas\" );\n\n\n/*--- ADaMデータセットの作成 ---*/\n%include saspgm( \"21_DeriveADSL.sas\" );\n%include saspgm( \"22_DeriveADLB.sas\" );\n%include saspgm( \"23_DeriveADVS.sas\" );\n%include saspgm( \"24_DeriveADRS.sas\" );\n%include saspgm( \"25_DeriveADAE.sas\" );\n\n\n/*--- 後処理とクリーンアップ ---*/\n%include saspgm( \"99_PostProcessingAndCleanup.sas\" );\n\n\n\n*---------------------- EOF （Run_ADAM-Prg.sas） ------------------------------- ;"
  },
  {
    "objectID": "posts/statistics/2025/解析プロジェクトの最初に作成するSAS_PGM.html#sas解析プロジェクトを始める前にプログラム初期設定のベストプラクティス",
    "href": "posts/statistics/2025/解析プロジェクトの最初に作成するSAS_PGM.html#sas解析プロジェクトを始める前にプログラム初期設定のベストプラクティス",
    "title": "解析プロジェクトの最初に作成するSASプログラム",
    "section": "",
    "text": "SASを使った解析プロジェクトを始める際、毎回同じような初期設定のコードを書いている方も多いのではないでしょうか。この技術メモでは、解析プロジェクトの最初に作成する**Run_ADAM-Prg.sas**というメインプログラムについて、その構成と各セクションの意図を詳しく解説します。この初期設定をしっかり行うことで、プロジェクト全体の効率性と再現性が大幅に向上します。\n\n\nproc datasets kill nolist ; run ; quit ;\ndm 'out ; clear ; log ; clear ;' ;\n\n/***********************************************************************\n* Project         : 臨床研究の統計解析プログラミング\n* Program name    : Run_ADAM-Prg.sas\n* Author          : Kota Sakamoto\n* Date created    : 2025/07/02\n* Purpose         : ADaM作成プログラム\n* Revision History :\n***********************************************************************/\nプログラムの冒頭にこれらを記述することで、SASセッションを完全にクリーンな状態から始めることができます。\n\nproc datasets kill nolist ; run ; quit ;: 現在のSASセッションに存在するすべてのライブラリ（特にWORKライブラリ）内のデータセットとカタログを削除します。これにより、以前の実行で残った一時ファイルによる予期せぬエラーを防ぎます。\ndm 'out ; clear ; log ; clear ;' ;: SASログウィンドウと出力ウィンドウの内容をクリアします。これにより、現在の実行に関するログのみが表示され、デバッグがしやすくなります。\n\nその下には、ご指定のプログラムヘッダを記述します。プロジェクト名、ファイル名、作成者、作成日、目的、変更履歴などの情報を含めることで、プログラムの目的が明確になり、後から見返したり他の人と共有したりする際に非常に役立ちます。特に変更履歴は、問題発生時の原因特定や、新しいメンバーへの引き継ぎにおいて不可欠です。\n\n\n\nSAS\n*--- オプション設定 ---*;\noptions noxwait;    * XコマンドでSASシステムから制御した後、自動的に再度SASシステムに戻る ;\noptions noxsync;  * Xコマンドで開始した処理の終了を待たずにSASシステムに戻る ;\noptions noquotelenmax;  * 引用符で囲んだ文字が長すぎる場合のNOTEを出さないようにする ;\noptions source2;    * 2次ソースステートメントをSASログに書き込む ;\nSASの**OPTIONSステートメント**は、SASセッションの動作を制御するためのものです。プロジェクトの性質や個人の好みに応じて設定しますが、特に重要なものをいくつかご紹介します。\n\nnoxwait / noxsync: 外部コマンド（Xコマンドなど）の実行に関するオプションです。これらを指定することで、外部プロセスの完了を待たずにSASに戻る、または外部プロセスの実行中にSASがブロックされないようにすることができます。これは、外部スクリプトの呼び出しや、大量のファイル操作を行う際に役立ちます。\nnoquotelenmax: 長い文字列を引用符で囲んだ際に表示されるNOTEメッセージを抑制します。ログをきれいに保ちたい場合に便利です。\nsource2: INCLUDEステートメントなどで読み込まれた二次ソースコードもSASログに出力するようになります。デバッグ時にどのコードが実行されているかを確認するのに非常に役立ちます。\n追加の推奨オプション:\n\nls=MAX / ps=MAX: ログの行長とページ長を最大に設定し、ログが見やすくなります。\nfmterr: フォーマットが見つからない場合にエラーを発生させます。予期せぬフォーマットエラーを防ぐために重要です。\nerrors=0: SASセッション中のエラー表示数を無制限にします。これにより、すべてのエラーを確認できます。\n\n\n\n\n\nSAS\n*--- マクロ変数のクリア ---*;\n%put _user_;\ndata vars;\n    set sashelp.vmacro;\nrun;\n\ndata symdel;\n  set sashelp.vmacro;\n  where scope = 'GLOBAL' and NAME not in ('SYS_SQL_IP_ALL', 'SYS_SQL_IP_STMT');\nrun;\n\ndata _null_;\n  set symdel;\n  call symdel(name);\nrun; \n前のセッションやテスト実行で残ったグローバルマクロ変数が、現在の実行に影響を与えることを防ぐために、不要なマクロ変数をクリアします。sashelp.vmacroは現在定義されているマクロ変数の情報を持ち、CALL SYMDEL関数を使って特定の変数を削除できます。これにより、常にクリーンな状態でプログラムを実行できます。\n\n\n\nSAS\n*--- データセットの一括クリア ---*;\nproc delete data=work._all_;\nrun;\nSASの**WORKライブラリ**は一時的なデータセットが格納される場所です。過去の実行で作成されたデータセットが残っていると、意図しない結果を招いたり、ディスク容量を圧迫したりする可能性があります。PROC DELETE DATA=WORK._ALL_を使用することで、WORKライブラリ内のすべてのデータセットを効率的に削除し、常にゼロベースで解析を開始できます。\n\n\n\nSAS\n*--- ディレクトリの設定 ---*;\n* フルパスの取得 ;\n%let path = %sysget(SAS_EXECFILEPATH);\n%put &path.;\n\n* ファイル名の取得 ;\n%let filename = %SCAN(&path., -1, \"\\\");\n%put &filename.;\n\n* ディレクトリの取得 ;\n%let drct = %substr(&path., 1, %length(&path.)-%length(%SCAN(&path.,-1,\"\\\"))-1);\n%put &drct.;\n\n* ディレクトリ設定 ;\ndata _null_;\n    call system(\"cd &drct.\");\nrun;\nプロジェクトのパスを動的に設定することは非常に重要です。これにより、プログラムを異なる環境やPCに移動しても、パスを手動で変更する必要がなくなります。\n\n%sysget(SAS_EXECFILEPATH): 現在実行しているSASプログラムのフルパスを取得します。\n%SCAN / %SUBSTR / %LENGTH: マクロ関数を組み合わせて、フルパスからファイル名やディレクトリパスを抽出します。\nCALL SYSTEM(\"cd &drct.\"): SASセッションのカレントディレクトリを実行プログラムのディレクトリに設定します。これにより、相対パスでのファイル指定が容易になります。\n\n\n\n\nSAS\n*--- マクロ変数の定義 ---*;\n%let InputPath = &drct.\\Input;\n%let OutputPath = &drct.\\Output;\n%let LogPath = &drct.\\Log;\n%let PrgPath = &drct.\\Prg;\n%let SettingsPath = &drct.\\Settings;\n%let SpecPath = &drct.\\Spec;\n\n%let SettingsFile = ADAM設定ファイル.xlsx;\n%let SettingsSheet = 設定;\n\n%put _user_;\nプロジェクト内で頻繁に使用するパスやファイル名をマクロ変数として定義します。これにより、コードの可読性が向上し、将来的にパスが変更になった場合でも、このセクションを修正するだけで済みます。\n\nInputPath: 生データや外部ファイルなど、入力データが置かれるディレクトリ\nOutputPath: 出力される解析結果やレポートが保存されるディレクトリ\nLogPath: SASログファイルが保存されるディレクトリ（ログの自動保存を設定する場合）\nPrgPath: サブプログラムが置かれるディレクトリ\nSettingsPath: 設定ファイルが置かれるディレクトリ\nSpecPath: 仕様書が置かれるディレクトリ\n\nまた、設定ファイル名やシート名などもマクロ変数として定義しておくと、変更があった際に対応が容易になります。\n\n\n\nSAS\n*--- 実行Prgのパス設定 ---*;\nfilename saspgm \"&PrgPath.\";\nFILENAMEステートメントを使用して、サブプログラムが格納されているディレクトリに**fileref（ファイル参照名）**を割り当てます。これにより、後続の%INCLUDEステートメントで、相対パスでサブプログラムを指定できるようになります。\n\n\n\n/*--- 基本設定の読み込み ---*/\n%include saspgm( \"01_LoadBaseSettings.sas\" );\n\n\n/*--- プロジェクト設定の読み込み ---*/\n%include saspgm( \"02_ImportProjectConfig.sas\" );\n\n\n/*--- データソース定義のロード ---*/\n%include saspgm( \"03_DefineDataSources.sas\" );\n\n\n/*--- 外部ライブラリ接続 ---*/\n%include saspgm( \"04_LinkExternalLibs.sas\" );\n\n\n/*--- フォーマット定義の生成 ---*/\n%include saspgm( \"05_GenerateFormatDefs.sas\" );\n\n\n/*--- 全体設定の統合 ---*/\n%include saspgm( \"06_IntegrateGlobalSettings.sas\");\n\n\n/*--- 共通マクロのロード ---*/\n%include saspgm( \"11_McrBuildMasterDataset.sas\" );\n%include saspgm( \"12_McrStandardizeDates.sas\" );\n%include saspgm( \"13_McrCreateCoreADaMs.sas\" );\n%include saspgm( \"14_McrAppendSuppData.sas\" );\n%include saspgm( \"15_McrRecodeVariables_V2.sas\" );\n\n\n/*--- ADaMデータセットの作成 ---*/\n%include saspgm( \"21_DeriveADSL.sas\" );\n%include saspgm( \"22_DeriveADLB.sas\" );\n%include saspgm( \"23_DeriveADVS.sas\" );\n%include saspgm( \"24_DeriveADRS.sas\" );\n%include saspgm( \"25_DeriveADAE.sas\" );\n\n\n/*--- 後処理とクリーンアップ ---*/\n%include saspgm( \"99_PostProcessingAndCleanup.sas\" );\nメインプログラムの役割は、各機能ごとのサブプログラムを**%INCLUDEステートメント**で呼び出すことです。これにより、プログラム全体がモジュール化され、以下の利点が得られます。\n\n可読性の向上: 各ファイルが特定の機能に集中するため、コードが読みやすくなります。\n保守性の向上: 特定の機能を変更したい場合、該当するサブプログラムのみを修正すればよいため、影響範囲が限定されます。\n再利用性の向上: 各サブプログラムは独立した機能を持つため、他のプロジェクトでも再利用しやすくなります。\nデバッグの容易性: エラーが発生した場合、どのモジュールで問題が起きているかを特定しやすくなります。\n\n特に、以下のような処理をモジュール化すると良いでしょう。\n\n基本設定の読み込み（01_LoadBaseSettings.sas）: 環境変数や基本的なオプション設定の読み込み。\nプロジェクト設定の読み込み（02_ImportProjectConfig.sas）: 解析に必要な各種設定を外部ファイルから読み込む処理。\nデータソース定義のロード（03_DefineDataSources.sas）: 使用するデータのソースや構造を定義する処理。\n外部ライブラリ接続（04_LinkExternalLibs.sas）: データベースや他のSASライブラリへの接続設定。\nフォーマット定義の生成（05_GenerateFormatDefs.sas）: 値のラベル付けなど、解析に必要なSASフォーマットの作成。\n全体設定の統合（06_IntegrateGlobalSettings.sas）: 読み込んだ設定や定義を統合する処理。\n共通マクロのロード（11_McrBuildMasterDataset.sasなど）: プロジェクト全体で利用する汎用的なSASマクロの定義。\nADaMデータセットの作成（21_DeriveADSL.sasなど）: 各ADaMデータセットの具体的な作成ロジック。\n後処理とクリーンアップ（99_PostProcessingAndCleanup.sas）: ログファイルの保存、一時的なリソースの解放など、プログラム終了時に行うべき処理。\n\n以下のプログラムも便利です。\n*-----------------------------------------------------------------------------*;\n* Initial ;\n*-----------------------------------------------------------------------------*;\n%let execpath = \"\" ;\n\n/* 現在実行しているプログラムのパスを取得 (ファイル名を含む) */\n%macro setexecpath ;\n  %let execpath = %sysfunc(getoption(sysin)) ;\n  %if %length(&execpath.) = 0 %then %let execpath = %sysget(sas_execfilepath) ;\n%mend setexecpath ;\n%setexecpath ;\n\n/* ファイル名からドメイン名の切出し */\n%let DOMAIN = %scan(%scan(&execpath, -1, \"\\\"), 1, \".\") ;\n/* パスのみの切り出し (プログラムパスからファイル名を削除) */\n%let CURRENT = %qsubstr(\"&execpath.\", 2, %eval(%index(\"&execpath.\", %scan(&execpath, -1, \"\\\")))-2) ;\n\n%put &DOMAIN. &CURRENT. ;\n\n/* 接続先をカレントパスに変更 */\nX \"cd &CURRENT.\" ;\n\n/* init.sas を実行 */\n%inc \"../../05_Macro\\init.sas\" ;\n\n/* 定数の定義 */\n%let FNAME = %upcase(&DOMAIN.) ;\n%let LABEL = Laboratory Tests Analysis Dataset ;\n%let key = STUDYID USUBJID APERIOD PARAMN VISITNUM ;\n\n\n\nこのRun_ADAM-Prg.sasのような初期設定プログラムは、SAS解析プロジェクトの基盤となります。プログラムの冒頭で環境を整え、パスを動的に管理し、処理をモジュール化することで、コードの品質、保守性、そして何よりも解析の信頼性を向上させることができます。ぜひ、あなたの解析プロジェクトでもこのベストプラクティスを取り入れてみてください。\n\n\n\n*--- データセットとログの一括クリア ---*;\nproc datasets kill nolist ; run ; quit ;\ndm 'out ; clear ; log ; clear ;' ;\n\n/***********************************************************************\n* Project         : 臨床研究の統計解析プログラミング\n* Program name    : Run_ADAM-Prg.sas\n* Author          : Kota Sakamoto\n* Date created    : 2025/07/02\n* Purpose         : ADaM作成プログラム\n* Revision History :\n***********************************************************************/\n\n*--- オプション設定 ---*;\noptions noxwait;    * XコマンドでSASシステムから制御した後、自動的に再度SASシステムに戻る ;\noptions noxsync;  * Xコマンドで開始した処理の終了を待たずにSASシステムに戻る ;\noptions noquotelenmax;  * 引用符で囲んだ文字が長すぎる場合のNOTEを出さないようにする ;\noptions source2;    * 2次ソースステートメントをSASログに書き込む ;\n\n*--- マクロ変数のクリア ---*;\n%put _user_;\ndata vars;\n    set sashelp.vmacro;\nrun;\n\ndata symdel;\n  set sashelp.vmacro;\n  where scope = 'GLOBAL' and NAME not in ('SYS_SQL_IP_ALL', 'SYS_SQL_IP_STMT');\nrun;\n\ndata _null_;\n  set symdel;\n  call symdel(name);\nrun;\n\n*--- ディレクトリの設定 ---*;\n* フルパスの取得 ;\n%let path = %sysget(SAS_EXECFILEPATH);\n%put &path.;\n\n* ファイル名の取得 ;\n%let filename = %SCAN(&path., -1, \"\\\");\n%put &filename.;\n\n* ディレクトリの取得 ;\n%let drct = %substr(&path., 1, %length(&path.)-%length(%SCAN(&path.,-1,\"\\\"))-1);\n%put &drct.;\n\n* ディレクトリ設定 ;\ndata _null_;\n    call system(\"cd &drct.\");\nrun;\n\n\n*--- マクロ変数の定義 ---*;\n%let InputPath = &drct.\\Input;\n%let OutputPath = &drct.\\Output;\n%let LogPath = &drct.\\Log;\n%let PrgPath = &drct.\\Prg;\n%let SettingsPath = &drct.\\Settings;\n%let SpecPath = &drct.\\Spec;\n\n%let SettingsFile = ADAM設定ファイル.xlsx;\n%let SettingsSheet = 設定;\n\n%put _user_;\n\n\n*--- 実行Prgのパス設定 ---*;\nfilename saspgm \"&PrgPath.\";\n\n\n/*--- 基本設定の読み込み ---*/\n%include saspgm( \"01_LoadBaseSettings.sas\" );\n\n\n/*--- プロジェクト設定の読み込み ---*/\n%include saspgm( \"02_ImportProjectConfig.sas\" );\n\n\n/*--- データソース定義のロード ---*/\n%include saspgm( \"03_DefineDataSources.sas\" );\n\n\n/*--- 外部ライブラリ接続 ---*/\n%include saspgm( \"04_LinkExternalLibs.sas\" );\n\n\n/*--- フォーマット定義の生成 ---*/\n%include saspgm( \"05_GenerateFormatDefs.sas\" );\n\n\n/*--- 全体設定の統合 ---*/\n%include saspgm( \"06_IntegrateGlobalSettings.sas\");\n\n\n/*--- 共通マクロのロード ---*/\n%include saspgm( \"11_McrBuildMasterDataset.sas\" );\n%include saspgm( \"12_McrStandardizeDates.sas\" );\n%include saspgm( \"13_McrCreateCoreADaMs.sas\" );\n%include saspgm( \"14_McrAppendSuppData.sas\" );\n%include saspgm( \"15_McrRecodeVariables_V2.sas\" );\n\n\n/*--- ADaMデータセットの作成 ---*/\n%include saspgm( \"21_DeriveADSL.sas\" );\n%include saspgm( \"22_DeriveADLB.sas\" );\n%include saspgm( \"23_DeriveADVS.sas\" );\n%include saspgm( \"24_DeriveADRS.sas\" );\n%include saspgm( \"25_DeriveADAE.sas\" );\n\n\n/*--- 後処理とクリーンアップ ---*/\n%include saspgm( \"99_PostProcessingAndCleanup.sas\" );\n\n\n\n*---------------------- EOF （Run_ADAM-Prg.sas） ------------------------------- ;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "",
    "text": "効率的なフォルダ構成でプロジェクトを管理する\n\n\nSAS（Statistical Analysis System）を使ったデータ分析プロジェクトにおいて、最初に決めるべき重要な要素の一つがフォルダ構成です。適切なフォルダ構成を設定することで、プロジェクトの管理が格段に楽になり、チームでの作業効率も向上します。\n多くのSAS初心者は、とりあえずデスクトップやマイドキュメントにファイルを保存してしまいがちですが、プロジェクトが進むにつれて「あのファイルはどこに保存したっけ？」「これは最新バージョン？」といった問題に直面することになります。\n\n\n\n\nSASユーザー総会2014年度：PMDAへの承認申請時 CDISC標準電子データ提出に向けた社内標準のリモデリング（塩野義製薬）\n\n\n\n\nSASプログラミングを実施する際は、フォルダ構成をまず設定します。ここでは、実際の業務で使用することができそうな標準的な構成例を紹介します。 任意のプロジェクトフォルダに対して以下のようなフォルダを作成します。テンプレートを作成しておくのが便利でしょう。もしくはProjectフォルダを作成して、自動でフォルダを作成するSASプログラムを準備しておくこともよいかもしれません。ここでの例は、あくまで1つの例であり、より使いやすくなるようなフォルダ構成に変更しても差し支えない。なお、このフォルダ構成はTLF作成のプログラムにおいても引用できる可能性がある。、TLFの解析を踏まえたフォルダ構成は別途提案する。\n\nInput\nLog\nOutput\nPrg\nSetting\nSpec\n\n\n\nInput：分析に使用する元データを格納\n\n外部から受け取ったCSVファイル、Excelファイル -\nデータベースから抽出した生データ\n既存のSASデータセット\n\nLog：SASプログラムの実行ログを保存\n\nプログラム実行時に出力されるログファイル\nエラーメッセージや警告の記録\n処理時間やデータ件数の確認用\n\nOutput：分析結果や成果物を保存\n\n作成されたSASデータセット\nデータ品質チェック結果\n\nPrg：SASプログラムファイル（.sas）を格納\n\nデータ処理プログラム（1_xx、2_xxのように実行するプログラムの順番が分かる方が望ましい）\nデータクリーニングプログラム\n派生変数作成プログラム\n\nSetting：\n\n設定ファイルや共通処理を保存\nよく使用するマクロ定義\nプロジェクト固有の設定\n\nSpec：仕様書や設計書類を格納\n\n解析用データセット仕様書\n\n\n\n\n\n実際のプロジェクトでは、解析用データセット作成プログラミングと、解析プログラミングはフォルダを分けることを推奨しています。\nここでは、解析用データセット作成に焦点を当てたフォルダ構成について詳しく説明します。\n\n\n解析用データセット作成用のフォルダ構成例：\n\nProject/\nInput/\n\nRaw_Data/ # 生データ（CSV、Excel等）\nExternal/ # 外部参照データ、マスタ情報\n\nLog/ # データ処理ログ\nOutput/ # 作成されたSASデータセット\nPrg/ # データ処理プログラム\nSetting/ # 設定ファイル、マクロ\n\n\n\n\nデータ取り込み（PROC IMPORT）\n\n解析用データ仕様書に基づいて、変数のlength,format,labelが入った空データセット（メタデータ）を作成する。\n文字エンコーディングの統一\n変数名の標準化\n\nデータクリーニング\n\n欠損値の処理（削除、補完、フラグ付け）\n重複レコードの確認と処理\nデータ型の統一\n\n派生変数の作成\n\n年齢の計算（生年月日から）\nカテゴリ変数の作成（連続変数の区分化）\n合計値や比率の計算\nフラグ変数の作成\n\n\n\n\n\nSASプログラミングにおいて、プログラムの更新履歴を管理することは非常に重要です。特にデータ処理では、どの時点のプログラムで作成されたデータセットなのかを明確にする必要があります。\n\n\nデータセット作成プログラムの命名例\n\n01_data_import_20250614.sas # 初回作成\n01_data_import_20250615.sas # 修正版\n01_data_import_20250620.sas # 最新版\n02_data_cleaning_20250614.sas # データクリーニング\n03_variable_creation_20250615.sas # 派生変数作成\n04_quality_check_20250620.sas # 品質チェック\n\n\n\n\n各SASプログラムの冒頭には、以下のような標準的なヘッダーを記述することを必須とします。\n/*=================================\nプログラム名: 01_data_import.sas\nプロジェクト: プロジェクト名\n作成者      : 山田太郎\n作成日      : 2025/06/14\n最終更新日  : 2025/06/20\nバージョン  : v1.2\n目的        : 顧客アンケートデータの取り込みとクリーニング\n\n入力データ  : Raw_Data/survey_data.csv\n出力データ  : Output/cleaned_survey.sas7bdat\n\n更新履歴:\nv1.0 2025/06/14 初回作成\nv1.1 2025/06/15 欠損値処理ロジック追加\nv1.2 2025/06/20 外れ値検出機能追加\n=================================*/\n\n\n\n\n*---------------------- EOF （プログラム名.sas） ------------------------------- ;\n\n\n\n\nプログラム実行時に、実行日時をログに記録する仕組みを組み込むことも有効です：\n/* 実行開始時刻を記録 */\n%let start_time = %sysfunc(datetime());\n%put NOTE: データセット作成開始 - %sysfunc(datetime(), datetime19.);\n\n/* データ取り込み処理 */\nproc import datafile=\"Input/Raw_Data/survey_data.csv\"\n    out=work.raw_data\n    dbms=csv replace;\n    getnames=yes;\nrun;\n\n/* 実行終了時刻を記録 */\n%let end_time = %sysfunc(datetime());\n%let elapsed_time = %sysevalf(&end_time - &start_time);\n%put NOTE: データセット作成終了 - %sysfunc(datetime(), datetime19.);\n%put NOTE: 実行時間: %sysfunc(&elapsed_time, time8.);\n%put NOTE: 処理件数: %sysfunc(attrn(open(work.raw_data), nobs));\n\n\n\nログファイルの活用 データ処理では、どのような処理が行われたかを正確に記録することが重要です：\n/* ログファイルの出力先を指定（日付付き） */\n%let today = %sysfunc(today(), yymmddn8.);\nproc printto log=\"Log/data_creation_&today..log\";\nrun;\n\n/* データ処理 */\n/* ... */\n\n/* ログ出力を元に戻す */\nproc printto;\nrun;\n\n\n\nSettingフォルダでは、事前に解析用データセット仕様書のinputするファイル情報やPath、InputデータのPaht、OutputデータのPathを指定しておく。これを作成しておくことで、第3者に解析用データセット作成プログラムを提供した際でも、このSettingフォルダのPathだけを更新することで、再現可能な状態がすぐに作ることができる。\nExcelファイルには以下の3つの要素を設定します。：\n\nパス（Path）：ファイルの保存場所\nファイル名：Excelファイル名\nシート名：参照するワークシート名"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#はじめに",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#はじめに",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "",
    "text": "SAS（Statistical Analysis System）を使ったデータ分析プロジェクトにおいて、最初に決めるべき重要な要素の一つがフォルダ構成です。適切なフォルダ構成を設定することで、プロジェクトの管理が格段に楽になり、チームでの作業効率も向上します。\n多くのSAS初心者は、とりあえずデスクトップやマイドキュメントにファイルを保存してしまいがちですが、プロジェクトが進むにつれて「あのファイルはどこに保存したっけ？」「これは最新バージョン？」といった問題に直面することになります。"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#参考文献",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#参考文献",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "",
    "text": "SASユーザー総会2014年度：PMDAへの承認申請時 CDISC標準電子データ提出に向けた社内標準のリモデリング（塩野義製薬）"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#解析用データセット作成における基本的なフォルダ構成案",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#解析用データセット作成における基本的なフォルダ構成案",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "",
    "text": "SASプログラミングを実施する際は、フォルダ構成をまず設定します。ここでは、実際の業務で使用することができそうな標準的な構成例を紹介します。 任意のプロジェクトフォルダに対して以下のようなフォルダを作成します。テンプレートを作成しておくのが便利でしょう。もしくはProjectフォルダを作成して、自動でフォルダを作成するSASプログラムを準備しておくこともよいかもしれません。ここでの例は、あくまで1つの例であり、より使いやすくなるようなフォルダ構成に変更しても差し支えない。なお、このフォルダ構成はTLF作成のプログラムにおいても引用できる可能性がある。、TLFの解析を踏まえたフォルダ構成は別途提案する。\n\nInput\nLog\nOutput\nPrg\nSetting\nSpec\n\n\n\nInput：分析に使用する元データを格納\n\n外部から受け取ったCSVファイル、Excelファイル -\nデータベースから抽出した生データ\n既存のSASデータセット\n\nLog：SASプログラムの実行ログを保存\n\nプログラム実行時に出力されるログファイル\nエラーメッセージや警告の記録\n処理時間やデータ件数の確認用\n\nOutput：分析結果や成果物を保存\n\n作成されたSASデータセット\nデータ品質チェック結果\n\nPrg：SASプログラムファイル（.sas）を格納\n\nデータ処理プログラム（1_xx、2_xxのように実行するプログラムの順番が分かる方が望ましい）\nデータクリーニングプログラム\n派生変数作成プログラム\n\nSetting：\n\n設定ファイルや共通処理を保存\nよく使用するマクロ定義\nプロジェクト固有の設定\n\nSpec：仕様書や設計書類を格納\n\n解析用データセット仕様書"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#解析用データセット作成に特化したフォルダ構成",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#解析用データセット作成に特化したフォルダ構成",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "",
    "text": "実際のプロジェクトでは、解析用データセット作成プログラミングと、解析プログラミングはフォルダを分けることを推奨しています。\nここでは、解析用データセット作成に焦点を当てたフォルダ構成について詳しく説明します。\n\n\n解析用データセット作成用のフォルダ構成例：\n\nProject/\nInput/\n\nRaw_Data/ # 生データ（CSV、Excel等）\nExternal/ # 外部参照データ、マスタ情報\n\nLog/ # データ処理ログ\nOutput/ # 作成されたSASデータセット\nPrg/ # データ処理プログラム\nSetting/ # 設定ファイル、マクロ\n\n\n\n\nデータ取り込み（PROC IMPORT）\n\n解析用データ仕様書に基づいて、変数のlength,format,labelが入った空データセット（メタデータ）を作成する。\n文字エンコーディングの統一\n変数名の標準化\n\nデータクリーニング\n\n欠損値の処理（削除、補完、フラグ付け）\n重複レコードの確認と処理\nデータ型の統一\n\n派生変数の作成\n\n年齢の計算（生年月日から）\nカテゴリ変数の作成（連続変数の区分化）\n合計値や比率の計算\nフラグ変数の作成"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#プログラムの日付管理とバージョン管理",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#プログラムの日付管理とバージョン管理",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "",
    "text": "SASプログラミングにおいて、プログラムの更新履歴を管理することは非常に重要です。特にデータ処理では、どの時点のプログラムで作成されたデータセットなのかを明確にする必要があります。\n\n\nデータセット作成プログラムの命名例\n\n01_data_import_20250614.sas # 初回作成\n01_data_import_20250615.sas # 修正版\n01_data_import_20250620.sas # 最新版\n02_data_cleaning_20250614.sas # データクリーニング\n03_variable_creation_20250615.sas # 派生変数作成\n04_quality_check_20250620.sas # 品質チェック\n\n\n\n\n各SASプログラムの冒頭には、以下のような標準的なヘッダーを記述することを必須とします。\n/*=================================\nプログラム名: 01_data_import.sas\nプロジェクト: プロジェクト名\n作成者      : 山田太郎\n作成日      : 2025/06/14\n最終更新日  : 2025/06/20\nバージョン  : v1.2\n目的        : 顧客アンケートデータの取り込みとクリーニング\n\n入力データ  : Raw_Data/survey_data.csv\n出力データ  : Output/cleaned_survey.sas7bdat\n\n更新履歴:\nv1.0 2025/06/14 初回作成\nv1.1 2025/06/15 欠損値処理ロジック追加\nv1.2 2025/06/20 外れ値検出機能追加\n=================================*/\n\n\n\n\n*---------------------- EOF （プログラム名.sas） ------------------------------- ;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#自動的な実行日時記録",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#自動的な実行日時記録",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "",
    "text": "プログラム実行時に、実行日時をログに記録する仕組みを組み込むことも有効です：\n/* 実行開始時刻を記録 */\n%let start_time = %sysfunc(datetime());\n%put NOTE: データセット作成開始 - %sysfunc(datetime(), datetime19.);\n\n/* データ取り込み処理 */\nproc import datafile=\"Input/Raw_Data/survey_data.csv\"\n    out=work.raw_data\n    dbms=csv replace;\n    getnames=yes;\nrun;\n\n/* 実行終了時刻を記録 */\n%let end_time = %sysfunc(datetime());\n%let elapsed_time = %sysevalf(&end_time - &start_time);\n%put NOTE: データセット作成終了 - %sysfunc(datetime(), datetime19.);\n%put NOTE: 実行時間: %sysfunc(&elapsed_time, time8.);\n%put NOTE: 処理件数: %sysfunc(attrn(open(work.raw_data), nobs));"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#実践的な運用のコツ",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#実践的な運用のコツ",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "",
    "text": "ログファイルの活用 データ処理では、どのような処理が行われたかを正確に記録することが重要です：\n/* ログファイルの出力先を指定（日付付き） */\n%let today = %sysfunc(today(), yymmddn8.);\nproc printto log=\"Log/data_creation_&today..log\";\nrun;\n\n/* データ処理 */\n/* ... */\n\n/* ログ出力を元に戻す */\nproc printto;\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#settingフォルダについて",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#settingフォルダについて",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "",
    "text": "Settingフォルダでは、事前に解析用データセット仕様書のinputするファイル情報やPath、InputデータのPaht、OutputデータのPathを指定しておく。これを作成しておくことで、第3者に解析用データセット作成プログラムを提供した際でも、このSettingフォルダのPathだけを更新することで、再現可能な状態がすぐに作ることができる。\nExcelファイルには以下の3つの要素を設定します。：\n\nパス（Path）：ファイルの保存場所\nファイル名：Excelファイル名\nシート名：参照するワークシート名"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#attrib-statement",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#attrib-statement",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "2.1 Attrib statement",
    "text": "2.1 Attrib statement\n具体的なプログラミングについては、別記事で解説をするが、ここでは解析用データセットを作成する上で大事なAttrib Statementについて解説する。\n以下記事が参考になる。\n\nATTRIBステートメント\n変数属性を定義した空のデータセットを作成する方法【まとめ】\n\n解析用データ仕様書にて、各変数のLabel、Length、formatを指定する必要がある。 その際に、解析用データセットに対してattrib statementを適用することで簡単に設定できる。\nちなみに、変数の順番だけを入れ替えるならばformat Statementでも簡単にできる。こちらのブログが参考になります。\ndata ADSL;\n    set ADSL;\n    attrib \n        STUDYID   label=\"Study Identifier\"           length=$12  format=$12.\n        USUBJID   label=\"Unique Subject Identifier\"  length=$40  format=$40.\n        SUBJID    label=\"Subject Identifier\"         length=$20  format=$20.\n        AGE       label=\"Age\"                        length=8    format=8.\n        SEX       label=\"Sex\"                        length=$1   format=SEX.\n        TRT01P    label=\"Planned Treatment for Period 1\"  length=$200  format=$200.\n        TRT01A    label=\"Actual Treatment for Period 1\"   length=$200  format=$200.\n        TRT01PN   label=\"Planned Treatment for Period 1 (N)\"  length=8  format=8.\n        TRT01AN   label=\"Actual Treatment for Period 1 (N)\"  length=8  format=8.\n        FASFL     label=\"Full Analysis Set Flag\"     length=$1   format=NYFL.\n    ;\nrun;\n実際は手入力ですることは人為的ミスの元となるのでマクロ化等で自動化することを推奨するが考え方は上記の通りである。"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#proc-format",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#proc-format",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "2.2 Proc format",
    "text": "2.2 Proc format\n以下記事が参考になる。 - PROC FORMAT入門1 : VALUEステートメント\nPROC FORMATは、SASにおいてユーザー定義フォーマットを作成するプロシージャです。数値や文字データを、より読みやすい形式に変換して表示することができます。 基本構文は以下の通りです。\nproc format;\n    value フォーマット名\n        値1 = \"ラベル1\"\n        値2 = \"ラベル2\"\n        値3 = \"ラベル3\";\nrun;\n文字フォーマットには先頭にドルマークを付けます。\nproc format;\n    value trtf\n        1 = \"プラセボ\"\n        2 = \"低用量\"\n        3 = \"高用量\"\n        . = \"欠測\";\n        \n    value sexf\n        1 = \"男性\"\n        2 = \"女性\"\n        . = \"不明\";\n        \n    value nyf\n        0 = \"No\"\n        1 = \"Yes\"\n        . = \"Missing\";\nrun;\n\nproc format;\n    value $sexf\n        \"M\" = \"男性\"\n        \"F\" = \"女性\"\n        \" \" = \"不明\";\n        \n    value $countryfmt\n        \"JPN\" = \"日本\"\n        \"USA\" = \"アメリカ合衆国\"\n        \"GBR\" = \"イギリス\"\n        other = \"その他\";\nrun;"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#データセットからのフォーマット作成",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#データセットからのフォーマット作成",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "2.3 データセットからのフォーマット作成",
    "text": "2.3 データセットからのフォーマット作成\n\n2.3.1 3.1 CNTLIN=オプションの使用\nデータセットからフォーマットを作成する場合、特定の変数名を持つデータセットを準備する必要があります。 必要な変数：\n\nFMTNAME：フォーマット名\nSTART：変換前の値（開始値）\nEND：変換前の値（終了値、範囲指定時）\nLABEL：変換後のラベル\nTYPE：フォーマットタイプ（‘N’=数値、‘C’=文字）\n\n/* 治療群フォーマット用データセット */\ndata trt_fmt;\n    retain fmtname 'trtf' type 'N';\n    input start end label $20.;\n    datalines;\n1 1 プラセボ\n2 2 低用量\n3 3 高用量\n. . 欠測\n;\nrun;\n\n/* 性別フォーマット用データセット */\ndata sex_fmt;\n    retain fmtname '$sexf' type 'C';\n    input start $1. end $1. label $10.;\n    datalines;\nM M 男性\nF F 女性\n   不明\n;\nrun;\n\n/* フォーマットの作成 */\nproc format cntlin=trt_fmt;\nrun;\n\nproc format cntlin=sex_fmt;\nrun;\nPROC FORMATは以下の2つの方法でフォーマットを作成できます：\nVALUEステートメント：直接コードに記述する方法。シンプルで直感的 CNTLIN=オプション：データセットから作成する方法。大量のフォーマットや動的な作成に適している\n適切なフォーマットの使用により、データの可読性が大幅に向上し、レポート作成時の効率も改善されます。"
  },
  {
    "objectID": "posts/statistics/2025/解析用データセット作成の流れ1.html#sasプログラムの例",
    "href": "posts/statistics/2025/解析用データセット作成の流れ1.html#sasプログラムの例",
    "title": "解析用データセット作成の流れ1（フォルダ構造等）",
    "section": "3.1 SASプログラムの例",
    "text": "3.1 SASプログラムの例\n以下のプロジェクトは人間が主導でProjectフォルダを適当な場所に作成して、そのフォルダにて以下のプログラムを実行するだけで、上記のフォルダ構造を作成するものである。\nproc datasets kill nolist ; run ; quit ; \ndm 'out ; clear ; log ; clear ;' ;\n\n/***********************************************************************\n* Project         : 臨床研究の統計解析プログラミング\n* Program name    : 01_Folder_Setting.sas\n* Author          : Kota Sakamoto\n* Date created    : 20250617\n* Purpose         :　プロジェクト開始時のフォルダ構造の標準化\n* Revision History :\n***********************************************************************/\n\n/* 1. /* --- 日付設定 --- */ */;\ndata _null_;\n   dt = datetime();\n   Date1 = put(datepart(dt), yymmdds10.);\n   Date2 = compress(Date1, \"/\");\n   Time1 = put(timepart(dt), tod8.);\n   Time2 = compress(Time1, \":\");   \n   call symputx('StDates', Date1);\n   call symputx('StDate', Date2);\n   call symputx('StTimes', Time1);\n   call symputx('StTime', Time2);\nrun;\n\n%put 日付: &StDates. (&StDate.) 時刻: &StTimes. (&StTime.);\n\n/* 2. /* --- フォルダのマクロ変数の取得 --- */ */;\n%LET execpath=\" \";\n%MACRO setexecpath;\n    %LET execpath=%SYSFUNC(GETOPTION(SYSIN));\n    %IF %LENGTH(&execpath)=0\n    %THEN %LET execpath=%SYSGET(SAS_EXECFILEPATH);\n%MEND setexecpath;\n%setexecpath;\n%PUT &execpath;\n\n%let CURRENT_DIR = %qsubstr(\"&execpath.\", 2, %eval(%index(\"&execpath.\", %scan(&execpath, -1, \"\\\")))-2) ;\n%put &CURRENT_DIR;\n\n\n/* 3. /* --- フォルダのlibnameの指定 --- */ */;\n%let folder1 = Document;\n%let folder2 = ADS_Program;\n%let folder3 = ADS_Program\\Input;\n%let folder4 = ADS_Program\\Input\\Raw;\n%let folder5 = ADS_Program\\Input\\External;\n%let folder6 = ADS_Program\\Log;\n%let folder7 = ADS_Program\\Output;\n%let folder8 = ADS_Program\\Prg;\n%let folder9 = ADS_Program\\Prg\\Develop;\n%let folder10 = ADS_Program\\Prg\\Fix;\n%let folder11 = ADS_Program\\Macro;\n%let folder12 = ADS_Program\\Setting;\n%let folder13 = ADS_Program\\Spec;\n%let folder14 = Analysis_Program;\n%let folder15 = Analysis_Program\\Input;\n%let folder16 = Analysis_Program\\Log;\n%let folder17 = Analysis_Program\\Output;\n%let folder18 = Analysis_Program\\Prg;\n%let folder19 = Analysis_Program\\Prg\\Develop;\n%let folder20 = Analysis_Program\\Prg\\Fix;\n%let folder21 = Analysis_Program\\Macro;\n%let folder22 = Analysis_Program\\Setting;\n%let folder23 = Analysis_Program\\Spec;\n%let folder24 = Paper;\n\n\n/* まとめて一気に実施する */\n%macro create_folder_paths;\n    data _null_;\n        %do i = 1 %to 24;\n            folder&i = cat(\"&CURRENT_DIR\", \"\\\", \"&&folder&i..\");\n            call symputx(\"folder&i\", folder&i);\n        %end;\n    run;\n%mend;\n\n%create_folder_paths;\n%put _user_;\n    \n\n/* 4. /* --- フォルダ作成 --- */ */;\n/* ディレクトリ自動作成マクロ */\n%macro create_dir(path);\n   %local parent_dir dir_name;\n   %let path = %sysfunc(strip(&path));\n   %let parent_dir = %substr(&path, 1, %length(&path)-%length(%scan(&path, -1, '\\')));\n   %let dir_name = %scan(&path, -1, '\\');\n   \n   %if %sysfunc(fileexist(&path)) = 0 %then %do;\n       %if %length(&parent_dir) &gt; 0 %then %do;\n           %if %sysfunc(fileexist(&parent_dir)) = 0 %then %do;\n               %create_dir(&parent_dir);\n           %end;\n       %end;\n       %let rc = %sysfunc(dcreate(&dir_name, &parent_dir));\n   %end;\n%mend;\n\n/* プロジェクトフォルダ構造の自動作成 */\n%macro create_all_folders;\n   %do i = 1 %to 24;\n       %create_dir(&&folder&i);\n   %end;\n%mend;\n\n%create_all_folders;\n\n/* 5. /* --- 解析用データセットと解析プログラムについて、開発日付を逐次作成する --- */ */;\ndata _null_;\n    /* 各フォルダパスを作成 */\n    Prg_Develop1= cat( \"&CURRENT_DIR\",\"ADS_Program\\Prg\\Develop\",\"\\\", \"&StDate\");\n    Prg_Develop2  = catx( \"&CURRENT_DIR\",\"Analysis_Program\\Prg\\Develop\",\"\\\", \"&StDate\");\n    \n    /* マクロ変数に格納 */\n    call symputx(\"Prg_Develop1\", Prg_Develop1);\n    call symputx(\"Prg_Develop2\", Prg_Develop2);\n  \nrun;\n\n/* 作成されたパスを確認 */\n%put &Prg_Develop1;\n%put  &Prg_Develop2;\n\n%create_dir(&Prg_Develop1);\n%create_dir(&Prg_Develop2);"
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html",
    "href": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html",
    "title": "Using simulation studies to evaluate statistical methods",
    "section": "",
    "text": "ノート\n\n\n\nSimulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement."
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#abstract",
    "href": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#abstract",
    "title": "Using simulation studies to evaluate statistical methods",
    "section": "",
    "text": "ノート\n\n\n\nSimulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement."
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#introduction",
    "href": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#introduction",
    "title": "Using simulation studies to evaluate statistical methods",
    "section": "2 Introduction",
    "text": "2 Introduction\nSimulation studies are computer experiments that involve creating data by pseudo-random sampling from known probability distributions. They are an invaluable tool for statistical research, particularly for the evaluation of new methods and for the comparison of alternative methods. Simulation studies are much used in the pages of Statistics in Medicine, but our experience is that some statisticians lack the necessary understanding to execute a simulation study with confidence, while others are overconfident and so fail to think carefully about design and report results poorly. Proper understanding of simulation studies would enable the former to both run and critically appraise published simulation studies themselves and the latter to conduct simulation studies with greater care and report with transparency. Simulation studies are empirical experiments, and statisticians should therefore use knowledge of experimental design and analysis in running them. As we shall see, inadequacies with design, analysis, and reporting lead to uncritical use and interpretation of simulation studies. In this context, better understanding of the rationale, design, execution, analysis, and reporting of simulation studies is necessary to improve understanding and interpretation of the findings.\n　Simulation studies are used to obtain empirical results about the performance of statistical methods in certain scenarios, as opposed to more general analytic (algebraic) results, which may cover many scenarios. It is not always possible, or may be difficult, to obtain analytic results. Simulation studies come into their own when methods make wrong assumptions or data are messy because they can assess the resilience of methods in such situations. This is not always possible with analytic results, where results may apply only when data arise from a specific model. “Monte Carlo simulation” means statistical techniques that use pseudo-random sampling, and has many uses that are not simulation studies. For example, it is required to implement multiple imputation and Markov Chain Monte Carlo methods. The remainder of this paper does not consider these uses, unless the properties of some such method are being evaluated by a simulation study. There are many ways to use simulation studies in medical statistics. Some examples are:\n\nTo check algebra (and code), or to provide reassurance that no large error has been made, where a new statistical method has been derived mathematically.\nTo assess the relevance of large-sample theory approximations (eg, considering the sampling distribution of an estimator) in finite samples.\nFor the absolute evaluation of a new or existing statistical method. Often a new method is checked using simulation to ensure it works in the scenarios for which it was designed.\nFor comparative evaluation of two or more statistical methods.\nFor calculation of sample size or power when designing a study under certain assumptions.1\n\nThis article is focused primarily on using simulation studies for the evaluation of methods. Simulation studies for this purpose are typically motivated by frequentist theory and used to evaluate the frequentist properties of methods, even if the methods are Bayesian.2,3 It seems that as a profession we fail to follow good practice regarding design, analysis, presentation and reporting in our simulation studies, as lamented previously by Hoaglin and Andrews,4 Hauck and Anderson,5 Ripley,6 Burton et al,7 and Koehler et al.8 For example, few reports of simulation studies acknowledge that Monte Carlo procedures will give different results when based on a different set of random numbers and hence are subject to uncertainty, yet failing to report measures of uncertainty would be unacceptable in medical research. There exist some wonderful books on simulation methods in general6,9,10 and several excellent articles encouraging rigor in specific aspects of simulation studies,1,4,5,8,11-16 but until now, no unified practical guidance on simulation studies. This tutorial provides such guidance. More specifically, we: introduce a structured approach for planning and reporting simulation studies; provide coherent terminology for simulation studies; offer guidance on coding simulation studies; critically discuss key performance measures and their estimation; make suggestions for structuring tabular and graphical presentation of results; and introduce several new graphical presentations. This guidance should enable practitioners to execute a simulation study for the first time and contains much for more experience practitioners. For reference, the main steps involved, key decisions and recommendations are summarised in Table 1. The structure of this tutorial is as follows.\nWe describe a review of a sample of the simulation studies reported in Statistics in Medicine Volume 34 (Section 2).\nIn Section 3, we outline a systematic approach to planning simulation studies, using the new “ADEMP” structure (which we define there). Section 4 gives guidance on computational considerations for coding simulation studies.\nIn Section 5, we discuss the purposes of various performance measures and their estimation, stressing the importance of estimating and reporting the Monte Carlo standard error (SE) as a measure of uncertainty due to using a finite number of simulation repetitions.\nSection 6 outlines how to report simulation studies, again using the ADEMP structure, and offers guidance on tabular and graphical presentation of results.\nSection 7 works through a simple simulation to illustrate in practice the approaches that we are advocating.\nSection 8 offers some concluding remarks, with a short Section 8.1 that considers some future directions. Examples are drawn from the review and from the authors’ areas of interest (which relate mainly to modeling survival data, missing data, meta-analysis, and randomised trial design)."
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#section2simulation-in-practice-a-review-of-statistics-in-medicine-volume-34",
    "href": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#section2simulation-in-practice-a-review-of-statistics-in-medicine-volume-34",
    "title": "Using simulation studies to evaluate statistical methods",
    "section": "3 Section2：SIMULATION IN PRACTICE: A REVIEW OF STATISTICS IN MEDICINE, VOLUME 34",
    "text": "3 Section2：SIMULATION IN PRACTICE: A REVIEW OF STATISTICS IN MEDICINE, VOLUME 34\nWe undertook a review of practice based on articles published in Volume 34 of Statistics in Medicine (2015). This review recorded information relevant to the ideas in this article. In this section, we briefly outline the review but do not give results, which instead are provided at relevant points. The raw data on which results are based are provided as a Stata file in the supplementary materials (see file “volume34reviewdata.dta”; pared down, without comments).\nWe restricted attention to research articles, excluding tutorials in biostatistics, commentaries, book reviews, corrections, letters to the editor and authors’ responses. In the volume, there were a total of 264 research articles of which 199 (75%) included at least one simulation study. In planning the review, we needed to select a sample size. Most of the questions of interest involved binary answers. For such questions, to estimate proportions with maximum standard error of 0.05 (occurring when the proportion is 0.5), we randomly selected 100 articles that involved a simulation study, before randomly assigning articles to a reviewer. TPM reviewed 35 simulation studies, IRW reviewed 34 and MJC reviewed 31. In case the reviewer was an author or coauthor of the article, the simulation study was swapped with another reviewer. TPM also reviewed five of the simulation studies allocated to each of the other reviewers to check agreement on key information (results on agreement are given in the Appendix and Figure A1)."
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#section3planning-simulation-studies-using-ademp",
    "href": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#section3planning-simulation-studies-using-ademp",
    "title": "Using simulation studies to evaluate statistical methods",
    "section": "4 Section3:PLANNING SIMULATION STUDIES USING ADEMP",
    "text": "4 Section3:PLANNING SIMULATION STUDIES USING ADEMP\nFor clarity about the concepts that will follow, we introduce some notation in Table 2. Note that θ is used to represent a connceptual estimand and its true value.\n\nIn the following sections, we outline the ADEMP structured approach to planning simulation studies.\nThis acronym comes from: Aims, Data-generating mechanisms, Methods, Estimands, Performance measures.\n\n4.1 3.1 Aims\nIn considering the aims of a simulation study, it is instructive to first consider desirable properties of an estimator θ̂ from a frequentist perspective.\n\n\\hat \\theta should be consistent: as n → ∞, \\hat \\theta → \\theta. It is also desirable that \\hat \\theta be unbiased for \\theta in finite samples: E[\\theta]=\\theta (though arguably less important since unbiasedness is not an invariant property). Some estimators may be consistent but exhibit small-sample bias (logistic regression for example).\nThe sample estimate \\hat V[\\theta] should be a consistent estimate of the sampling variance of \\hat \\theta (see, for example, the work of Kenward and Roger.18)\nConfidence intervals should have the property that at least 100(1 − α)% of intervals contain θ (see Section 5.2).\nIt is desirable that \\hat V[\\theta] be as small as possible: that \\hat \\theta be an efficient estimator of \\theta.\n\nThere are other properties we might desire, but these tend to involve combinations of the above. For example, short average confidence interval length may be desirable; this relates to (4) and its validity depends on (1), (2), and (3). Mean squared error is a combination of (1) and (4). Further, properties may be traded off; small bias may be accepted if there is a substantial reduction in Var ̂(θ). The aims of a simulation study will typically be set out in relation to the above properties, depending on what specifically we wish to learn. A simulation study might primarily investigate: large- or small-sample bias (eg, see the work of White19); precision, particularly relative to other available methods (eg, see the work of White20); Variance estimation (eg, see the work of Hughes et al21); or robustness to misspecification (eg, see the work of Morris et al22). There is a distinction between simulation studies that offer a proof-of-concept, ie, showing that a method is viable (or fallible) in some settings, and those that aim to stretch or break methods, ie, identifying settings where the method may fail. Both are useful and important in statistical research. For example, one may be faced with two competing methods of analysis, both of which are equally easy to implement. Even if the choice is unlikely to materially affect the results, it may be useful to have unrealistically extreme data-generating mechanisms to understand when and how each method fails.22 Alternatively, it may be of interest to compare methods where some or all methods have been shown to work in principle but the methods under scrutiny were designed to address slightly different problems. They may be put head-to-head in realistic scenarios. This could be to investigate properties when one method is correct – How badly do others fail? – or when all are incorrect in some way – Which is most robust? No method will be perfect, and it is useful to understand how methods are likely to perform in the sort of scenarios that might be expected in practice. However, such an approach poses tough questions in terms of generating data: Does the data-generating mechanism favor certain methods over others? How can this be checked and justified? One common justification is by reference to motivating data. However, in the absence of a broad spectrum of such motivating data, there is a risk of failing to convince readers that a method is fit for general use.\n\n\n4.2 3.2 Data-generating mechanisms\nWe use the term “data-generating mechanism” to denote how random numbers are used to generate a dataset. This is in preference to “data-generating model,” which implies parametric models and so is a specific class of data-generating mechanism. It is not the purpose of this article to explain how specific types of data should be generated. See Ripley6 or Morgan9 for methods to simulate data from specific distributions. In planning a simulation study, it is usual to spend more time deciding on data-generating mechanisms than any other element of ADEMP. There are many subtleties and potential pitfalls, some of which we will mention below. Data may be generated by producing parametric draws from a known model (once or many times), or by repeated resampling with replacement from a specific dataset (where the true data-generating model is unknown). For resampling studies, the true data-generating mechanism is unknown and resamples are used to study the sampling distribution. While parametric simulation can explore many different data-generating mechanisms (which may be completely unrealistic), resampling typically explores only one mechanism (which will be relevant for at least the study at hand). The choice of data-generating mechanism(s) will depend on the aims. As noted above, we might investigate a method under a simple data-generating mechanism, a realistic mechanism, or a completely unrealistic mechanism designed to stretch a method to breaking point.\nSimulation studies provide us with empirical results for specific scenarios. For this reason, simulation studies will often involve more than one data-generating mechanism to ensure coverage of different scenarios. For example, it is very common to vary the sample size of simulated datasets because performance often varies over nobs (see Section 5.2). Much can be controlled in a simulation study and statistical principles for designing experiments therefore can and should be called on.\nIn particular, there is often more than one factor that will vary across specific data-generating mechanisms. Factors that are frequently varied are sample size (several values) and true parameter values (for example, setting one or more parameters to be zero or nonzero). Varying these factorially is likely to be more informative than one-by-one away from a “base-case” data-generating mechanism, as doing so permits the exploration of interactions between factors. There are however practical implications that might make this infeasible. The first regards presentation of results (covered in Section 6) and the second computational time. If the issue is simply around presentation, it may be preferable to define a “base case” but perform a factorial simulation study anyway, and if results are consistent with no interaction, presentation can vary factors away from the base case one-by-one. If the main issue with executing a fully factorial design is computational time, it may be necessary for the simulation study to follow a non factorial structure. Three approaches are noted below.\nA first pragmatic check may be to consider interactions only where main effects exist. If performance seems acceptable and does not vary according to factor A, it would seem unlikely to have chosen a data-generating mechanism that happened to exhibit this property when performance would have been poor for other choices of data-generating mechanism.\nA more careful approach could be taken based on making and checking predictions beyond the data-generating mechanisms initially used; an idea similar to external validation. Suppose we have two factors, A and B, where A ∈ {1, … , 8} and B ∈ {1, … , 5} in the data-generating mechanism. The base-case is A = 1, B = 1. If the nonfactorial portion of the design varies A from 1 to 8 holding B = 1, and varies B from 1 to 5 holding A = 1, this portion of the simulation study could be used to predict performance when A = 8, B = 5. Predictions may be purely qualitative (“bias increases as A increases and as B increases, so when we increase both together, we would expect even larger bias”), or quantitative (based on the marginal effects after fitting a model to existing results, thereby producing explicit predictions at unexplored values of A and B). The simulation study can then be re-run for that single data-generating mechanism, say A = 8, B = 5 and predictions compared with the empirical results (with a responsibility to expore further when predictions are poor or incorrect). Finally, a more satisfactory solution is of course to use a fractional factorial design for the data-generating mechanisms.3,23\nWe now issue some specific pitfalls to help readers in choosing data-generating mechanisms (specifically acknowledg- ing Stephen Senn’s input).\n\nResampling with replacement from a dataset but failing to appreciate that results are relevant to an infinite population with the exact characteristics of that dataset. For example, if a trial had a nonsignificant result, the treatment effect is nonzero in the implicit population.24\nMissing the distinction between the logical flow of Bayesian and frequentist simulation. Repeated simulation with a single parameter value is explicitly frequentist. The fact that θ̂ is on average equal to θ does not imply that θ is on average equal to θ̂.\nFailing to distinguish between what the simulator can know and what the estimator can know.25\nEmploying tricks in data-generation without appreciating that the resulting data are not what was desired. As an example, suppose one wishes to simulate bivariate data with a desired R2, say 0.3. For any given repetition, the observed R2 will not equal 0.3, but this could be fixed by scaling the residuals. This would produce unintended side effects for other statistics.\n\nIn our review, 97 simulation studies used some form of parametric model to generate data while three used resampling methods. Of the 97 that simulated from a parametric model, 27 based parameter values on data, one based parameter values partly on data, and the remaining 69 on no data. Of these 97, 91 (94%) provided the parameters used. The most careful example26 explored analysis of meta-analysis data and drew the design factors from empirical data on 14,886 performed meta-analyses from 1,991 Cochrane Reviews. The total number of data-generating mechanisms per simulation study ranged from 1 to 4.2 × 1010; Figure A2 (in the Appendix) summarises aspects of the data-generating mechanisms. Where more than one factor was varied, fully factorial designs were the most frequent, while some used partially factorial designs. None used any of the alternative approaches we have described.\n\n\n4.3 3.3 Estimands and other targets\nThe majority of simulation studies evaluate or compare methods for estimating one or more population quantities, which we term estimands and denote by θ. An estimand is usually a parameter of the data generating model, but is occasionally some other quantity. For example, when fitting regression models with parameter β = (β0 … βc), the estimand may be a specific β, a measure of prognostic ability, the fitted outcome mean, or something else. In order to choose a relevant estimand, it is important to understand the aims of analysis in practice. The choice of estimand is sometimes a simple matter of stating a parameter of interest. At other times, it is more subtle. For example, a logistic regression model unadjusted for covariates implies a marginal estimand; a model adjusted for covariates implied a conditional estimand with a different true value (this example is expanded on in Section 3.4). Not all simulation studies evaluate or compare methods that concern an estimand. Other simulation studies evaluate methods for testing a null hypothesis, for selecting a model, or for prediction. We refer to these as targets of the simulation study. The same statistical method could be evaluated against multiple targets. For example, the best method to select a regression model to estimate the coefficient of an exposure (targeting an estimand) may differ from the best model for prediction of outcomes (targeting prediction). Where a simulation study evaluates methods for design, rather than analysis, of a biomedical study, the design is the target. Table 3 summarises different possible targets of a simulation study and suggests some performance measures (described more fully in Section 3.5) that may be relevant for each target, with examples taken from Volume 34.\nIn our review, 64 simulation studies targeted an estimand, 21 targeted a null hypothesis, eight targeted a selected model, three targeted predictive performance, and four had some other target. Of the 64 targeting an estimand, 51 stated what the estimand was (either in the description of the simulation study or elsewhere in the article). A figure detailing the number of estimands in simulation studies that targeted an estimand is given in the Appendix, Figure A3.\n\n\n\n4.4 3.4 Methods\nThe term “method” is generic. Most often it refers to a model for analysis, but might refer to a design or some procedure (such as a decision rule). For example, Kahan31 and Campbell and Dean32 evaluated procedures that involved choosing an analysis based on the result of a preliminary test in the same data. In some simulation studies, there will be only one method with no comparators. In this case, selecting the method to be evaluated is very simple. When we aim to compare several methods in order to identify the best, it is important to include serious contenders. There are two issues. First, it is necessary to have knowledge of previous work in the area to understand which methods are and are not serious contenders. Some methods may be legitimately excluded if they have already been shown to be flawed, and it may be unnecessary to include such methods if the only consequences are repetition of previous research and bloating\n\n\n4.5 3.5 Performance measures\nThe term “performance measure” describes a numerical quantity used to assess the performance of a method. The equivalent term “operating characteristic” is sometimes used, particularly in the context of study designs (see, for example, the work of Royston et al39). Statistical methods for estimation may output for example an estimate \\hat \\theta_i , an estimate of variance \\hat V[\\hat \\theta_i] (or standard error SÊ(θ̂)i), degrees of freedom, confidence intervals, test statistics, and more (such as an estimate of prognostic performance). The performance measures required in a simulation study depend on the aims and what the study targets (see Section 3.3). When the target is an estimand, the most obvious performance measure to consider is bias: the amount by which \\hat \\theta exceeds θ on average (this can be positive or negative). Precision and coverage of (1 − α ) confidence intervals will also be of interest. Meanwhile, if the target is a null hypothesis, power and type I error rates will be of primary interest. A simulation study targeting an estimand may of course also assess power and type I error. The performance measures seen in our review are summarised in Table 4. The denominator changes according across performance measures because some are not applicable for some simulation studies. Further, sometimes simulation studies had secondary targets. For example, nine simulation studies primarily targeted a null hypothesis but secondarily targeted an estimand and could have assessed bias, and one of these did so. For eight articles, some performance measures were unclear. In some, a performance measure was given a name that its formula demonstrated to be misleading (an example is the term “mean error,” which is bias, when the formula is for mean absolute error), emphasizing the importance of clear terminology in simulation studies.\nDescription and estimation of common performance measures of interest are given in Section 5. An important point to appreciate in design and analysis is that simulation studies are empirical experiments, meaning performance measures are themselves estimated, and estimates of performance are thus subject to error. This fundamental feature of simulation studies does not seem to be widely appreciated, as previously noted.6 The implications are two-fold. First, we should present estimates of uncertainty (quantified as the Monte Carlo standard error; see Section 5.2). Second, we need to consider the number of repetitions nsim and how this can be chosen (see Section 5.2)."
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#computational-and-programming-issues-in-simulation-studies",
    "href": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#computational-and-programming-issues-in-simulation-studies",
    "title": "Using simulation studies to evaluate statistical methods",
    "section": "5 4 COMPUTATIONAL AND PROGRAMMING ISSUES IN SIMULATION STUDIES",
    "text": "5 4 COMPUTATIONAL AND PROGRAMMING ISSUES IN SIMULATION STUDIES\nIn this section, we discuss consideration when coding a simulation study. It is useful to understand what sort of data are involved. There may be up to four classes of dataset, listed and described in Table 5.\n\n5.1 4.1 Random numbers: setting seeds and storing states\nAll statistical packages capable of Monte Carlo simulation use a pseudo-random-number generator. Each random number is a deterministic function of the current “state” of the random-number generator. After a random number is produced,the state changes, ready to produce the next random number. Because the function is deterministic, the state can be set.Typically, the state is set using a “seed.” Seeds do not necessarily map 1:1 to states and provide doors onto the path of possible states. After enough random-number draws (a very large number in software using modern pseudo-random-number generators), the state will eventually repeat: the path is circular. The “pseudo” element to random-number generators is sometimes characterised as negative. This is perhaps an artefact of the fact that some early algorithms provided very poor imitations of random numbers. However, modern-era algorithms such as the Mersenne Twister do not suffer from these problems and can, for simulation purposes, be regarded as truly random when used correctly. The toss of a coin or roll of a die may be regarded as equally deterministic, albeit the result of a complex set of unknown factors that act in an uncontrollable fashion. These are not denigrated with the term “pseudo-random”: in statistical teaching, they are often given as the ultimate example of randomness. However, many stage magicians can control the flip of a coin! If a computer pseudo-random number generator is sufficiently unpredictable and passes the various tests for randomness, it is churlish to regard the “pseudo” aspect as a weakness.\nThere are several positive implications of using a deterministic and reproducible process for generating random num- bers. First, if the number of repetitions is regarded as insufficient, the simulation study can continue from its end state.\nSecond and more importantly, if a certain repetition results in some failure such as nonconvergence, the starting state for that repetition can be noted and the repetition re-run under that state, enabling better understanding of when the method\ndoes not work so that issues leading to nonconvergence can be tackled. Finally, the whole simulation study can be inde- pendently run by other researchers, giving the potential for exact (rather than approximate) reproduction of results and\nthe scope for additional methods to be included. Our practical advice for utilizing the deterministic nature of random-number generators is simple but strong: (1) set the seed at the beginning, once and only once; (2) store the state of the random-number generator often (ideally once at the beginning of each repetition and once following repetition i = nsim). This is important; the following chunk of pseudocode demonstrates the concept:\nThe reason for this advice is to avoid unintended dependence between simulated datasets. We will illustrate our caution: one undesirable method of knowing the states for nsim repetitions is to set an initial seed and generate a single vector of length nsim by recording the starting state, generating a single random number, recording the new state, and so on. For the simulation itself, the seed for the ith repetition is then set to the ith element. To clarify the problem, let nobs = nsim = 4 and let the first simulation step be generation of vector x from a Uniform(0,1) distribution. The first repetition simulates x1 (which changes the random number state four times) and proceeds. The second repetition then simulates x2, which is made up of observations 2 to 4 from repetition i = 1 and just one new value. Run in Stata 15 (see supplementary material, ie, file “corrstates.do”), the resulting draws of x for the four repetitions are: Note that elements with the same shading contain the same values across rows. The fourth element of x1 is the first element of x4 and appears in all repetitions. Only when i &gt; nobs is the draw of x actually independent of the first repetition. Such dependency in simulated data can compromise both performance estimates and Monte Carlo SEs and must be avoided.\n\n\n5.2 4.1.1 “Stream” random numbers\nIt is common for parts of simulation studies – fractions of all the repetitions, for example – to be run in parallel on different cores of high-performance computers (which this article will not mention further). If the advice to set the seed once only is followed, the implication for parallelisation is that, while runs for different data-generating mechanisms may be parallelised, it is inadvisable to parallelise repetitions within a specific data-generating mechanism.\nSuppose we wish to parallelise two sets of nsim∕2 repetitions. Any simulation study will use random numbers (in order) from a section of the circle. Here, each set of repetitions is represented by a clockwise arrow, and uses 80◦ of the total 360◦ of random numbers available in the full circle (a caricature for illustrative purposes; in practice, a much smaller fraction would be used). The seed dictates the position on the circle at which an arrow begins (and thus ends). The random numbers used up by the first nsim∕2 repetitions are represented by the red arrow and for the second nsim by the blue arrow. The left circle depicts two chunks run in parallel with two different, arbitrarily-chosen starting seeds. By chance, they may overlap as seen. This would be a cause for concern. The right circle uses separate streams of random numbers. This breaks the circle into quadrants, and setting the same value of a seed within a stream means that the separate chunks will start at the equivalent point on the quadrants and here there is no chance that one stream will enter another. In the absence of streams, repetitions should not be parallelised for the same data-generating mechanism. In Stata (version 15 or newer), the stream is set with . set rngstream # prior to setting the seed. In SAS, it is achieved within a data step with . call stream(#); In R, this can be achieved with the rstream package. Regardless of the package, the same seed must be used within different values of #. When a simulation study uses multiple data-generating mechanisms, these may be run in parallel. Because performance is typically estimated separately for different data-generating mechanisms, using the same seeds is less of a problem (and may in fact be advantageous, as described in Section 5.4). Many programs execute methods involving some stochastic element. Examples include multiple imputation, the bootstrap, the g-computation formula, multistate models, and Bayesian methods that use Markov Chain Monte Carlo. Commands to implement these methods involve some random-number generation. It is important to check that such programs do not manipulate the seed. Some packages do have a default seed if not input by the user. If they do set the seed internally, many of the nsim results will be highly correlated, if not identical, and results should not then be trusted. Checking for such behavior is worthwhile. One simple technique is to display the current state of the random-number generator, twice issue the command, and display the state after each run. If the first and second states are the same, then the program probably does not use random numbers. If the first and second states differ but the second and third do not, the seed is being reset by the program.\n\n\n5.3 4.2 Start small and build up code\nAs with any coding task it is all-too-easy to obtain misleading results in a simulation study through very minor coding errors; see, for example, the comments section of Bartlett,40 where fixing an error in a single line of code completely changed the results. A function may be sloppily written as a-bc such that it is unclear if(a-b)c or a-(b*c)was intended; a machine will interpret this code but will not discern the intention.Errors are often detected when results are unexpected: for example, when bias appears much greater than theory suggests. One design implication is that methods with known properties should be included where possible as a check that these properties are exhibited. One straightforward and intuitive approach for minimizing errors is to start small and specific for one repetition, then build and generalise, including plenty of built-in checks. In a simulation study with nsim &gt; 1 and several simulated variables, a good starting point is to generate one simulated dataset with large nobs. If variables are being generated separately then the code for each should be added one by one and the generated data explored to (1) check that the code behaves as expected and (2) ensure the data have the desired characteristics. For example, Stata’s rnormal(m,s)function simulates normal variates with meanmand standard devi- ations. The usual notation for a normal distribution uses a mean and variance. We have seen this syntax trip up several good programmers. By checking the standard deviation of a variable simulated by rnormal()in a single large simulated dataset, it should be obvious if it does not behave in the expected fashion. The simulation file should be built to include different data-generating mechanisms, methods, or estimands, again checking that behavior is as expected. Using the above example again, if the basic data-generating mechanism used N(μ, 1), the issue with specifying standard deviations vs variances would not be detected, but it would for data-generating mechanisms with σ2 ≠ 1. When satisfied with the large dataset being generated, we apply each method. Once satisfied that one large run is behaving sensibly, it is worth setting the required nobs for the simulation study and exploring the simulated datasets produced under a handful of different seeds. When satisfied that the program still behaves sensibly, it may be worth running a few (say tens of) repetitions. If, for example, convergence problems are anticipated, or bias is expected to be 0, this can be checked informally without the full set of simulations. After thoroughly checking through and generalizing code, the full set of nsim repetitions may be run. However, recall the precaution in Section 4.1 to store the states of the random-number generator and the reasons. If failure occurs in repetition 4120 of 5000, we will want to understand why. In this case, a record of the 4120th start state means we can reproduce the problematic dataset quickly. While the ability to reproduce specific errors is useful, it is also practically helpful to be able to continue even when an error occurs. For this purpose, we direct readers to the capture command in Stata and the try command in R. The failed analysis must be recorded as a missing value in the Estimates dataset, together with reasons if possible.\n\n\n5.4 4.3 Using different software packages for different methods\nIt is frequently the case that competing methods are implemented in different software packages, and it would be more burdensome to try and code them all in one package than to implement them in different packages. There are two possible solutions. The first is to simulate data separately in the different packages and then use the methods on those data. The second is to simulate data in one package and export simulated data so that different methods are based on the same simulated datasets. Both approaches are valid in principle, but we advocate the latter. First, if data are generated independently for differ- ent methods, there will be different (random) Monte Carlo error affecting each repetition. By using the same simulated data for both comparisons, this Monte Carlo error will affect methods’ performance in the same way because methods are matched on the same generated data. Second, it is cumbersome to do a job twice, and because different software packages have different quirks, it will not be easy to ensure data really are being generated identically. Third, it is impor- tant to understand that our aim is to compare methods, and while the software implementation may be important to evaluate, the way the software package simulates data is not of interest: using a method in practice would involve a soft- ware implementation, but not simulating data using that package. Whatever data an analyst was faced with would be the same regardless of the software being used."
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#analysis-of-estimates-data",
    "href": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#analysis-of-estimates-data",
    "title": "Using simulation studies to evaluate statistical methods",
    "section": "6 5. ANALYSIS OF ESTIMATES DATA",
    "text": "6 5. ANALYSIS OF ESTIMATES DATA\nThis section describes estimation for various performance measures along with Monte Carlo SEs. We advocate two preliminaries: checking for missing estimates and plots of the estimates data.\n\n6.1 5.1 Checking the estimates data and preliminaries\nThe number of missing values, eg, of θ̂ i and SÊ(θ̂ i) (for example due to nonconvergence), is the first performance measure to assess. The data produced under repetitions for which missing values were returned should be explored to understand how a method failed (see Section 4) and, ideally, the code made more robust to reduce the frequency of failures.\nMissing values in the estimates dataset pose a missing data problem regarding the analysis of other performance mea- sures. It seems implausible that values would be missing completely at random41; estimates will usually be missing due\nto nonconvergence so will likely depend on some characteristic/s of a given simulated dataset. When the “method” being evaluated involves an analyst’s procedure (as described in Section 3.4), for example, the model changes if the first-choice model does not converge, this can reduce or remove missing values from the estimates data (though it changes the nature of the method being evaluated; see Section 3.4). If more than two methods are evaluated, and one always returns an estimate θ̂\ni, then missing values for another method may be related to the returned values for the first method. In the presence of a nontrivial proportion of missing estimates data, analysis of further performance measures should be tentative, particularly when comparing methods with different numbers of θ̂\ni missing. “Nontrivial” means any proportion that could meaningfully alter estimated performance. If we\nare interested in detecting tiny biases, even 1% may be nontrivial. Before undertaking a formal analysis of the estimates dataset, it is sensible to undertake some exploratory analysis. Plots are often helpful here. For example, Kahan31 assessed the performance of a two-stage procedure for the analysis of factorial trials. The procedure was unbiased (both conditionally and unconditionally), yet a histogram of θ̂\ni exhibited a\nbimodal distribution with modes equally spaced at either side of θ, with almost no values of θ̂\ni close to θ. This may cause\nconcern and would have been missed had the analysis proceeded straight to the estimation of performance. For simulation studies targeting an estimand, the following plots are often informative:\n\n\n6.2 5.2 Estimation of performance and monte carlo standard errors for some common performance measures\nThis section outlines some common performance measures, properties they are designed to assess, how they are estimated and how Monte Carlo standard errors are computed. We suppress the “hat” notation for performance measures, but emphasise that these are estimates. For interpretation of results, performance measures should usually be considered jointly (one could prefer a method with zero variance by conveniently ignoring bias).\n\nMonte Carlo standard errors quantify simulation uncertainty: they provide an estimate of the SE of (estimated) per- formance due to using finite nsim. The Monte Carlo SE targets the sampling distribution of repeatedly running the same simulation study (with nsim repetitions) under different random-number seeds. In our review of simulation studies in Statistics in Medicine Volume 34, 93 did not mention Monte Carlo SEs for estimated performance. The formulas for computing Monte Carlo SEs given in Table 6 with description and comments in thetext. For empirical SE, relative % increase in precision, and relative error, the Monte Carlo SE formulas assume normally distributed θ̂; for non-normal θ̂, robust SEs exist; see White and Carlin.42 Bias is frequently of central interest, and quantifies whether a method targets θ on average. Frequentist theory holds unbiasedness to be a key property. The mean of θ̂ i, θ̄, is often reported instead. This is estimated in the same way but without subtracting the constant θ, and so has the same Monte Carlo SE. It is sometimes preferable to report the relative bias, rather than absolute. If different values of θ are used for different data-generating mechanisms then relative bias permits a more straightforward comparison across values. However, relative bias can be used only for |θ| &gt; 0. The absence of bias is one property of an estimator; while it is often of central interest, we may sometimes accept small biases because of other good properties. The empirical SE is a measure of the precision or efficiency of the estimator of θ. It depends only on θ̂ i and does\nnot require knowledge of θ. The empirical SE estimates the long-run standard deviation of θ̂\ni over the nsim repetitions. Several other designations are in common use; in our review, the terms used included “empirical standard deviation,” “Monte Carlo standard deviation,” “observed SE,” and “sampling SE.” The empirical standard error can be hard to interpret for a single method (unless compared to a lower bound), and the relative precision is often of interest when comparing methods. Note that, if either method is biased, relative precision should be interpreted with caution because an estimator that is biased towards the null can have small empirical SE as a result of the bias: θ̂\ni∕2 has smaller empirical SE than θ̂ i. A related measure, which also takes the true value of θ into account, is the mean squared error (MSE). The MSE is the\nsum of the squared bias and variance of θ̂. This appears a natural way to integrate both measures into one summary perfor- mance measure (low variance is penalised for bias), but we caution that, for method comparisons, the relative influence of\n\n\n6.3 5.3 Sample size for simulation studies\n\n\n6.4 5.4 Remarks on analysis"
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#reporting",
    "href": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#reporting",
    "title": "Using simulation studies to evaluate statistical methods",
    "section": "7 6. REPORTING",
    "text": "7 6. REPORTING\n\n7.1 6.1 The “methods” section\n\n\n7.2 6.2 Presentation of results"
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#worked-illustrative-example",
    "href": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#worked-illustrative-example",
    "title": "Using simulation studies to evaluate statistical methods",
    "section": "8 7. WORKED ILLUSTRATIVE EXAMPLE",
    "text": "8 7. WORKED ILLUSTRATIVE EXAMPLE\n\n8.1 7.1 Design of example\n\n\n8.2 7.2 Exploration and visualisation of results\n\n\n8.3 7.3 Analysis of example\n\n\n8.4 7.4 Conclusions of example"
  },
  {
    "objectID": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#concluding-remarks",
    "href": "posts/statistics/2025/論文紹介_Using_simulation_studies_to_evaluate.html#concluding-remarks",
    "title": "Using simulation studies to evaluate statistical methods",
    "section": "9 8. CONCLUDING REMARKS",
    "text": "9 8. CONCLUDING REMARKS\n\n9.1 8.1 Future directions\n\n\n9.2 8.2 Final remark"
  }
]